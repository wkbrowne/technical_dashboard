{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Ranking by Mutual Information\n",
    "\n",
    "This notebook analyzes all computed features using mutual information with proper statistical significance testing and uniqueness bias mitigation.\n",
    "\n",
    "## Overview\n",
    "- **Parallel MI computation** for efficiency with large feature sets\n",
    "- **Statistical significance testing** against randomized baselines\n",
    "- **Uniqueness bias mitigation** using sample weights from overlap analysis\n",
    "- **Comprehensive analysis** of feature effectiveness for triple barrier targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 64\n",
      "Notebook ready for mutual information analysis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scientific computing\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Configure display and logging\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")\n",
    "print(f\"Notebook ready for mutual information analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features and targets...\n",
      "Features shape: (3762490, 436)\n",
      "Targets shape: (1180922, 14)\n",
      "\n",
      "Feature columns: 436\n",
      "Target columns: ['symbol', 't0', 't_hit', 'hit', 'entry_px', 'top', 'bot', 'h_used', 'price_hit', 'ret_from_entry', 'n_overlapping_trajs', 'weight_overlap', 'weight_class_balance', 'weight_final']\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../artifacts\")\n",
    "FEATURES_FILE = \"features_complete.parquet\"  # or features_daily.parquet\n",
    "TARGETS_FILE = \"targets_triple_barrier.parquet\"\n",
    "\n",
    "# Load data files\n",
    "print(\"Loading features and targets...\")\n",
    "features_df = pd.read_parquet(DATA_DIR / FEATURES_FILE)\n",
    "targets_df = pd.read_parquet(DATA_DIR / TARGETS_FILE)\n",
    "\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Targets shape: {targets_df.shape}\")\n",
    "print(f\"\\nFeature columns: {len(features_df.columns)}\")\n",
    "print(f\"Target columns: {list(targets_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for mutual information analysis...\n",
      "Aligned data shape: (1180922, 440)\n",
      "Feature columns identified: 434\n",
      "Using combined sample weights: min=0.217, max=6.479\n",
      "\n",
      "Data Quality Summary:\n",
      "  Total samples: 1,180,922\n",
      "  Target distribution: {-1: 653665, 0: 190951, 1: 336306}\n",
      "  Features with >50% missing: 15\n",
      "  Infinite values: 14\n"
     ]
    }
   ],
   "source": [
    "# Data alignment and preparation\n",
    "def prepare_data_for_analysis(features_df: pd.DataFrame, targets_df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare and align features with targets for mutual information analysis.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (aligned_features, targets, sample_weights)\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for mutual information analysis...\")\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    if 'date' in features_df.columns:\n",
    "        features_df['date'] = pd.to_datetime(features_df['date'])\n",
    "    if 't0' in targets_df.columns:\n",
    "        targets_df['t0'] = pd.to_datetime(targets_df['t0'])\n",
    "    \n",
    "    # Align features with targets (inner join on symbol + date)\n",
    "    if 'date' in features_df.columns and 't0' in targets_df.columns:\n",
    "        # Standard alignment: features.date = targets.t0\n",
    "        aligned_df = features_df.merge(\n",
    "            targets_df[['symbol', 't0', 'hit', 'weight_final', 'n_overlapping_trajs']],\n",
    "            left_on=['symbol', 'date'], \n",
    "            right_on=['symbol', 't0'],\n",
    "            how='inner'\n",
    "        )\n",
    "    else:\n",
    "        # Fallback: assume data is already aligned by index\n",
    "        print(\"Warning: Using index-based alignment - ensure data is properly aligned\")\n",
    "        aligned_df = pd.concat([features_df, targets_df[['hit', 'weight_final']]], axis=1).dropna()\n",
    "    \n",
    "    print(f\"Aligned data shape: {aligned_df.shape}\")\n",
    "    \n",
    "    # Identify feature columns (exclude metadata and target columns)\n",
    "    exclude_cols = {'symbol', 'date', 't0', 'hit', 'weight_final', 'weight_overlap', \n",
    "                    'weight_class_balance', 'n_overlapping_trajs'}\n",
    "    feature_cols = [col for col in aligned_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Feature columns identified: {len(feature_cols)}\")\n",
    "    \n",
    "    # Extract features, targets, and weights\n",
    "    X = aligned_df[feature_cols].copy()\n",
    "    y = aligned_df['hit'].values\n",
    "    \n",
    "    # Use final combined weights if available, otherwise uniform weights\n",
    "    if 'weight_final' in aligned_df.columns:\n",
    "        sample_weights = aligned_df['weight_final'].values\n",
    "        print(f\"Using combined sample weights: min={sample_weights.min():.3f}, max={sample_weights.max():.3f}\")\n",
    "    else:\n",
    "        sample_weights = np.ones(len(X))\n",
    "        print(\"Warning: No sample weights found - using uniform weights\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(f\"\\nData Quality Summary:\")\n",
    "    print(f\"  Total samples: {len(X):,}\")\n",
    "    print(f\"  Target distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "    print(f\"  Features with >50% missing: {(X.isnull().mean() > 0.5).sum()}\")\n",
    "    print(f\"  Infinite values: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "    \n",
    "    return X, y, sample_weights, aligned_df\n",
    "\n",
    "# Prepare data\n",
    "X, y, sample_weights, aligned_df = prepare_data_for_analysis(features_df, targets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallel Mutual Information Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mutual information for 434 features...\n",
      "Preprocessing features...\n",
      "Preprocessed 434 features\n",
      "Processing 9 batches of ~50 features each\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38d834af9b240dea3a9358c0f21f0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing MI:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Batch 0 had errors\n",
      "Warning: Batch 1 had errors\n",
      "Warning: Batch 2 had errors\n",
      "Warning: Batch 3 had errors\n",
      "Warning: Batch 4 had errors\n",
      "Warning: Batch 5 had errors\n",
      "Warning: Batch 6 had errors\n",
      "Warning: Batch 7 had errors\n",
      "Warning: Batch 8 had errors\n",
      "Computed MI for 434 features\n",
      "MI score range: [0.000000, 0.000000]\n"
     ]
    }
   ],
   "source": [
    "class ParallelMutualInformation:\n",
    "    \"\"\"\n",
    "    Efficient parallel computation of mutual information with significance testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_jobs=-1, batch_size=50, random_state=42):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "        \n",
    "    def preprocess_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocess features for mutual information computation.\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing features...\")\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_processed = X_processed.fillna(X_processed.median())\n",
    "        \n",
    "        # Handle infinite values\n",
    "        X_processed = X_processed.replace([np.inf, -np.inf], np.nan)\n",
    "        X_processed = X_processed.fillna(X_processed.median())\n",
    "        \n",
    "        # Clip extreme outliers (beyond 5 standard deviations)\n",
    "        numeric_cols = X_processed.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            mean_val = X_processed[col].mean()\n",
    "            std_val = X_processed[col].std()\n",
    "            if std_val > 0:\n",
    "                lower_bound = mean_val - 5 * std_val\n",
    "                upper_bound = mean_val + 5 * std_val\n",
    "                X_processed[col] = np.clip(X_processed[col], lower_bound, upper_bound)\n",
    "        \n",
    "        print(f\"Preprocessed {len(X_processed.columns)} features\")\n",
    "        return X_processed\n",
    "    \n",
    "    def compute_mi_batch(self, feature_batch: pd.DataFrame, y: np.ndarray, \n",
    "                        sample_weight: np.ndarray, batch_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute mutual information for a batch of features.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Compute MI for this batch\n",
    "            mi_scores = mutual_info_classif(\n",
    "                feature_batch.values, y, \n",
    "                discrete_features=False,\n",
    "                sample_weight=sample_weight,\n",
    "                random_state=self.random_state + batch_idx\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'batch_idx': batch_idx,\n",
    "                'feature_names': list(feature_batch.columns),\n",
    "                'mi_scores': mi_scores,\n",
    "                'n_features': len(feature_batch.columns)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            return {\n",
    "                'batch_idx': batch_idx,\n",
    "                'feature_names': list(feature_batch.columns),\n",
    "                'mi_scores': np.zeros(len(feature_batch.columns)),\n",
    "                'n_features': len(feature_batch.columns),\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def compute_mutual_information(self, X: pd.DataFrame, y: np.ndarray, \n",
    "                                 sample_weight: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute mutual information scores for all features in parallel.\n",
    "        \"\"\"\n",
    "        print(f\"Computing mutual information for {len(X.columns)} features...\")\n",
    "        \n",
    "        # Preprocess features\n",
    "        X_processed = self.preprocess_features(X)\n",
    "        \n",
    "        # Create feature batches\n",
    "        feature_cols = list(X_processed.columns)\n",
    "        feature_batches = [feature_cols[i:i+self.batch_size] \n",
    "                          for i in range(0, len(feature_cols), self.batch_size)]\n",
    "        \n",
    "        print(f\"Processing {len(feature_batches)} batches of ~{self.batch_size} features each\")\n",
    "        \n",
    "        # Parallel computation\n",
    "        with tqdm(total=len(feature_batches), desc=\"Computing MI\") as pbar:\n",
    "            def update_progress(result):\n",
    "                pbar.update(1)\n",
    "                return result\n",
    "            \n",
    "            results = Parallel(n_jobs=self.n_jobs, verbose=0)(\n",
    "                delayed(self.compute_mi_batch)(\n",
    "                    X_processed[batch_features], y, sample_weight, i\n",
    "                )\n",
    "                for i, batch_features in enumerate(feature_batches)\n",
    "            )\n",
    "            \n",
    "            for _ in results:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Combine results\n",
    "        all_features = []\n",
    "        all_scores = []\n",
    "        \n",
    "        for result in sorted(results, key=lambda x: x['batch_idx']):\n",
    "            all_features.extend(result['feature_names'])\n",
    "            all_scores.extend(result['mi_scores'])\n",
    "            \n",
    "            if 'error' in result:\n",
    "                print(f\"Warning: Batch {result['batch_idx']} had errors\")\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        mi_results = pd.DataFrame({\n",
    "            'feature': all_features,\n",
    "            'mi_score': all_scores\n",
    "        }).sort_values('mi_score', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Computed MI for {len(mi_results)} features\")\n",
    "        print(f\"MI score range: [{mi_results['mi_score'].min():.6f}, {mi_results['mi_score'].max():.6f}]\")\n",
    "        \n",
    "        return mi_results\n",
    "\n",
    "# Initialize MI computer\n",
    "mi_computer = ParallelMutualInformation(n_jobs=-1, batch_size=50)\n",
    "\n",
    "# Compute mutual information scores\n",
    "mi_results = mi_computer.compute_mutual_information(X, y, sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignificanceTester:\n",
    "    \"\"\"\n",
    "    Test statistical significance of MI scores against randomized baselines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_permutations=1000, n_jobs=-1, random_state=42):\n",
    "        self.n_permutations = n_permutations\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def generate_random_baseline(self, X: pd.DataFrame, y: np.ndarray, \n",
    "                                sample_weight: np.ndarray, perm_idx: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate randomized baseline MI scores for one permutation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create random permutation of targets while preserving sample weights\n",
    "            np.random.seed(self.random_state + perm_idx)\n",
    "            y_permuted = np.random.permutation(y)\n",
    "            \n",
    "            # Compute MI with permuted targets\n",
    "            mi_scores = mutual_info_classif(\n",
    "                X.values, y_permuted,\n",
    "                discrete_features=False,\n",
    "                sample_weight=sample_weight,\n",
    "                random_state=self.random_state + perm_idx\n",
    "            )\n",
    "            \n",
    "            return mi_scores\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in permutation {perm_idx}: {e}\")\n",
    "            return np.zeros(X.shape[1])\n",
    "    \n",
    "    def compute_significance(self, X: pd.DataFrame, y: np.ndarray, \n",
    "                           sample_weight: np.ndarray, observed_mi: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute statistical significance of observed MI scores.\n",
    "        \"\"\"\n",
    "        print(f\"Computing statistical significance with {self.n_permutations} permutations...\")\n",
    "        \n",
    "        # Preprocess features (same as for observed MI)\n",
    "        X_processed = mi_computer.preprocess_features(X)\n",
    "        \n",
    "        # Generate random baselines in parallel\n",
    "        print(\"Generating randomized baselines...\")\n",
    "        with tqdm(total=self.n_permutations, desc=\"Random baselines\") as pbar:\n",
    "            random_scores = Parallel(n_jobs=self.n_jobs, verbose=0)(\n",
    "                delayed(self.generate_random_baseline)(\n",
    "                    X_processed, y, sample_weight, i\n",
    "                )\n",
    "                for i in range(self.n_permutations)\n",
    "            )\n",
    "            \n",
    "            for _ in random_scores:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Convert to array: [n_permutations, n_features]\n",
    "        random_scores_array = np.array(random_scores)\n",
    "        \n",
    "        print(f\"Generated {random_scores_array.shape[0]} random baselines for {random_scores_array.shape[1]} features\")\n",
    "        \n",
    "        # Compute p-values and effect sizes\n",
    "        results = []\n",
    "        feature_names = list(X_processed.columns)\n",
    "        \n",
    "        for i, feature in enumerate(feature_names):\n",
    "            observed_score = observed_mi[observed_mi['feature'] == feature]['mi_score'].iloc[0]\n",
    "            random_feature_scores = random_scores_array[:, i]\n",
    "            \n",
    "            # P-value: proportion of random scores >= observed score\n",
    "            p_value = (np.sum(random_feature_scores >= observed_score) + 1) / (self.n_permutations + 1)\n",
    "            \n",
    "            # Effect size: standardized difference from random baseline\n",
    "            random_mean = np.mean(random_feature_scores)\n",
    "            random_std = np.std(random_feature_scores)\n",
    "            effect_size = (observed_score - random_mean) / (random_std + 1e-8)\n",
    "            \n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'mi_score': observed_score,\n",
    "                'random_mean': random_mean,\n",
    "                'random_std': random_std,\n",
    "                'p_value': p_value,\n",
    "                'effect_size': effect_size\n",
    "            })\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        significance_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Multiple testing correction\n",
    "        rejected, p_adjusted, _, _ = multipletests(\n",
    "            significance_df['p_value'], \n",
    "            method='fdr_bh', \n",
    "            alpha=0.05\n",
    "        )\n",
    "        \n",
    "        significance_df['p_adjusted'] = p_adjusted\n",
    "        significance_df['significant'] = rejected\n",
    "        \n",
    "        # Add significance categories\n",
    "        significance_df['significance_level'] = pd.cut(\n",
    "            significance_df['p_adjusted'],\n",
    "            bins=[0, 0.001, 0.01, 0.05, 1.0],\n",
    "            labels=['p<0.001', '0.001â‰¤p<0.01', '0.01â‰¤p<0.05', 'pâ‰¥0.05']\n",
    "        )\n",
    "        \n",
    "        # Sort by MI score\n",
    "        significance_df = significance_df.sort_values('mi_score', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Significance testing complete:\")\n",
    "        print(f\"  Significant features (FDR<0.05): {significance_df['significant'].sum()}\")\n",
    "        print(f\"  Highly significant (p<0.001): {(significance_df['p_adjusted'] < 0.001).sum()}\")\n",
    "        \n",
    "        return significance_df\n",
    "\n",
    "# Initialize significance tester\n",
    "sig_tester = SignificanceTester(n_permutations=1000, n_jobs=-1)\n",
    "\n",
    "# Compute significance (this may take a while)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING SIGNIFICANCE TESTING - THIS MAY TAKE 10-30 MINUTES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_results = sig_tester.compute_significance(X, y, sample_weights, mi_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced feature analysis\n",
    "def analyze_feature_types(results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classify and analyze different types of features.\n",
    "    \"\"\"\n",
    "    results_enhanced = results_df.copy()\n",
    "    \n",
    "    # Feature type classification\n",
    "    def classify_feature(feature_name):\n",
    "        name = feature_name.lower()\n",
    "        if name.startswith('w_'):\n",
    "            return 'Weekly'\n",
    "        elif any(x in name for x in ['rsi', 'macd', 'ema', 'sma', 'ma_']):\n",
    "            return 'Trend/Momentum'\n",
    "        elif any(x in name for x in ['vol', 'volatility', 'regime']):\n",
    "            return 'Volatility'\n",
    "        elif any(x in name for x in ['volume', 'vol_']):\n",
    "            return 'Volume'\n",
    "        elif any(x in name for x in ['hurst', 'fractal']):\n",
    "            return 'Fractal/Hurst'\n",
    "        elif any(x in name for x in ['distance', 'zscore', 'breakout']):\n",
    "            return 'Technical'\n",
    "        elif any(x in name for x in ['alpha', 'beta', 'relstr', 'xsec']):\n",
    "            return 'Cross-sectional'\n",
    "        elif any(x in name for x in ['breadth', 'advance']):\n",
    "            return 'Market Breadth'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    results_enhanced['feature_type'] = results_enhanced['feature'].apply(classify_feature)\n",
    "    \n",
    "    # Weekly vs Daily\n",
    "    results_enhanced['temporal_type'] = results_enhanced['feature'].apply(\n",
    "        lambda x: 'Weekly' if x.startswith('w_') else 'Daily'\n",
    "    )\n",
    "    \n",
    "    return results_enhanced\n",
    "\n",
    "# Analyze feature types\n",
    "final_results = analyze_feature_types(final_results)\n",
    "\n",
    "# Display top results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 20 FEATURES BY MUTUAL INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_features = final_results.head(20)[[\n",
    "    'feature', 'feature_type', 'mi_score', 'p_adjusted', 'effect_size', 'significance_level'\n",
    "]]\n",
    "\n",
    "print(top_features.to_string(index=False, float_format='%.6f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Top 20 Features by MI Score',\n",
    "        'MI Distribution vs Random Baseline', \n",
    "        'Feature Type Performance',\n",
    "        'Weekly vs Daily Features'\n",
    "    ],\n",
    "    specs=[\n",
    "        [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "        [{\"secondary_y\": False}, {\"secondary_y\": False}]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 1. Top features bar chart\n",
    "top_20 = final_results.head(20)\n",
    "colors = ['red' if p < 0.001 else 'orange' if p < 0.01 else 'yellow' if p < 0.05 else 'gray' \n",
    "          for p in top_20['p_adjusted']]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=top_20['mi_score'],\n",
    "        y=top_20['feature'],\n",
    "        orientation='h',\n",
    "        marker_color=colors,\n",
    "        name='MI Score'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# 2. MI distribution histogram\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=final_results['mi_score'],\n",
    "        nbinsx=50,\n",
    "        name='Observed MI',\n",
    "        marker_color='blue',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=final_results['random_mean'],\n",
    "        nbinsx=50,\n",
    "        name='Random Baseline',\n",
    "        marker_color='red',\n",
    "        opacity=0.7\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Feature type performance\n",
    "type_stats = final_results.groupby('feature_type').agg({\n",
    "    'mi_score': 'mean',\n",
    "    'significant': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=type_stats['feature_type'],\n",
    "        y=type_stats['mi_score'],\n",
    "        name='Avg MI Score',\n",
    "        marker_color='green'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Weekly vs Daily comparison\n",
    "temporal_stats = final_results.groupby('temporal_type').agg({\n",
    "    'mi_score': ['mean', 'count'],\n",
    "    'significant': 'sum'\n",
    "}).reset_index()\n",
    "temporal_stats.columns = ['temporal_type', 'mean_mi', 'count', 'significant']\n",
    "temporal_stats['pct_significant'] = temporal_stats['significant'] / temporal_stats['count'] * 100\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=temporal_stats['temporal_type'],\n",
    "        y=temporal_stats['mean_mi'],\n",
    "        name='Mean MI Score',\n",
    "        marker_color='purple'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Mutual Information Feature Analysis Results\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total features analyzed: {len(final_results):,}\")\n",
    "print(f\"Statistically significant features: {final_results['significant'].sum():,} ({final_results['significant'].mean()*100:.1f}%)\")\n",
    "print(f\"Highly significant (p<0.001): {(final_results['p_adjusted'] < 0.001).sum():,}\")\n",
    "print(f\"Mean MI score: {final_results['mi_score'].mean():.6f}\")\n",
    "print(f\"Mean random baseline: {final_results['random_mean'].mean():.6f}\")\n",
    "print(f\"Best feature: {final_results.iloc[0]['feature']} (MI={final_results.iloc[0]['mi_score']:.6f})\")\n",
    "\n",
    "print(\"\\nFeature Type Breakdown:\")\n",
    "type_summary = final_results.groupby('feature_type').agg({\n",
    "    'mi_score': ['count', 'mean'],\n",
    "    'significant': 'sum'\n",
    "}).round(4)\n",
    "print(type_summary)\n",
    "\n",
    "print(\"\\nTemporal Feature Comparison:\")\n",
    "print(temporal_stats[['temporal_type', 'count', 'mean_mi', 'significant', 'pct_significant']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Uniqueness Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueness bias validation\n",
    "def validate_uniqueness_bias_correction(aligned_df: pd.DataFrame, final_results: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze the effectiveness of uniqueness bias correction.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UNIQUENESS BIAS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'n_overlapping_trajs' not in aligned_df.columns:\n",
    "        print(\"Warning: No overlap information available for bias analysis\")\n",
    "        return\n",
    "    \n",
    "    # Compute unweighted MI for comparison\n",
    "    print(\"Computing unweighted MI scores for comparison...\")\n",
    "    X_processed = mi_computer.preprocess_features(X)\n",
    "    \n",
    "    unweighted_mi = mutual_info_classif(\n",
    "        X_processed.values, y,\n",
    "        discrete_features=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'feature': list(X_processed.columns),\n",
    "        'weighted_mi': final_results['mi_score'],\n",
    "        'unweighted_mi': unweighted_mi\n",
    "    })\n",
    "    \n",
    "    # Analyze correlation with overlap counts\n",
    "    feature_overlap_stats = []\n",
    "    \n",
    "    for feature in comparison_df['feature'].head(50):  # Top 50 features\n",
    "        if feature in aligned_df.columns:\n",
    "            feature_data = aligned_df[[feature, 'n_overlapping_trajs']].dropna()\n",
    "            if len(feature_data) > 10:\n",
    "                correlation = feature_data[feature].corr(feature_data['n_overlapping_trajs'])\n",
    "                feature_overlap_stats.append({\n",
    "                    'feature': feature,\n",
    "                    'overlap_correlation': correlation\n",
    "                })\n",
    "    \n",
    "    overlap_stats_df = pd.DataFrame(feature_overlap_stats)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Weighted vs Unweighted MI scatter\n",
    "    axes[0,0].scatter(comparison_df['unweighted_mi'], comparison_df['weighted_mi'], alpha=0.6)\n",
    "    axes[0,0].plot([0, comparison_df[['weighted_mi', 'unweighted_mi']].max().max()], \n",
    "                   [0, comparison_df[['weighted_mi', 'unweighted_mi']].max().max()], 'r--')\n",
    "    axes[0,0].set_xlabel('Unweighted MI')\n",
    "    axes[0,0].set_ylabel('Weighted MI')\n",
    "    axes[0,0].set_title('Weighted vs Unweighted MI Scores')\n",
    "    \n",
    "    # 2. Difference analysis\n",
    "    comparison_df['mi_difference'] = comparison_df['weighted_mi'] - comparison_df['unweighted_mi']\n",
    "    axes[0,1].hist(comparison_df['mi_difference'], bins=50, alpha=0.7)\n",
    "    axes[0,1].axvline(0, color='red', linestyle='--')\n",
    "    axes[0,1].set_xlabel('MI Difference (Weighted - Unweighted)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].set_title('Distribution of MI Score Differences')\n",
    "    \n",
    "    # 3. Sample weight distribution\n",
    "    axes[1,0].hist(sample_weights, bins=50, alpha=0.7)\n",
    "    axes[1,0].set_xlabel('Sample Weight')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].set_title('Distribution of Sample Weights')\n",
    "    \n",
    "    # 4. Overlap correlation analysis\n",
    "    if len(overlap_stats_df) > 0:\n",
    "        axes[1,1].hist(overlap_stats_df['overlap_correlation'].dropna(), bins=20, alpha=0.7)\n",
    "        axes[1,1].axvline(0, color='red', linestyle='--')\n",
    "        axes[1,1].set_xlabel('Correlation with Overlap Count')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "        axes[1,1].set_title('Feature-Overlap Correlations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"Sample weight statistics:\")\n",
    "    print(f\"  Min: {sample_weights.min():.4f}, Max: {sample_weights.max():.4f}\")\n",
    "    print(f\"  Mean: {sample_weights.mean():.4f}, Std: {sample_weights.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nMI Score Comparison:\")\n",
    "    print(f\"  Mean weighted MI: {comparison_df['weighted_mi'].mean():.6f}\")\n",
    "    print(f\"  Mean unweighted MI: {comparison_df['unweighted_mi'].mean():.6f}\")\n",
    "    print(f\"  Correlation: {comparison_df['weighted_mi'].corr(comparison_df['unweighted_mi']):.4f}\")\n",
    "    \n",
    "    significant_changes = (abs(comparison_df['mi_difference']) > 0.001).sum()\n",
    "    print(f\"  Features with significant changes: {significant_changes} ({significant_changes/len(comparison_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(overlap_stats_df) > 0:\n",
    "        high_overlap_corr = (abs(overlap_stats_df['overlap_correlation']) > 0.3).sum()\n",
    "        print(f\"\\nOverlap bias indicators:\")\n",
    "        print(f\"  Features with high overlap correlation: {high_overlap_corr}/{len(overlap_stats_df)}\")\n",
    "\n",
    "# Run bias analysis\n",
    "validate_uniqueness_bias_correction(aligned_df, final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "output_dir = Path(\"../artifacts\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Main results file\n",
    "final_results.to_csv(output_dir / \"feature_ranking_mutual_information.csv\", index=False)\n",
    "final_results.to_parquet(output_dir / \"feature_ranking_mutual_information.parquet\", index=False)\n",
    "\n",
    "# Top features only\n",
    "top_significant = final_results[final_results['significant']].head(100)\n",
    "top_significant.to_csv(output_dir / \"top_significant_features.csv\", index=False)\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = {\n",
    "    'analysis_date': pd.Timestamp.now().isoformat(),\n",
    "    'total_features': len(final_results),\n",
    "    'significant_features': final_results['significant'].sum(),\n",
    "    'highly_significant': (final_results['p_adjusted'] < 0.001).sum(),\n",
    "    'mean_mi_score': final_results['mi_score'].mean(),\n",
    "    'best_feature': final_results.iloc[0]['feature'],\n",
    "    'best_mi_score': final_results.iloc[0]['mi_score'],\n",
    "    'sample_size': len(aligned_df),\n",
    "    'n_permutations': sig_tester.n_permutations\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(output_dir / \"mutual_information_analysis_summary.json\", 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE - RESULTS EXPORTED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Comprehensive results: {output_dir / 'feature_ranking_mutual_information.csv'}\")\n",
    "print(f\"Top significant features: {output_dir / 'top_significant_features.csv'}\")\n",
    "print(f\"Analysis summary: {output_dir / 'mutual_information_analysis_summary.json'}\")\n",
    "\n",
    "print(f\"\\nKey findings:\")\n",
    "print(f\"  ðŸ† Best feature: {summary_stats['best_feature']} (MI={summary_stats['best_mi_score']:.6f})\")\n",
    "print(f\"  âœ… Significant features: {summary_stats['significant_features']:,}/{summary_stats['total_features']:,} ({summary_stats['significant_features']/summary_stats['total_features']*100:.1f}%)\")\n",
    "print(f\"  ðŸŽ¯ Highly significant: {summary_stats['highly_significant']:,} (p<0.001)\")\n",
    "print(f\"  ðŸ“Š Mean MI score: {summary_stats['mean_mi_score']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
