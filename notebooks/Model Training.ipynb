{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3682b8f9-a89a-47b6-8579-ee19310bd2f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded features: (2904927, 147)\n",
      "Columns: ['low', 'volume', 'close', 'high', 'adjclose', 'open', 'ret', 'ma_10', 'pct_slope_ma_10', 'sign_ma_10', 'ma_20', 'pct_slope_ma_20', 'sign_ma_20', 'ma_30', 'pct_slope_ma_30', 'sign_ma_30', 'ma_50', 'pct_slope_ma_50', 'sign_ma_50', 'ma_75', 'pct_slope_ma_75', 'sign_ma_75', 'ma_100', 'pct_slope_ma_100', 'sign_ma_100', 'ma_150', 'pct_slope_ma_150', 'sign_ma_150', 'ma_200', 'pct_slope_ma_200', 'sign_ma_200', 'trend_score_granular', 'trend_score_sign', 'trend_score_slope', 'trend_persist_ema', 'trend_alignment', 'rv_10', 'rv_20', 'rv_60', 'rv_100', 'rv_ratio_10_60', 'rv_ratio_20_100', 'vol_regime', 'vol_regime_ema10', 'rv_z_60', 'vol_of_vol_20d', 'rv60_slope_norm', 'rv100_slope_norm', 'quiet_trend', 'hurst_ret_64', 'hurst_ret_128', 'hurst_ret_64_emaHL5', 'pct_dist_ma_20', 'pct_dist_ma_20_z', 'pct_dist_ma_50', 'pct_dist_ma_50_z', 'pct_dist_ma_100', 'pct_dist_ma_100_z', 'pct_dist_ma_200', 'pct_dist_ma_200_z', 'min_pct_dist_ma', 'relative_dist_20_50', 'relative_dist_20_50_z', 'hl_range', 'hl_range_pct_close', 'true_range', 'tr_pct_close', 'atr_percent', 'gap_pct', 'gap_atr_ratio', '5d_high', '5d_low', '5d_range', '5d_range_pct_close', 'pos_in_5d_range', 'breakout_up_5d', 'breakout_dn_5d', 'range_expansion_5d', 'range_z_5d', '10d_high', '10d_low', '10d_range', '10d_range_pct_close', 'pos_in_10d_range', 'breakout_up_10d', 'breakout_dn_10d', 'range_expansion_10d', 'range_z_10d', '20d_high', '20d_low', '20d_range', '20d_range_pct_close', 'pos_in_20d_range', 'breakout_up_20d', 'breakout_dn_20d', 'range_expansion_20d', 'range_z_20d', 'range_x_rvol20', 'vol_ma_20', 'vol_ma_50', 'vol_z_20', 'vol_z_60', 'rvol_20', 'rvol_50', 'dollar_vol_ma_20', 'rdollar_vol_20', 'obv', 'obv_z_60', 'vol_rolling_20d', 'vol_rolling_60d', 'volshock_z', 'volshock_dir', 'volshock_ema', 'vol_regime_cs_median', 'vol_regime_rel', 'alpha_resid_spy', 'alpha_mom_spy_ema10', 'alpha_mom_spy_20_ema10', 'alpha_mom_spy_60_ema10', 'alpha_mom_spy_120_ema10', 'alpha_resid_sector', 'alpha_mom_sector_ema10', 'alpha_mom_sector_20_ema10', 'alpha_mom_sector_60_ema10', 'alpha_mom_sector_120_ema10', 'alpha_mom_combo_ema10', 'alpha_mom_combo_20_ema10', 'alpha_mom_combo_60_ema10', 'alpha_mom_combo_120_ema10', 'rel_strength_spy', 'rel_strength_spy_norm', 'rel_strength_spy_slope20', 'rel_strength_sector', 'rel_strength_sector_norm', 'rel_strength_sector_slope20', 'pct_universe_above_ma20', 'pct_universe_above_ma50', 'pct_universe_above_ma200', 'ad_line_universe', 'xsec_mom_5d_z', 'xsec_mom_5d_sect_neutral_z', 'xsec_mom_20d_z', 'xsec_mom_20d_sect_neutral_z', 'xsec_mom_60d_z', 'xsec_mom_60d_sect_neutral_z', 'symbol', 'date']\n",
      "Date range: 2020-08-07 00:00:00 to 2025-08-08 00:00:00\n",
      "Unique symbols: 2311\n",
      "Sample data:\n",
      "           low    volume  close   high  adjclose   open       ret   ma_10  \\\n",
      "2904922  12.36  290900.0  12.60  12.72     12.60  12.65  0.002384  13.359   \n",
      "2904923  12.31  381400.0  12.54  13.19     12.54  12.60 -0.004773  13.181   \n",
      "2904924  11.93  765800.0  12.41  12.46     12.41  12.44 -0.010421  12.993   \n",
      "2904925  11.96  393614.0  12.16  12.59     12.16  12.39 -0.020351  12.808   \n",
      "2904926    NaN       NaN    NaN    NaN       NaN    NaN       NaN  12.750   \n",
      "\n",
      "         pct_slope_ma_10  sign_ma_10  ...  pct_universe_above_ma200  \\\n",
      "2904922         0.053466         1.0  ...                 59.780224   \n",
      "2904923         0.035509         1.0  ...                 58.241760   \n",
      "2904924         0.014761         1.0  ...                 56.483521   \n",
      "2904925        -0.007055        -1.0  ...                 57.362640   \n",
      "2904926        -0.018929        -1.0  ...                  0.000000   \n",
      "\n",
      "         ad_line_universe  xsec_mom_5d_z  xsec_mom_5d_sect_neutral_z  \\\n",
      "2904922           21507.0      -0.499898                   -0.571028   \n",
      "2904923           21434.0      -0.718440                   -0.825237   \n",
      "2904924           21429.0      -0.381271                   -0.273184   \n",
      "2904925           21389.0      -0.320635                   -0.248400   \n",
      "2904926           21389.0      -0.560794                   -0.392429   \n",
      "\n",
      "         xsec_mom_20d_z  xsec_mom_20d_sect_neutral_z  xsec_mom_60d_z  \\\n",
      "2904922       -0.223625                    -0.392394        0.070684   \n",
      "2904923       -0.145231                    -0.283620        0.111075   \n",
      "2904924       -0.480540                    -0.384715        0.088016   \n",
      "2904925       -0.642725                    -0.563518       -0.095567   \n",
      "2904926       -0.573139                    -0.557522        0.052044   \n",
      "\n",
      "         xsec_mom_60d_sect_neutral_z  symbol       date  \n",
      "2904922                    -0.038423    ZYME 2025-08-04  \n",
      "2904923                     0.032107    ZYME 2025-08-05  \n",
      "2904924                     0.025273    ZYME 2025-08-06  \n",
      "2904925                    -0.125627    ZYME 2025-08-07  \n",
      "2904926                    -0.089641    ZYME 2025-08-08  \n",
      "\n",
      "[5 rows x 147 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the features parquet file\n",
    "import pandas as pd\n",
    "\n",
    "features_df = pd.read_parquet('../artifacts/features_long.parquet')\n",
    "print(f\"✅ Loaded features: {features_df.shape}\")\n",
    "print(f\"Columns: {list(features_df.columns)}\")\n",
    "print(f\"Date range: {features_df['date'].min()} to {features_df['date'].max()}\")\n",
    "print(f\"Unique symbols: {features_df['symbol'].nunique()}\")\n",
    "print(f\"Sample data:\")\n",
    "print(features_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ba7082-cbb6-4c39-9b58-6a754d3ed0d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN_count</th>\n",
       "      <th>Inf_count</th>\n",
       "      <th>Total_rows</th>\n",
       "      <th>NaN_pct</th>\n",
       "      <th>Inf_pct</th>\n",
       "      <th>Max_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha_mom_combo_120_ema10</th>\n",
       "      <td>625110</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>21.518957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.518957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_sector_120_ema10</th>\n",
       "      <td>625110</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>21.518957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.518957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_sector_60_ema10</th>\n",
       "      <td>582965</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>20.068146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.068146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_combo_60_ema10</th>\n",
       "      <td>582965</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>20.068146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.068146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_combo_20_ema10</th>\n",
       "      <td>553425</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>19.051253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.051253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_sector_20_ema10</th>\n",
       "      <td>553425</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>19.051253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.051253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_resid_sector</th>\n",
       "      <td>544993</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>18.760988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.760988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_sector_ema10</th>\n",
       "      <td>542875</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>18.688077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.688077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_combo_ema10</th>\n",
       "      <td>542875</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>18.688077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.688077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_slope_ma_200</th>\n",
       "      <td>470991</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>16.213523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.213523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_dist_ma_200_z</th>\n",
       "      <td>470975</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>16.212972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.212972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rel_strength_sector_slope20</th>\n",
       "      <td>460160</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>15.840673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.840673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rel_strength_sector_norm</th>\n",
       "      <td>458030</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>15.767350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.767350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_spy_120_ema10</th>\n",
       "      <td>428376</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.746532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.746532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_dist_ma_200</th>\n",
       "      <td>427628</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.720783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.720783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ma_200</th>\n",
       "      <td>425362</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.642778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.642778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rel_strength_sector</th>\n",
       "      <td>417517</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.372719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.372719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_slope_ma_150</th>\n",
       "      <td>413952</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.249997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.249997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_spy_60_ema10</th>\n",
       "      <td>382711</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>13.174548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.174548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ma_150</th>\n",
       "      <td>368242</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>12.676463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.676463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             NaN_count  Inf_count  Total_rows    NaN_pct  \\\n",
       "alpha_mom_combo_120_ema10       625110          0     2904927  21.518957   \n",
       "alpha_mom_sector_120_ema10      625110          0     2904927  21.518957   \n",
       "alpha_mom_sector_60_ema10       582965          0     2904927  20.068146   \n",
       "alpha_mom_combo_60_ema10        582965          0     2904927  20.068146   \n",
       "alpha_mom_combo_20_ema10        553425          0     2904927  19.051253   \n",
       "alpha_mom_sector_20_ema10       553425          0     2904927  19.051253   \n",
       "alpha_resid_sector              544993          0     2904927  18.760988   \n",
       "alpha_mom_sector_ema10          542875          0     2904927  18.688077   \n",
       "alpha_mom_combo_ema10           542875          0     2904927  18.688077   \n",
       "pct_slope_ma_200                470991          0     2904927  16.213523   \n",
       "pct_dist_ma_200_z               470975          0     2904927  16.212972   \n",
       "rel_strength_sector_slope20     460160          0     2904927  15.840673   \n",
       "rel_strength_sector_norm        458030          0     2904927  15.767350   \n",
       "alpha_mom_spy_120_ema10         428376          0     2904927  14.746532   \n",
       "pct_dist_ma_200                 427628          0     2904927  14.720783   \n",
       "ma_200                          425362          0     2904927  14.642778   \n",
       "rel_strength_sector             417517          0     2904927  14.372719   \n",
       "pct_slope_ma_150                413952          0     2904927  14.249997   \n",
       "alpha_mom_spy_60_ema10          382711          0     2904927  13.174548   \n",
       "ma_150                          368242          0     2904927  12.676463   \n",
       "\n",
       "                             Inf_pct    Max_pct  \n",
       "alpha_mom_combo_120_ema10        0.0  21.518957  \n",
       "alpha_mom_sector_120_ema10       0.0  21.518957  \n",
       "alpha_mom_sector_60_ema10        0.0  20.068146  \n",
       "alpha_mom_combo_60_ema10         0.0  20.068146  \n",
       "alpha_mom_combo_20_ema10         0.0  19.051253  \n",
       "alpha_mom_sector_20_ema10        0.0  19.051253  \n",
       "alpha_resid_sector               0.0  18.760988  \n",
       "alpha_mom_sector_ema10           0.0  18.688077  \n",
       "alpha_mom_combo_ema10            0.0  18.688077  \n",
       "pct_slope_ma_200                 0.0  16.213523  \n",
       "pct_dist_ma_200_z                0.0  16.212972  \n",
       "rel_strength_sector_slope20      0.0  15.840673  \n",
       "rel_strength_sector_norm         0.0  15.767350  \n",
       "alpha_mom_spy_120_ema10          0.0  14.746532  \n",
       "pct_dist_ma_200                  0.0  14.720783  \n",
       "ma_200                           0.0  14.642778  \n",
       "rel_strength_sector              0.0  14.372719  \n",
       "pct_slope_ma_150                 0.0  14.249997  \n",
       "alpha_mom_spy_60_ema10           0.0  13.174548  \n",
       "ma_150                           0.0  12.676463  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with Infs (4 found):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN_count</th>\n",
       "      <th>Inf_count</th>\n",
       "      <th>Total_rows</th>\n",
       "      <th>NaN_pct</th>\n",
       "      <th>Inf_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>range_expansion_5d</th>\n",
       "      <td>205421</td>\n",
       "      <td>462</td>\n",
       "      <td>2904927</td>\n",
       "      <td>7.071469</td>\n",
       "      <td>0.015904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>range_x_rvol20</th>\n",
       "      <td>217948</td>\n",
       "      <td>240</td>\n",
       "      <td>2904927</td>\n",
       "      <td>7.502701</td>\n",
       "      <td>0.008262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>range_expansion_10d</th>\n",
       "      <td>206814</td>\n",
       "      <td>118</td>\n",
       "      <td>2904927</td>\n",
       "      <td>7.119422</td>\n",
       "      <td>0.004062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>range_expansion_20d</th>\n",
       "      <td>213236</td>\n",
       "      <td>39</td>\n",
       "      <td>2904927</td>\n",
       "      <td>7.340494</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     NaN_count  Inf_count  Total_rows   NaN_pct   Inf_pct\n",
       "range_expansion_5d      205421        462     2904927  7.071469  0.015904\n",
       "range_x_rvol20          217948        240     2904927  7.502701  0.008262\n",
       "range_expansion_10d     206814        118     2904927  7.119422  0.004062\n",
       "range_expansion_20d     213236         39     2904927  7.340494  0.001343"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Only numeric columns for inf check\n",
    "numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "nan_counts = features_df.isna().sum()\n",
    "inf_counts = pd.Series(0, index=features_df.columns)\n",
    "\n",
    "inf_counts[numeric_cols] = np.isinf(features_df[numeric_cols].to_numpy()).sum(axis=0)\n",
    "\n",
    "nan_inf_summary = pd.DataFrame({\n",
    "    \"NaN_count\": nan_counts,\n",
    "    \"Inf_count\": inf_counts,\n",
    "    \"Total_rows\": len(features_df)\n",
    "})\n",
    "\n",
    "nan_inf_summary[\"NaN_pct\"] = nan_inf_summary[\"NaN_count\"] / nan_inf_summary[\"Total_rows\"] * 100\n",
    "nan_inf_summary[\"Inf_pct\"] = nan_inf_summary[\"Inf_count\"] / nan_inf_summary[\"Total_rows\"] * 100\n",
    "\n",
    "nan_inf_summary.sort_values([\"NaN_count\", \"Inf_count\"], ascending=False)\n",
    "# Filter to top offenders by max percentage of NaNs/Infs\n",
    "top_nan_inf = (\n",
    "    nan_inf_summary\n",
    "    .assign(Max_pct=lambda df: df[[\"NaN_pct\", \"Inf_pct\"]].max(axis=1))\n",
    "    .sort_values(\"Max_pct\", ascending=False)\n",
    ")\n",
    "\n",
    "# Show top 20 offenders\n",
    "display(top_nan_inf.head(20))\n",
    "\n",
    "# Only numeric columns for inf check\n",
    "numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "nan_counts = features_df.isna().sum()\n",
    "inf_counts = pd.Series(0, index=features_df.columns)\n",
    "\n",
    "inf_counts[numeric_cols] = np.isinf(features_df[numeric_cols].to_numpy()).sum(axis=0)\n",
    "\n",
    "nan_inf_summary = pd.DataFrame({\n",
    "    \"NaN_count\": nan_counts,\n",
    "    \"Inf_count\": inf_counts,\n",
    "    \"Total_rows\": len(features_df)\n",
    "})\n",
    "\n",
    "nan_inf_summary[\"NaN_pct\"] = nan_inf_summary[\"NaN_count\"] / nan_inf_summary[\"Total_rows\"] * 100\n",
    "nan_inf_summary[\"Inf_pct\"] = nan_inf_summary[\"Inf_count\"] / nan_inf_summary[\"Total_rows\"] * 100\n",
    "\n",
    "# Filter to columns with any inf values\n",
    "inf_only = nan_inf_summary.query(\"Inf_count > 0\")\n",
    "\n",
    "print(f\"Columns with Infs ({len(inf_only)} found):\")\n",
    "display(inf_only.sort_values(\"Inf_count\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81ffd67e-fec7-46ef-86fc-58182334f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==== CONFIG ====\n",
    "DATE_COL = \"date\"              # adjust to match features_df\n",
    "MIN_TRAIN_DAYS = 252            # ~1 year of training\n",
    "TEST_DAYS = 21                  # ~1 month test per fold\n",
    "H = 5                           # horizon in days\n",
    "EMBARGO_DAYS = 1\n",
    "EXPANDING = True\n",
    "MAX_FOLDS = 5\n",
    "RECENT_ONLY_CUTOFF_DAYS = 365   # only use folds with test_start in last year\n",
    "\n",
    "# Model hyperparameters\n",
    "# Shared stability-focused GBM hyperparameters\n",
    "GBM_COMMON_PARAMS = {\n",
    "    \"n_estimators\": 500,         # More trees for stability\n",
    "    \"max_depth\": 4,              # Keep shallow to avoid overfitting\n",
    "    \"learning_rate\": 0.02,       # Lower LR for smoother convergence\n",
    "    \"subsample\": 0.8,            # Row subsampling for robustness\n",
    "    \"colsample_bytree\": 0.8,     # Feature subsampling\n",
    "    \"min_child_weight\": 5,       # Avoid splits on very small leaf sizes\n",
    "    \"reg_lambda\": 1.0,           # L2 regularization\n",
    "    \"reg_alpha\": 0.1,            # L1 regularization\n",
    "    \"gamma\": 0.1,                # Minimum loss reduction for split\n",
    "    \"random_state\": 42           # Reproducibility\n",
    "}\n",
    "\n",
    "# Model-specific parameters\n",
    "REG_PARAMS = {\n",
    "    **GBM_COMMON_PARAMS,\n",
    "    \"objective\": \"reg:squarederror\"\n",
    "}\n",
    "\n",
    "CLS_PARAMS = {\n",
    "    **GBM_COMMON_PARAMS,\n",
    "    \"objective\": \"binary:logistic\",  # Or 'multi:softprob' for multiclass\n",
    "    \"eval_metric\": \"logloss\"         # Good for classification stability\n",
    "}\n",
    "# --- Symbol / Price / Winsorization config ---\n",
    "SYMBOL_COL = \"symbol\"      # column in features_df with ticker or asset ID\n",
    "PRICE_COL  = \"adjclose\"    # column to use for log-return calculations\n",
    "PCT_WINSOR = (0.01, 0.01)  # winsorize limits (lower, upper) for returns\n",
    "\n",
    "\n",
    "from hyperopt import hp\n",
    "\n",
    "# Regression space\n",
    "REG_SPACE = {\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 200, 800, 50),  # fewer than default to save time\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 8, 1),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", -3.0, -1.2),  # ~0.05 to 0.3\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.6, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.6, 1.0),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 10, 1),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", -2.3, 1.0),  # ~0.1 to 2.7\n",
    "}\n",
    "\n",
    "# Classification space\n",
    "CLS_SPACE = {\n",
    "    \"n_estimators\": hp.quniform(\"cls_n_estimators\", 200, 800, 50),\n",
    "    \"max_depth\": hp.quniform(\"cls_max_depth\", 3, 8, 1),\n",
    "    \"learning_rate\": hp.loguniform(\"cls_learning_rate\", -3.0, -1.2),\n",
    "    \"subsample\": hp.uniform(\"cls_subsample\", 0.6, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"cls_colsample_bytree\", 0.6, 1.0),\n",
    "    \"min_child_weight\": hp.quniform(\"cls_min_child_weight\", 1, 10, 1),\n",
    "    \"reg_lambda\": hp.loguniform(\"cls_reg_lambda\", -2.3, 1.0),\n",
    "}\n",
    "\n",
    "# ---------- Controls ----------\n",
    "\n",
    "EVAL_FRAC          = 0.2                  # last 20% of each TRAIN fold used as validation\n",
    "EARLY_STOP_ROUNDS  = 50\n",
    "FIXED_RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1587a234-d933-4530-bcf8-043a49afd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "from typing import Iterator, Tuple\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, \\\n",
    "            mean_absolute_error, accuracy_score, precision_recall_fscore_support,roc_auc_score\n",
    "\n",
    "def winsorize_series(s: pd.Series, limits=(0.01, 0.01)) -> pd.Series:\n",
    "    mask = s.notna()\n",
    "    if not mask.any():\n",
    "        return s.copy()\n",
    "    arr_w = winsorize(s[mask].astype(float).to_numpy(), limits=limits)\n",
    "    out = pd.Series(np.nan, index=s.index)\n",
    "    out.loc[mask] = arr_w\n",
    "    return out\n",
    "def walk_forward_splits(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    min_train_days: int,\n",
    "    test_days: int,\n",
    "    horizon: int,\n",
    "    embargo_days: int = 1,\n",
    "    max_folds: int = 10_000,\n",
    "    expanding: bool = True,\n",
    ") -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Time-based folds with purge (horizon-1) + embargo.\n",
    "    Splits are formed on UNIQUE DATES across the whole panel.\n",
    "    If expanding=True, the training window grows each fold.\n",
    "    \"\"\"\n",
    "    dates = pd.to_datetime(df[date_col]).sort_values().unique()\n",
    "    n_dates = len(dates)\n",
    "    k = 0\n",
    "    start = 0\n",
    "\n",
    "    while True:\n",
    "        train_end = start + min_train_days\n",
    "        test_end = train_end + test_days\n",
    "        if test_end > n_dates or (k >= max_folds):\n",
    "            break\n",
    "\n",
    "        # Expanding vs fixed train window\n",
    "        if expanding:\n",
    "            train_dates = dates[:train_end]  # 0 .. train_end-1\n",
    "        else:\n",
    "            train_dates = dates[start:train_end]\n",
    "\n",
    "        test_dates = dates[train_end:test_end]\n",
    "        if len(test_dates) == 0:\n",
    "            break\n",
    "\n",
    "        test_start_date = test_dates[0]\n",
    "        test_end_date = test_dates[-1]\n",
    "\n",
    "        # Purge + embargo\n",
    "        purge_start = test_start_date - np.timedelta64(horizon - 1, \"D\")\n",
    "        purge_end = test_end_date\n",
    "        embargo_end = test_end_date + np.timedelta64(embargo_days, \"D\")\n",
    "\n",
    "        d = pd.to_datetime(df[date_col]).values\n",
    "        in_base_train = (d <= train_dates[-1]) if expanding else ((d >= train_dates[0]) & (d <= train_dates[-1]))\n",
    "        in_purge = (d >= purge_start) & (d <= purge_end)\n",
    "        in_embargo = (d > test_end_date) & (d <= embargo_end)\n",
    "        in_test = (d >= test_start_date) & (d <= test_end_date)\n",
    "\n",
    "        train_mask = in_base_train & (~in_purge) & (~in_embargo) & (~in_test)\n",
    "        test_mask = in_test\n",
    "\n",
    "        tr_idx = np.flatnonzero(train_mask)\n",
    "        te_idx = np.flatnonzero(test_mask)\n",
    "\n",
    "        if len(tr_idx) and len(te_idx):\n",
    "            yield tr_idx, te_idx\n",
    "            k += 1\n",
    "\n",
    "        # advance window by one test block\n",
    "        start += test_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe83b36-c5c5-4cba-adb0-84708e035f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Targets built | H=5 | rows(all)=2,904,927 → model_df=2,685,476 | features=142\n",
      "Date span: 2020-08-07 → 2025-08-08 | Symbols: 2311\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- sanity checks ---\n",
    "req = [\"features_df\", \"winsorize_series\", \"SYMBOL_COL\", \"DATE_COL\", \"PRICE_COL\", \"H\", \"PCT_WINSOR\"]\n",
    "missing = [r for r in req if r not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing required names: {missing}\")\n",
    "\n",
    "# --- copy & sort ---\n",
    "features_df = features_df.copy()\n",
    "features_df[DATE_COL] = pd.to_datetime(features_df[DATE_COL])\n",
    "features_df = features_df.sort_values([SYMBOL_COL, DATE_COL])\n",
    "\n",
    "# --- raw daily log returns (for targets & eval) ---\n",
    "if PRICE_COL not in features_df.columns:\n",
    "    raise ValueError(f\"Missing '{PRICE_COL}' in features_df.\")\n",
    "features_df[\"ret_raw\"] = (\n",
    "    features_df\n",
    "      .groupby(SYMBOL_COL, group_keys=False)[PRICE_COL]\n",
    "      .apply(lambda s: np.log(s.astype(float)).diff())\n",
    ")\n",
    "\n",
    "# --- winsorized returns (for model inputs only; NOT for targets) ---\n",
    "features_df[\"ret_wins\"] = (\n",
    "    features_df\n",
    "      .groupby(SYMBOL_COL, group_keys=False)[\"ret_raw\"]\n",
    "      .apply(lambda s: winsorize_series(s, limits=PCT_WINSOR))\n",
    ")\n",
    "\n",
    "# --- H‑day targets on RAW returns (leakage‑safe) ---\n",
    "# H‑day log return ≈ sum of next H daily log returns = log(P_{t+H}/P_t)\n",
    "features_df[\"target_logret_H\"] = (\n",
    "    features_df\n",
    "      .groupby(SYMBOL_COL)[\"ret_raw\"]\n",
    "      .transform(lambda r: r.shift(-H).rolling(H, min_periods=H).sum())\n",
    ")\n",
    "features_df[\"target_dir_H\"] = (features_df[\"target_logret_H\"] > 0).astype(\"int8\")\n",
    "\n",
    "# --- feature list (drop IDs, targets, raw OHLC; keep adjclose-derived) ---\n",
    "exclude_cols = {SYMBOL_COL, DATE_COL, \"target_logret_H\", \"target_dir_H\", \"ret_raw\"}\n",
    "exclude_cols |= {\"open\", \"high\", \"low\", \"close\"}  # keep adjclose + derived features\n",
    "features = [c for c in features_df.columns\n",
    "            if (c not in exclude_cols) and (not str(c).lower().startswith(\"target\"))]\n",
    "\n",
    "# --- clean known ±inf offenders (set to NaN; impute inside each fold) ---\n",
    "for col in (\"range_expansion_5d\", \"range_expansion_10d\", \"range_x_rvol20\", 'range_expansion_20d'):\n",
    "    if col in features_df.columns:\n",
    "        v = pd.to_numeric(features_df[col], errors=\"coerce\")\n",
    "        features_df.loc[~np.isfinite(v), col] =0\n",
    "\n",
    "# --- final modeling frame: drop rows with missing target only ---\n",
    "model_df = features_df.dropna(subset=[\"target_logret_H\"]).copy()\n",
    "\n",
    "print(\n",
    "    f\"✅ Targets built | H={H} | rows(all)={len(features_df):,} → model_df={len(model_df):,} | features={len(features)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Date span: {features_df[DATE_COL].min().date()} → {features_df[DATE_COL].max().date()} | \"\n",
    "    f\"Symbols: {features_df[SYMBOL_COL].nunique()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "847e9c34-5696-44ec-b0cf-ca98ed9260e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total folds available: 47\n",
      "✅ Using 3 folds with test_start ≥ 2024-08-01 (expanding=True)\n",
      "  Fold 1: train 2020-08-13 → 2025-04-16 | test 2025-04-21 → 2025-05-19 (n_train=2,520,036, n_test=48,037)\n",
      "  Fold 2: train 2020-08-13 → 2025-05-15 | test 2025-05-20 → 2025-06-18 (n_train=2,565,780, n_test=48,196)\n",
      "  Fold 3: train 2020-08-13 → 2025-06-13 | test 2025-06-20 → 2025-07-21 (n_train=2,611,651, n_test=48,426)\n"
     ]
    }
   ],
   "source": [
    "# ==== BUILD & FILTER FOLDS ====\n",
    "# Align fold base to the training base (rows with valid targets only)\n",
    "base_df = features_df.dropna(subset=[\"target_logret_H\"]).copy()\n",
    "\n",
    "all_folds = list(\n",
    "    walk_forward_splits(\n",
    "        base_df,                     # <-- was features_df\n",
    "        date_col=DATE_COL,\n",
    "        min_train_days=MIN_TRAIN_DAYS,\n",
    "        test_days=TEST_DAYS,\n",
    "        horizon=H,\n",
    "        embargo_days=EMBARGO_DAYS,\n",
    "        max_folds=10_000,\n",
    "        expanding=EXPANDING,\n",
    "    )\n",
    ")\n",
    "\n",
    "meta = []\n",
    "for tr_idx, te_idx in all_folds:\n",
    "    tr_dates = base_df.iloc[tr_idx][DATE_COL]\n",
    "    te_dates = base_df.iloc[te_idx][DATE_COL]\n",
    "    meta.append(dict(\n",
    "        tr_idx=tr_idx, te_idx=te_idx,\n",
    "        train_start=tr_dates.min(), train_end=tr_dates.max(),\n",
    "        test_start=te_dates.min(), test_end=te_dates.max(),\n",
    "        n_train=len(tr_idx), n_test=len(te_idx),\n",
    "    ))\n",
    "\n",
    "print(f\"✅ Total folds available: {len(meta)}\")\n",
    "\n",
    "cutoff_date = base_df[DATE_COL].max() - pd.Timedelta(days=RECENT_ONLY_CUTOFF_DAYS)\n",
    "recent_meta = [m for m in meta if m['test_start'] >= cutoff_date]\n",
    "if not recent_meta:\n",
    "    recent_meta = meta[-MAX_FOLDS:]\n",
    "else:\n",
    "    recent_meta = sorted(recent_meta, key=lambda m: m['test_start'])[-MAX_FOLDS:]\n",
    "\n",
    "print(f\"✅ Using {len(recent_meta)} folds with test_start ≥ {cutoff_date.date()} (expanding={EXPANDING})\")\n",
    "for i, f in enumerate(recent_meta, 1):\n",
    "    print(f\"  Fold {i}: train {f['train_start'].date()} → {f['train_end'].date()} | \"\n",
    "          f\"test {f['test_start'].date()} → {f['test_end'].date()} \"\n",
    "          f\"(n_train={f['n_train']:,}, n_test={f['n_test']:,})\")\n",
    "\n",
    "folds = [(f['tr_idx'], f['te_idx']) for f in recent_meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6115e3a-970b-4eb6-8a84-8d090744a27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "066b3a38-d532-4e14-9a5b-eaaed8526324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total possible folds: 3 | Running: 3 (MIN_TRAIN_DAYS=252, TEST_DAYS=21, EMBARGO_DAYS=1, H=5)\n",
      "Fold 01 [2020-08-13→2025-04-16 | 2025-04-21→2025-05-19]  |  [REG] RMSE=0.0906  MAE=0.0604  R^2=-0.1235  Corr=0.1813  ||  [CLS] Acc=0.4781  AUC=0.5926  Prec=0.8031  Rec=0.2325  F1=0.3606\n",
      "Fold 02 [2020-08-13→2025-05-15 | 2025-05-20→2025-06-18]  |  [REG] RMSE=0.0859  MAE=0.0511  R^2=-0.1136  Corr=0.1143  ||  [CLS] Acc=0.4415  AUC=0.4247  Prec=0.0000  Rec=0.0000  F1=0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m X_te \u001b[38;5;241m=\u001b[39m model_df\u001b[38;5;241m.\u001b[39miloc[te_idx][features]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace([np\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf], np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m     48\u001b[0m med \u001b[38;5;241m=\u001b[39m X_tr\u001b[38;5;241m.\u001b[39mmedian(numeric_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 49\u001b[0m X_tr \u001b[38;5;241m=\u001b[39m \u001b[43mX_tr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m X_te \u001b[38;5;241m=\u001b[39m X_te\u001b[38;5;241m.\u001b[39mfillna(med)\n\u001b[1;32m     52\u001b[0m y_reg_tr \u001b[38;5;241m=\u001b[39m model_df\u001b[38;5;241m.\u001b[39miloc[tr_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_logret_H\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/core/frame.py:5220\u001b[0m, in \u001b[0;36mDataFrame.fillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   5209\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5210\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39mfillna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_shared_doc_kwargs)\n\u001b[1;32m   5211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfillna\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5218\u001b[0m     downcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 5220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5223\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/core/generic.py:6485\u001b[0m, in \u001b[0;36mNDFrame.fillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   6483\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   6484\u001b[0m         downcast_k \u001b[38;5;241m=\u001b[39m downcast \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict \u001b[38;5;28;01melse\u001b[39;00m downcast\u001b[38;5;241m.\u001b[39mget(k)\n\u001b[0;32m-> 6485\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m result[k]\u001b[38;5;241m.\u001b[39mfillna(v, limit\u001b[38;5;241m=\u001b[39mlimit, downcast\u001b[38;5;241m=\u001b[39mdowncast_k)\n\u001b[1;32m   6486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6488\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(value):\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/core/frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/core/frame.py:3845\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3842\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(existing_piece, DataFrame):\n\u001b[1;32m   3843\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m-> 3845\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/core/frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis), key, value)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iset_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;66;03m# check if we are modifying a copy\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m \u001b[38;5;66;03m# try to set first as we want an invalid\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;66;03m# value exception to occur first\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/core/frame.py:3794\u001b[0m, in \u001b[0;36mDataFrame._iset_item_mgr\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_iset_item_mgr\u001b[39m(\n\u001b[1;32m   3791\u001b[0m     \u001b[38;5;28mself\u001b[39m, loc: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray, value, inplace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3792\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;66;03m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[39;00m\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/core/internals/managers.py:1141\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[0;34m(self, loc, value, inplace)\u001b[0m\n\u001b[1;32m   1139\u001b[0m             removed_blknos\u001b[38;5;241m.\u001b[39mappend(blkno_l)\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1141\u001b[0m             \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs[blk\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(blk))\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(removed_blknos):\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m# Remove blocks & update blknos accordingly\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/pandas/core/internals/blocks.py:388\u001b[0m, in \u001b[0;36mBlock.delete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mDelete given loc(-s) from block in-place.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Argument 1 to \"delete\" has incompatible type \"Union[ndarray[Any, Any],\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# ExtensionArray]\"; expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Sequence[_SupportsArray[dtype[Any]]], Sequence[Sequence\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# [_SupportsArray[dtype[Any]]]], Sequence[Sequence[Sequence[\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# _SupportsArray[dtype[Any]]]]], Sequence[Sequence[Sequence[Sequence[\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# _SupportsArray[dtype[Any]]]]]]]\"  [arg-type]\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmgr_locs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr_locs\u001b[38;5;241m.\u001b[39mdelete(loc)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/svd_forecasting/lib/python3.10/site-packages/numpy/lib/function_base.py:5359\u001b[0m, in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   5356\u001b[0m     slobj[axis] \u001b[38;5;241m=\u001b[39m keep\n\u001b[1;32m   5357\u001b[0m     new \u001b[38;5;241m=\u001b[39m arr[\u001b[38;5;28mtuple\u001b[39m(slobj)]\n\u001b[0;32m-> 5359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m   5360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap(new)\n\u001b[1;32m   5361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# ---- How many folds to actually train/eval ----\n",
    "MAX_FOLDS_TO_RUN = 3\n",
    "\n",
    "# Same base as fold-building\n",
    "model_df = features_df.dropna(subset=[\"target_logret_H\"]).copy()\n",
    "model_df[DATE_COL] = pd.to_datetime(model_df[DATE_COL])  # dtype only; no reordering\n",
    "\n",
    "# ---- Reuse the prebuilt `folds` ----\n",
    "assert 'folds' in globals(), \"Expected `folds` from the fold-building cell.\"\n",
    "n_rows = len(model_df)\n",
    "\n",
    "# Defensive: keep only splits whose indices fit current model_df\n",
    "safe_splits = []\n",
    "for tr_idx, te_idx in folds:\n",
    "    if len(tr_idx) and len(te_idx) and tr_idx.max() < n_rows and te_idx.max() < n_rows:\n",
    "        safe_splits.append((tr_idx, te_idx))\n",
    "\n",
    "if not safe_splits:\n",
    "    raise RuntimeError(\"No valid folds after alignment. Make sure folds were built on the same frame/order.\")\n",
    "\n",
    "splits = safe_splits[:MAX_FOLDS_TO_RUN]\n",
    "print(f\"Total possible folds: {len(safe_splits)} | Running: {len(splits)} \"\n",
    "      f\"(MIN_TRAIN_DAYS={MIN_TRAIN_DAYS}, TEST_DAYS={TEST_DAYS}, EMBARGO_DAYS={EMBARGO_DAYS}, H={H})\")\n",
    "\n",
    "# ---- Pre-allocate OOF arrays ----\n",
    "oof_reg_preds = np.full(n_rows, np.nan, dtype=float)\n",
    "oof_cls_proba = np.full(n_rows, np.nan, dtype=float)\n",
    "oof_cls_pred  = np.full(n_rows, -1,   dtype=int)\n",
    "\n",
    "fold_summaries = []\n",
    "\n",
    "# ---- Train / evaluate ----\n",
    "for fold_no, (tr_idx, te_idx) in enumerate(splits, start=1):\n",
    "    tr_dates = (model_df.iloc[tr_idx][DATE_COL].min(), model_df.iloc[tr_idx][DATE_COL].max())\n",
    "    te_dates = (model_df.iloc[te_idx][DATE_COL].min(), model_df.iloc[te_idx][DATE_COL].max())\n",
    "\n",
    "    X_tr = model_df.iloc[tr_idx][features].astype(\"float32\").replace([np.inf, -np.inf], np.nan)\n",
    "    X_te = model_df.iloc[te_idx][features].astype(\"float32\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    med = X_tr.median(numeric_only=True)\n",
    "    X_tr = X_tr.fillna(med)\n",
    "    X_te = X_te.fillna(med)\n",
    "\n",
    "    y_reg_tr = model_df.iloc[tr_idx][\"target_logret_H\"].astype(\"float32\").to_numpy()\n",
    "    y_reg_te = model_df.iloc[te_idx][\"target_logret_H\"].astype(\"float32\").to_numpy()\n",
    "    y_cls_tr = model_df.iloc[tr_idx][\"target_dir_H\"].astype(\"int8\").to_numpy()\n",
    "    y_cls_te = model_df.iloc[te_idx][\"target_dir_H\"].astype(\"int8\").to_numpy()\n",
    "\n",
    "    reg = XGBRegressor(**REG_PARAMS).fit(X_tr, y_reg_tr)\n",
    "\n",
    "    pos = int((y_cls_tr == 1).sum()); neg = int((y_cls_tr == 0).sum())\n",
    "    spw = (neg / max(pos, 1)) if pos > 0 else 1.0\n",
    "    cls = XGBClassifier(**CLS_PARAMS, scale_pos_weight=spw).fit(X_tr, y_cls_tr)\n",
    "\n",
    "    y_reg_hat = reg.predict(X_te)\n",
    "    y_cls_pro = cls.predict_proba(X_te)[:, 1]\n",
    "    y_cls_hat = (y_cls_pro >= 0.5).astype(int)\n",
    "\n",
    "    oof_reg_preds[te_idx] = y_reg_hat\n",
    "    oof_cls_proba[te_idx] = y_cls_pro\n",
    "    oof_cls_pred[te_idx]  = y_cls_hat\n",
    "\n",
    "    rmse = root_mean_squared_error(y_reg_te, y_reg_hat)\n",
    "    mae  = mean_absolute_error(y_reg_te, y_reg_hat)\n",
    "    r2   = r2_score(y_reg_te, y_reg_hat)\n",
    "    corr = np.corrcoef(y_reg_te, y_reg_hat)[0, 1] if len(y_reg_te) > 1 else np.nan\n",
    "\n",
    "    acc = accuracy_score(y_cls_te, y_cls_hat)\n",
    "    auc = roc_auc_score(y_cls_te, y_cls_pro) if np.unique(y_cls_te).size == 2 else np.nan\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_cls_te, y_cls_hat, average=\"binary\", zero_division=0)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold_no:02d} \"\n",
    "        f\"[{tr_dates[0].date()}→{tr_dates[1].date()} | {te_dates[0].date()}→{te_dates[1].date()}]  \"\n",
    "        f\"|  [REG] RMSE={rmse:.4f}  MAE={mae:.4f}  R^2={r2:.4f}  Corr={corr:.4f}  \"\n",
    "        f\"||  [CLS] Acc={acc:.4f}  AUC={auc:.4f}  Prec={prec:.4f}  Rec={rec:.4f}  F1={f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    fold_summaries.append(dict(fold=fold_no, rmse=rmse, mae=mae, r2=r2, corr=corr,\n",
    "                               acc=acc, auc=auc, prec=prec, rec=rec, f1=f1))\n",
    "\n",
    "print(f\"\\nRan {len(splits)} folds (out of {len(safe_splits)} possible).\")\n",
    "\n",
    "# ---- OOF summary ----\n",
    "mask_reg = np.isfinite(oof_reg_preds) & model_df[\"target_logret_H\"].notna().values\n",
    "y_true_reg = model_df.loc[mask_reg, \"target_logret_H\"].to_numpy()\n",
    "y_pred_reg = oof_reg_preds[mask_reg]\n",
    "rmse = root_mean_squared_error(y_true_reg, y_pred_reg)\n",
    "mae  = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "r2   = r2_score(y_true_reg, y_pred_reg)\n",
    "corr = np.corrcoef(y_true_reg, y_pred_reg)[0, 1] if len(y_true_reg) > 1 else np.nan\n",
    "\n",
    "mask_cls = np.isfinite(oof_cls_proba) & model_df[\"target_dir_H\"].notna().values\n",
    "y_true_cls = model_df.loc[mask_cls, \"target_dir_H\"].to_numpy().astype(int)\n",
    "y_prob_cls = oof_cls_proba[mask_cls]\n",
    "y_pred_cls = (y_prob_cls >= 0.5).astype(int)\n",
    "acc = accuracy_score(y_true_cls, y_pred_cls)\n",
    "auc = roc_auc_score(y_true_cls, y_prob_cls) if np.unique(y_true_cls).size == 2 else np.nan\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true_cls, y_pred_cls, average=\"binary\", zero_division=0)\n",
    "\n",
    "print(\"\\n==== OOF Summary (evaluated on RAW returns) ====\")\n",
    "print(f\"[REG] H={H}d log-return  | RMSE={rmse:.4f}  MAE={mae:.4f}  R^2={r2:.4f}  Corr={corr:.4f}\")\n",
    "print(f\"[CLS] H={H}d direction   | Acc={acc:.4f}  AUC={auc:.4f}  Prec={prec:.4f}  Rec={rec:.4f}  F1={f1:.4f}\")\n",
    "\n",
    "cm = pd.crosstab(pd.Series(y_true_cls, name=\"Actual\"),\n",
    "                 pd.Series(y_pred_cls, name=\"Predicted\"))\n",
    "print(\"\\nConfusion Matrix (OOF):\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed6540-6de9-4599-877b-af1c9531b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_latest_per_symbol(latest_df: pd.DataFrame,\n",
    "                              feature_cols: List[str],\n",
    "                              reg_model: XGBRegressor,\n",
    "                              cls_model: XGBClassifier,\n",
    "                              train_medians: pd.Series) -> pd.DataFrame:\n",
    "    last_idx = latest_df.groupby(SYMBOL_COL)[DATE_COL].idxmax()\n",
    "    snap = latest_df.loc[last_idx, [SYMBOL_COL, DATE_COL] + feature_cols].copy()\n",
    "    Xsnap = snap[feature_cols].astype(\"float32\").fillna(train_medians)\n",
    "\n",
    "    reg_hat = reg_model.predict(Xsnap)\n",
    "    up_prob = cls_model.predict_proba(Xsnap)[:, 1]\n",
    "    up_lab  = (up_prob >= 0.5).astype(int)\n",
    "\n",
    "    out = snap[[SYMBOL_COL, DATE_COL]].copy()\n",
    "    out[f\"pred_logret_{H}d\"]   = reg_hat\n",
    "    out[f\"pred_up_{H}d_prob\"]  = up_prob\n",
    "    out[f\"pred_up_{H}d\"]       = up_lab\n",
    "    return out.sort_values(f\"pred_logret_{H}d\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Example usage with the last trained fold’s models:\n",
    "# (If you want a production “latest” model, retrain on *all* data up to the latest date.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0f36a-7556-4786-b40a-914b47977121",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52b288-5372-44e3-8eca-dd94d1bb08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Hyperopt is ready. Call:\n",
      "  best_cls, cls_trials = tune_classifier(max_evals=25)\n",
      "  best_reg, reg_trials = tune_regressor(max_evals=25)\n",
      "  8%|██████████▏                                                                                                                    | 2/25 [1:28:14<18:45:17, 2935.56s/trial, best loss: 0.4757726980405964]"
     ]
    }
   ],
   "source": [
    "# === Hyperopt wrapper for XGB (classification OR regression) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# ---- Reuse your existing base (no reordering!) ----\n",
    "assert 'folds' in globals(), \"Expected `folds` from the fold-building cell.\"\n",
    "assert 'model_df' in globals(), \"Expected `model_df` to be defined.\"\n",
    "assert 'features' in globals(), \"Expected `features` to be defined.\"\n",
    "n_rows = len(model_df)\n",
    "\n",
    "# ---- A small, safe subset of folds to keep tuning fast ----\n",
    "MAX_FOLDS_TO_RUN = min(3, len(folds))  # tweak if you want more signal\n",
    "splits = folds[:MAX_FOLDS_TO_RUN]\n",
    "\n",
    "# ---------- CV runner (single evaluation of a param set) ----------\n",
    "def run_cv_once(params, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns: dict with aggregate metrics over folds.\n",
    "    - We always compute both reg + cls metrics so you can optimize either.\n",
    "    - Early stopping uses a time-based split: last EVAL_FRAC of *train fold* as validation.\n",
    "    \"\"\"\n",
    "    reg_metrics, cls_metrics = [], []\n",
    "\n",
    "    # Cast a few here to avoid surprises\n",
    "    n_estimators      = int(params[\"n_estimators\"])\n",
    "    max_depth         = int(params[\"max_depth\"])\n",
    "    min_child_weight  = float(params[\"min_child_weight\"])\n",
    "    learning_rate     = float(params[\"learning_rate\"])\n",
    "    subsample         = float(params[\"subsample\"])\n",
    "    colsample_bytree  = float(params[\"colsample_bytree\"])\n",
    "    reg_lambda        = float(params[\"reg_lambda\"])\n",
    "    gamma             = float(params[\"gamma\"])\n",
    "    early_stopping    = int(params.get(\"early_stopping_rounds\", EARLY_STOP_ROUNDS))\n",
    "    rng_state         = int(params.get(\"random_state\", FIXED_RANDOM_STATE))\n",
    "\n",
    "    for (tr_idx, te_idx) in splits:\n",
    "        # --- time-based val split inside train fold ---\n",
    "        tr_idx_sorted = np.sort(tr_idx)\n",
    "        n_tr = len(tr_idx_sorted)\n",
    "        n_val = max(1, int(n_tr * EVAL_FRAC))\n",
    "        val_idx = tr_idx_sorted[-n_val:]\n",
    "        tr_core_idx = tr_idx_sorted[:-n_val] if n_tr > n_val else tr_idx_sorted\n",
    "\n",
    "        # --- X: sanitize inf -> NaN, then impute with train-core medians only ---\n",
    "        X_tr_core = model_df.iloc[tr_core_idx][features].astype(\"float32\").replace([np.inf, -np.inf], np.nan)\n",
    "        X_val     = model_df.iloc[val_idx][features].astype(\"float32\").replace([np.inf, -np.inf], np.nan)\n",
    "        X_te      = model_df.iloc[te_idx][features].astype(\"float32\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        med = X_tr_core.median(numeric_only=True)\n",
    "        X_tr_core = X_tr_core.fillna(med)\n",
    "        X_val     = X_val.fillna(med)\n",
    "        X_te      = X_te.fillna(med)\n",
    "\n",
    "        # --- y ---\n",
    "        y_reg_tr_core = model_df.iloc[tr_core_idx][\"target_logret_H\"].astype(\"float32\").to_numpy()\n",
    "        y_reg_val     = model_df.iloc[val_idx][\"target_logret_H\"].astype(\"float32\").to_numpy()\n",
    "        y_reg_te      = model_df.iloc[te_idx][\"target_logret_H\"].astype(\"float32\").to_numpy()\n",
    "\n",
    "        y_cls_tr_core = model_df.iloc[tr_core_idx][\"target_dir_H\"].astype(\"int8\").to_numpy()\n",
    "        y_cls_val     = model_df.iloc[val_idx][\"target_dir_H\"].astype(\"int8\").to_numpy()\n",
    "        y_cls_te      = model_df.iloc[te_idx][\"target_dir_H\"].astype(\"int8\").to_numpy()\n",
    "\n",
    "        # --- models (no reg_alpha; tree_method='hist') ---\n",
    "        common = dict(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            min_child_weight=min_child_weight,\n",
    "            reg_lambda=reg_lambda,\n",
    "            gamma=gamma,\n",
    "            tree_method=\"hist\",\n",
    "            random_state=rng_state,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        reg = XGBRegressor(objective=\"reg:squarederror\",eval_metric=\"rmse\", **common)\n",
    "        reg.fit(\n",
    "            X_tr_core, y_reg_tr_core,\n",
    "            eval_set=[(X_val, y_reg_val)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        pos = int((y_cls_tr_core == 1).sum()); neg = int((y_cls_tr_core == 0).sum())\n",
    "        spw = (neg / max(pos, 1)) if pos > 0 else 1.0\n",
    "        cls = XGBClassifier(eval_metric=\"logloss\", scale_pos_weight=spw, **common)\n",
    "        cls.fit(\n",
    "            X_tr_core, y_cls_tr_core,\n",
    "            eval_set=[(X_val, y_cls_val)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # --- predictions & metrics ---\n",
    "        y_reg_hat = reg.predict(X_te)\n",
    "        reg_metrics.append({\n",
    "            \"rmse\": root_mean_squared_error(y_reg_te, y_reg_hat),\n",
    "            \"mae\":  mean_absolute_error(y_reg_te, y_reg_hat),\n",
    "        })\n",
    "\n",
    "        y_cls_pro = cls.predict_proba(X_te)[:, 1]\n",
    "        y_cls_hat = (y_cls_pro >= 0.5).astype(int)\n",
    "        auc = roc_auc_score(y_cls_te, y_cls_pro) if np.unique(y_cls_te).size == 2 else np.nan\n",
    "        cls_metrics.append({\n",
    "            \"acc\": accuracy_score(y_cls_te, y_cls_hat),\n",
    "            \"auc\": auc,\n",
    "        })\n",
    "\n",
    "    # Aggregate\n",
    "    mean_rmse = float(np.mean([m[\"rmse\"] for m in reg_metrics]))\n",
    "    mean_mae  = float(np.mean([m[\"mae\"]  for m in reg_metrics]))\n",
    "    auc_vals  = [m[\"auc\"] for m in cls_metrics if np.isfinite(m[\"auc\"])]\n",
    "    mean_auc  = float(np.mean(auc_vals)) if auc_vals else np.nan\n",
    "    mean_acc  = float(np.mean([m[\"acc\"] for m in cls_metrics]))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[CV] rmse={mean_rmse:.5f}  mae={mean_mae:.5f}  auc={mean_auc:.5f}  acc={mean_acc:.5f}\")\n",
    "\n",
    "    return dict(mean_rmse=mean_rmse, mean_mae=mean_mae, mean_auc=mean_auc, mean_acc=mean_acc)\n",
    "\n",
    "# ---------- Hyperopt objective ----------\n",
    "def hyperopt_objective(params, model_type=\"cls\"):\n",
    "    metrics = run_cv_once(params, verbose=False)\n",
    "    if model_type == \"cls\":\n",
    "        # Maximize AUC -> minimize 1 - AUC; fallback to 1 - Acc if AUC NaN\n",
    "        loss = (1.0 - metrics[\"mean_auc\"]) if np.isfinite(metrics[\"mean_auc\"]) else (1.0 - metrics[\"mean_acc\"])\n",
    "    elif model_type == \"reg\":\n",
    "        # Minimize RMSE\n",
    "        loss = metrics[\"mean_rmse\"]\n",
    "    else:\n",
    "        # Joint objective (optional): rmse + (1 - auc)\n",
    "        loss = metrics[\"mean_rmse\"] + (1.0 - metrics[\"mean_auc\"] if np.isfinite(metrics[\"mean_auc\"]) else 0.5*(1.0 - metrics[\"mean_acc\"]))\n",
    "    return {\"loss\": float(loss), \"status\": STATUS_OK, \"metrics\": metrics}\n",
    "\n",
    "# ---------- Search spaces (no reg_alpha) ----------\n",
    "space_common = {\n",
    "    \"n_estimators\":     scope.int(hp.quniform(\"n_estimators\", 300, 1200, 50)),\n",
    "    \"max_depth\":        scope.int(hp.quniform(\"max_depth\", 4, 10, 1)),\n",
    "    \"learning_rate\":    hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.20)),\n",
    "    \"subsample\":        hp.uniform(\"subsample\", 0.5, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "    \"min_child_weight\": hp.loguniform(\"min_child_weight\", np.log(1e-1), np.log(10.0)),\n",
    "    \"reg_lambda\":       hp.loguniform(\"reg_lambda\", np.log(1e-2), np.log(10.0)),\n",
    "    \"gamma\":            hp.loguniform(\"gamma\", np.log(1e-8), np.log(1.0)),\n",
    "    \"early_stopping_rounds\": scope.int(hp.quniform(\"early_stopping_rounds\", 30, 100, 10)),\n",
    "    \"random_state\":     42,   # keep fixed for repeatability\n",
    "}\n",
    "\n",
    "# ---------- Convenience runners ----------\n",
    "def tune_classifier(max_evals=25):\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=lambda p: hyperopt_objective(p, model_type=\"cls\"),\n",
    "        space=space_common,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(123),   # ✅ avoids RandomState.integers error\n",
    "        show_progressbar=True,\n",
    "    )\n",
    "    # Cast ints cleanly and attach fixed bits\n",
    "    best = {\n",
    "        \"n_estimators\": int(best[\"n_estimators\"]),\n",
    "        \"max_depth\": int(best[\"max_depth\"]),\n",
    "        \"learning_rate\": float(best[\"learning_rate\"]),\n",
    "        \"subsample\": float(best[\"subsample\"]),\n",
    "        \"colsample_bytree\": float(best[\"colsample_bytree\"]),\n",
    "        \"min_child_weight\": float(best[\"min_child_weight\"]),\n",
    "        \"reg_lambda\": float(best[\"reg_lambda\"]),\n",
    "        \"gamma\": float(best[\"gamma\"]),\n",
    "        \"early_stopping_rounds\": int(best[\"early_stopping_rounds\"]),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"eval_metric\": \"auc\",\n",
    "    }\n",
    "    return best, trials\n",
    "\n",
    "def tune_regressor(max_evals=25):\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=lambda p: hyperopt_objective(p, model_type=\"reg\"),\n",
    "        space=space_common,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(321),   # ✅\n",
    "        show_progressbar=True,\n",
    "    )\n",
    "    best = {\n",
    "        \"n_estimators\": int(best[\"n_estimators\"]),\n",
    "        \"max_depth\": int(best[\"max_depth\"]),\n",
    "        \"learning_rate\": float(best[\"learning_rate\"]),\n",
    "        \"subsample\": float(best[\"subsample\"]),\n",
    "        \"colsample_bytree\": float(best[\"colsample_bytree\"]),\n",
    "        \"min_child_weight\": float(best[\"min_child_weight\"]),\n",
    "        \"reg_lambda\": float(best[\"reg_lambda\"]),\n",
    "        \"gamma\": float(best[\"gamma\"]),\n",
    "        \"early_stopping_rounds\": int(best[\"early_stopping_rounds\"]),\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "    }\n",
    "    return best, trials\n",
    "\n",
    "print(\"✔ Hyperopt is ready. Call:\")\n",
    "print(\"  best_cls, cls_trials = tune_classifier(max_evals=25)\")\n",
    "print(\"  best_reg, reg_trials = tune_regressor(max_evals=25)\")\n",
    "#---- Example usage (pick ONE to run) ----\n",
    "best_cls_params, cls_trials = tune_classifier(max_evals=25)\n",
    "print(\"Best CLS params:\", best_cls_params)\n",
    "# best_reg_params, reg_trials = tune_regressor(max_evals=25)\n",
    "# print(\"Best REG params:\", best_reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ad8f59f-18e8-40b4-a03a-5c45f701d0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de745a-bc55-4d69-b1cd-b7dc0a2c777a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (svd_forecasting)",
   "language": "python",
   "name": "svd_forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
