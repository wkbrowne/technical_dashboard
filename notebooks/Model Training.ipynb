{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3682b8f9-a89a-47b6-8579-ee19310bd2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded features: (2904927, 147)\n",
      "Columns: ['volume', 'low', 'adjclose', 'high', 'open', 'close', 'ret', 'ma_10', 'pct_slope_ma_10', 'sign_ma_10', 'ma_20', 'pct_slope_ma_20', 'sign_ma_20', 'ma_30', 'pct_slope_ma_30', 'sign_ma_30', 'ma_50', 'pct_slope_ma_50', 'sign_ma_50', 'ma_75', 'pct_slope_ma_75', 'sign_ma_75', 'ma_100', 'pct_slope_ma_100', 'sign_ma_100', 'ma_150', 'pct_slope_ma_150', 'sign_ma_150', 'ma_200', 'pct_slope_ma_200', 'sign_ma_200', 'trend_score_granular', 'trend_score_sign', 'trend_score_slope', 'trend_persist_ema', 'trend_alignment', 'rv_10', 'rv_20', 'rv_60', 'rv_100', 'rv_ratio_10_60', 'rv_ratio_20_100', 'vol_regime', 'vol_regime_ema10', 'rv_z_60', 'vol_of_vol_20d', 'rv60_slope_norm', 'rv100_slope_norm', 'quiet_trend', 'hurst_ret_64', 'hurst_ret_128', 'hurst_ret_64_emaHL5', 'pct_dist_ma_20', 'pct_dist_ma_20_z', 'pct_dist_ma_50', 'pct_dist_ma_50_z', 'pct_dist_ma_100', 'pct_dist_ma_100_z', 'pct_dist_ma_200', 'pct_dist_ma_200_z', 'min_pct_dist_ma', 'relative_dist_20_50', 'relative_dist_20_50_z', 'hl_range', 'hl_range_pct_close', 'true_range', 'tr_pct_close', 'atr_percent', 'gap_pct', 'gap_atr_ratio', '5d_high', '5d_low', '5d_range', '5d_range_pct_close', 'pos_in_5d_range', 'breakout_up_5d', 'breakout_dn_5d', 'range_expansion_5d', 'range_z_5d', '10d_high', '10d_low', '10d_range', '10d_range_pct_close', 'pos_in_10d_range', 'breakout_up_10d', 'breakout_dn_10d', 'range_expansion_10d', 'range_z_10d', '20d_high', '20d_low', '20d_range', '20d_range_pct_close', 'pos_in_20d_range', 'breakout_up_20d', 'breakout_dn_20d', 'range_expansion_20d', 'range_z_20d', 'range_x_rvol20', 'vol_ma_20', 'vol_ma_50', 'vol_z_20', 'vol_z_60', 'rvol_20', 'rvol_50', 'dollar_vol_ma_20', 'rdollar_vol_20', 'obv', 'obv_z_60', 'vol_rolling_20d', 'vol_rolling_60d', 'volshock_z', 'volshock_dir', 'volshock_ema', 'vol_regime_cs_median', 'vol_regime_rel', 'alpha_resid_spy', 'alpha_mom_spy_ema10', 'alpha_mom_spy_20_ema10', 'alpha_mom_spy_60_ema10', 'alpha_mom_spy_120_ema10', 'alpha_resid_sector', 'alpha_mom_sector_ema10', 'alpha_mom_sector_20_ema10', 'alpha_mom_sector_60_ema10', 'alpha_mom_sector_120_ema10', 'alpha_mom_combo_ema10', 'alpha_mom_combo_20_ema10', 'alpha_mom_combo_60_ema10', 'alpha_mom_combo_120_ema10', 'rel_strength_spy', 'rel_strength_spy_norm', 'rel_strength_spy_slope20', 'rel_strength_sector', 'rel_strength_sector_norm', 'rel_strength_sector_slope20', 'pct_universe_above_ma20', 'pct_universe_above_ma50', 'pct_universe_above_ma200', 'ad_line_universe', 'xsec_mom_5d_z', 'xsec_mom_5d_sect_neutral_z', 'xsec_mom_20d_z', 'xsec_mom_20d_sect_neutral_z', 'xsec_mom_60d_z', 'xsec_mom_60d_sect_neutral_z', 'symbol', 'date']\n",
      "Date range: 2020-08-07 00:00:00 to 2025-08-08 00:00:00\n",
      "Unique symbols: 2311\n",
      "Sample data:\n",
      "           volume    low  adjclose   high   open  close       ret   ma_10  \\\n",
      "2904922  290900.0  12.36     12.60  12.72  12.65  12.60  0.002384  13.359   \n",
      "2904923  381400.0  12.31     12.54  13.19  12.60  12.54 -0.004773  13.181   \n",
      "2904924  765800.0  11.93     12.41  12.46  12.44  12.41 -0.010421  12.993   \n",
      "2904925  393614.0  11.96     12.16  12.59  12.39  12.16 -0.020351  12.808   \n",
      "2904926       NaN    NaN       NaN    NaN    NaN    NaN       NaN  12.750   \n",
      "\n",
      "         pct_slope_ma_10  sign_ma_10  ...  pct_universe_above_ma200  \\\n",
      "2904922         0.053466         1.0  ...                 59.780224   \n",
      "2904923         0.035509         1.0  ...                 58.241760   \n",
      "2904924         0.014761         1.0  ...                 56.483521   \n",
      "2904925        -0.007055        -1.0  ...                 57.362640   \n",
      "2904926        -0.018929        -1.0  ...                  0.000000   \n",
      "\n",
      "         ad_line_universe  xsec_mom_5d_z  xsec_mom_5d_sect_neutral_z  \\\n",
      "2904922           21507.0      -0.499898                   -0.571028   \n",
      "2904923           21434.0      -0.718440                   -0.825237   \n",
      "2904924           21429.0      -0.381271                   -0.273184   \n",
      "2904925           21389.0      -0.320635                   -0.248400   \n",
      "2904926           21389.0      -0.560794                   -0.392429   \n",
      "\n",
      "         xsec_mom_20d_z  xsec_mom_20d_sect_neutral_z  xsec_mom_60d_z  \\\n",
      "2904922       -0.223625                    -0.392394        0.070684   \n",
      "2904923       -0.145231                    -0.283620        0.111075   \n",
      "2904924       -0.480540                    -0.384715        0.088016   \n",
      "2904925       -0.642725                    -0.563518       -0.095567   \n",
      "2904926       -0.573139                    -0.557522        0.052044   \n",
      "\n",
      "         xsec_mom_60d_sect_neutral_z  symbol       date  \n",
      "2904922                    -0.038423    ZYME 2025-08-04  \n",
      "2904923                     0.032107    ZYME 2025-08-05  \n",
      "2904924                     0.025273    ZYME 2025-08-06  \n",
      "2904925                    -0.125627    ZYME 2025-08-07  \n",
      "2904926                    -0.089641    ZYME 2025-08-08  \n",
      "\n",
      "[5 rows x 147 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the features parquet file\n",
    "import pandas as pd\n",
    "\n",
    "features_df = pd.read_parquet('../artifacts/features_long.parquet')\n",
    "print(f\"✅ Loaded features: {features_df.shape}\")\n",
    "print(f\"Columns: {list(features_df.columns)}\")\n",
    "print(f\"Date range: {features_df['date'].min()} to {features_df['date'].max()}\")\n",
    "print(f\"Unique symbols: {features_df['symbol'].nunique()}\")\n",
    "print(f\"Sample data:\")\n",
    "print(features_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1587a234-d933-4530-bcf8-043a49afd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "from typing import Iterator, Tuple\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error, \\\n",
    "            mean_absolute_error, accuracy_score, precision_recall_fscore_support,roc_auc_score\n",
    "\n",
    "def winsorize_series(s: pd.Series, limits=(0.01, 0.01)) -> pd.Series:\n",
    "    mask = s.notna()\n",
    "    if not mask.any():\n",
    "        return s.copy()\n",
    "    arr_w = winsorize(s[mask].astype(float).to_numpy(), limits=limits)\n",
    "    out = pd.Series(np.nan, index=s.index)\n",
    "    out.loc[mask] = arr_w\n",
    "    return out\n",
    "def walk_forward_splits(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"date\",\n",
    "    min_train_days: int = 252,\n",
    "    test_days: int = 63,\n",
    "    horizon: int = None,     # default to global H if available\n",
    "    embargo_days: int = 1,\n",
    ") -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Time-based folds with purge (horizon-1) + embargo (embargo_days).\n",
    "    Splits are by unique dates across the whole panel.\n",
    "    \"\"\"\n",
    "    if horizon is None:\n",
    "        horizon = globals().get(\"H\", 10)\n",
    "    dates = pd.to_datetime(df[date_col]).sort_values().unique()\n",
    "    start = 0\n",
    "    while True:\n",
    "        train_end = start + min_train_days\n",
    "        test_end  = train_end + test_days\n",
    "        if test_end > len(dates):\n",
    "            break\n",
    "\n",
    "        train_dates = dates[:train_end]\n",
    "        test_dates  = dates[train_end:test_end]\n",
    "\n",
    "        test_start_date = test_dates[0]\n",
    "        test_end_date   = test_dates[-1]\n",
    "\n",
    "        # purge + embargo windows\n",
    "        purge_start = test_start_date - np.timedelta64(horizon - 1, 'D')\n",
    "        purge_end   = test_end_date\n",
    "        embargo_end = test_end_date + np.timedelta64(embargo_days, 'D')\n",
    "\n",
    "        d = pd.to_datetime(df[date_col]).values\n",
    "        in_base_train = (d <= train_dates[-1])\n",
    "        in_purge      = (d >= purge_start) & (d <= purge_end)\n",
    "        in_embargo    = (d > test_end_date) & (d <= embargo_end)\n",
    "        in_test       = (d >= test_start_date) & (d <= test_end_date)\n",
    "\n",
    "        train_mask = in_base_train & (~in_purge) & (~in_embargo) & (~in_test)\n",
    "        test_mask  = in_test\n",
    "\n",
    "        tr_idx = np.flatnonzero(train_mask)\n",
    "        te_idx = np.flatnonzero(test_mask)\n",
    "        if tr_idx.size and te_idx.size:\n",
    "            yield tr_idx, te_idx\n",
    "\n",
    "        start += test_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6115e3a-970b-4eb6-8a84-8d090744a27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaN_count</th>\n",
       "      <th>Inf_count</th>\n",
       "      <th>Total_rows</th>\n",
       "      <th>NaN_pct</th>\n",
       "      <th>Inf_pct</th>\n",
       "      <th>Max_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha_mom_combo_120_ema10</th>\n",
       "      <td>625110</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>21.518957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.518957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_sector_120_ema10</th>\n",
       "      <td>625110</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>21.518957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.518957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_sector_60_ema10</th>\n",
       "      <td>582965</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>20.068146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.068146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_combo_60_ema10</th>\n",
       "      <td>582965</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>20.068146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.068146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_combo_20_ema10</th>\n",
       "      <td>553425</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>19.051253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.051253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_sector_20_ema10</th>\n",
       "      <td>553425</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>19.051253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.051253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_resid_sector</th>\n",
       "      <td>544993</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>18.760988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.760988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_sector_ema10</th>\n",
       "      <td>542875</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>18.688077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.688077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_combo_ema10</th>\n",
       "      <td>542875</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>18.688077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.688077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_slope_ma_200</th>\n",
       "      <td>470991</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>16.213523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.213523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_dist_ma_200_z</th>\n",
       "      <td>470975</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>16.212972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.212972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rel_strength_sector_slope20</th>\n",
       "      <td>460160</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>15.840673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.840673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rel_strength_sector_norm</th>\n",
       "      <td>458030</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>15.767350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.767350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_spy_120_ema10</th>\n",
       "      <td>428376</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.746532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.746532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_dist_ma_200</th>\n",
       "      <td>427628</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.720783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.720783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ma_200</th>\n",
       "      <td>425362</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.642778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.642778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rel_strength_sector</th>\n",
       "      <td>417517</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.372719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.372719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_slope_ma_150</th>\n",
       "      <td>413952</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>14.249997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.249997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_mom_spy_60_ema10</th>\n",
       "      <td>382711</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>13.174548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.174548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ma_150</th>\n",
       "      <td>368242</td>\n",
       "      <td>0</td>\n",
       "      <td>2904927</td>\n",
       "      <td>12.676463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.676463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             NaN_count  Inf_count  Total_rows    NaN_pct  \\\n",
       "alpha_mom_combo_120_ema10       625110          0     2904927  21.518957   \n",
       "alpha_mom_sector_120_ema10      625110          0     2904927  21.518957   \n",
       "alpha_mom_sector_60_ema10       582965          0     2904927  20.068146   \n",
       "alpha_mom_combo_60_ema10        582965          0     2904927  20.068146   \n",
       "alpha_mom_combo_20_ema10        553425          0     2904927  19.051253   \n",
       "alpha_mom_sector_20_ema10       553425          0     2904927  19.051253   \n",
       "alpha_resid_sector              544993          0     2904927  18.760988   \n",
       "alpha_mom_sector_ema10          542875          0     2904927  18.688077   \n",
       "alpha_mom_combo_ema10           542875          0     2904927  18.688077   \n",
       "pct_slope_ma_200                470991          0     2904927  16.213523   \n",
       "pct_dist_ma_200_z               470975          0     2904927  16.212972   \n",
       "rel_strength_sector_slope20     460160          0     2904927  15.840673   \n",
       "rel_strength_sector_norm        458030          0     2904927  15.767350   \n",
       "alpha_mom_spy_120_ema10         428376          0     2904927  14.746532   \n",
       "pct_dist_ma_200                 427628          0     2904927  14.720783   \n",
       "ma_200                          425362          0     2904927  14.642778   \n",
       "rel_strength_sector             417517          0     2904927  14.372719   \n",
       "pct_slope_ma_150                413952          0     2904927  14.249997   \n",
       "alpha_mom_spy_60_ema10          382711          0     2904927  13.174548   \n",
       "ma_150                          368242          0     2904927  12.676463   \n",
       "\n",
       "                             Inf_pct    Max_pct  \n",
       "alpha_mom_combo_120_ema10        0.0  21.518957  \n",
       "alpha_mom_sector_120_ema10       0.0  21.518957  \n",
       "alpha_mom_sector_60_ema10        0.0  20.068146  \n",
       "alpha_mom_combo_60_ema10         0.0  20.068146  \n",
       "alpha_mom_combo_20_ema10         0.0  19.051253  \n",
       "alpha_mom_sector_20_ema10        0.0  19.051253  \n",
       "alpha_resid_sector               0.0  18.760988  \n",
       "alpha_mom_sector_ema10           0.0  18.688077  \n",
       "alpha_mom_combo_ema10            0.0  18.688077  \n",
       "pct_slope_ma_200                 0.0  16.213523  \n",
       "pct_dist_ma_200_z                0.0  16.212972  \n",
       "rel_strength_sector_slope20      0.0  15.840673  \n",
       "rel_strength_sector_norm         0.0  15.767350  \n",
       "alpha_mom_spy_120_ema10          0.0  14.746532  \n",
       "pct_dist_ma_200                  0.0  14.720783  \n",
       "ma_200                           0.0  14.642778  \n",
       "rel_strength_sector              0.0  14.372719  \n",
       "pct_slope_ma_150                 0.0  14.249997  \n",
       "alpha_mom_spy_60_ema10           0.0  13.174548  \n",
       "ma_150                           0.0  12.676463  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Only numeric columns for inf check\n",
    "numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "nan_counts = features_df.isna().sum()\n",
    "inf_counts = pd.Series(0, index=features_df.columns)\n",
    "\n",
    "inf_counts[numeric_cols] = np.isinf(features_df[numeric_cols].to_numpy()).sum(axis=0)\n",
    "\n",
    "nan_inf_summary = pd.DataFrame({\n",
    "    \"NaN_count\": nan_counts,\n",
    "    \"Inf_count\": inf_counts,\n",
    "    \"Total_rows\": len(features_df)\n",
    "})\n",
    "\n",
    "nan_inf_summary[\"NaN_pct\"] = nan_inf_summary[\"NaN_count\"] / nan_inf_summary[\"Total_rows\"] * 100\n",
    "nan_inf_summary[\"Inf_pct\"] = nan_inf_summary[\"Inf_count\"] / nan_inf_summary[\"Total_rows\"] * 100\n",
    "\n",
    "nan_inf_summary.sort_values([\"NaN_count\", \"Inf_count\"], ascending=False)\n",
    "# Filter to top offenders by max percentage of NaNs/Infs\n",
    "top_nan_inf = (\n",
    "    nan_inf_summary\n",
    "    .assign(Max_pct=lambda df: df[[\"NaN_pct\", \"Inf_pct\"]].max(axis=1))\n",
    "    .sort_values(\"Max_pct\", ascending=False)\n",
    ")\n",
    "\n",
    "# Show top 20 offenders\n",
    "top_nan_inf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03167484-d484-4180-889f-b60ad36888e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Built 3 folds (max=3, expanding=True)\n",
      "  Fold 1: train 2021-03-01 → 2022-02-23 | test 2022-02-28 → 2022-05-26 (n_train=449,224, n_test=120,277)\n",
      "  Fold 2: train 2021-03-01 → 2022-05-20 | test 2022-05-27 → 2022-08-26 (n_train=565,566, n_test=122,647)\n",
      "  Fold 3: train 2021-03-01 → 2022-08-24 | test 2022-08-29 → 2022-11-25 (n_train=692,036, n_test=123,501)\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Fold controls ----\n",
    "MAX_FOLDS   = 3        # <= your request\n",
    "EXPANDING   = True     # expanding train window; set False for fixed-length rolling\n",
    "\n",
    "def walk_forward_splits(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    min_train_days: int,\n",
    "    test_days: int,\n",
    "    horizon: int,\n",
    "    embargo_days: int = 1,\n",
    "    max_folds: int = 3,\n",
    "    expanding: bool = True,\n",
    ") -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Time-based folds with purge (horizon-1) + embargo.\n",
    "    Splits are formed on UNIQUE DATES across the whole panel.\n",
    "    If expanding=True, the training window grows each fold.\n",
    "    \"\"\"\n",
    "    dates = pd.to_datetime(df[date_col]).sort_values().unique()\n",
    "    n_dates = len(dates)\n",
    "    k = 0\n",
    "    start = 0\n",
    "\n",
    "    while True:\n",
    "        train_end = start + min_train_days              # index into `dates` (exclusive of test)\n",
    "        test_end  = train_end + test_days\n",
    "        if test_end > n_dates or (k >= max_folds):\n",
    "            break\n",
    "\n",
    "        # Expanding vs fixed train window\n",
    "        if expanding:\n",
    "            # train from 0 .. train_end-1\n",
    "            train_dates = dates[:train_end]\n",
    "        else:\n",
    "            # fixed-length rolling window: start .. train_end-1\n",
    "            train_dates = dates[start:train_end]\n",
    "\n",
    "        test_dates = dates[train_end:test_end]\n",
    "\n",
    "        test_start_date = test_dates[0]\n",
    "        test_end_date   = test_dates[-1]\n",
    "\n",
    "        # Purge window: anything whose target overlaps the test period\n",
    "        purge_start = test_start_date - np.timedelta64(horizon - 1, \"D\")\n",
    "        purge_end   = test_end_date\n",
    "        embargo_end = test_end_date + np.timedelta64(embargo_days, \"D\")\n",
    "\n",
    "        d = pd.to_datetime(df[date_col]).values\n",
    "        in_base_train = (d <= train_dates[-1]) if expanding else ((d >= train_dates[0]) & (d <= train_dates[-1]))\n",
    "        in_purge      = (d >= purge_start) & (d <= purge_end)\n",
    "        in_embargo    = (d >  test_end_date) & (d <= embargo_end)\n",
    "        in_test       = (d >= test_start_date) & (d <= test_end_date)\n",
    "\n",
    "        train_mask = in_base_train & (~in_purge) & (~in_embargo) & (~in_test)\n",
    "        test_mask  = in_test\n",
    "\n",
    "        tr_idx = np.flatnonzero(train_mask)\n",
    "        te_idx = np.flatnonzero(test_mask)\n",
    "\n",
    "        if len(tr_idx) and len(te_idx):\n",
    "            yield tr_idx, te_idx\n",
    "            k += 1\n",
    "\n",
    "        # advance the window by one test block\n",
    "        start += test_days\n",
    "\n",
    "# ---- Build just a few strong folds ----\n",
    "folds = list(\n",
    "    walk_forward_splits(\n",
    "        model_df,\n",
    "        date_col=DATE_COL,\n",
    "        min_train_days=MIN_TRAIN_DAYS,\n",
    "        test_days=TEST_DAYS,\n",
    "        horizon=H,\n",
    "        embargo_days=EMBARGO_DAYS,\n",
    "        max_folds=MAX_FOLDS,\n",
    "        expanding=EXPANDING,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"✅ Built {len(folds)} folds (max={MAX_FOLDS}, expanding={EXPANDING})\")\n",
    "for i, (tr, te) in enumerate(folds, 1):\n",
    "    tr_dates = (model_df.iloc[tr][DATE_COL].min(), model_df.iloc[tr][DATE_COL].max())\n",
    "    te_dates = (model_df.iloc[te][DATE_COL].min(), model_df.iloc[te][DATE_COL].max())\n",
    "    print(f\"  Fold {i}: train {tr_dates[0].date()} → {tr_dates[1].date()} \"\n",
    "          f\"| test {te_dates[0].date()} → {te_dates[1].date()} \"\n",
    "          f\"(n_train={len(tr):,}, n_test={len(te):,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b3a38-d532-4e14-9a5b-eaaed8526324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total possible folds: 3 | Running: 3 (MIN_TRAIN_DAYS=252, TEST_DAYS=63, EMBARGO_DAYS=1, H=5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# ---- Config for limiting folds ----\n",
    "MAX_FOLDS_TO_RUN = 3  # cap the number of folds we actually train/evaluate\n",
    "\n",
    "# ---- Pre-allocate OOF containers ----\n",
    "oof_reg_preds = np.full(len(model_df), np.nan, dtype=float)\n",
    "oof_cls_proba = np.full(len(model_df), np.nan, dtype=float)\n",
    "oof_cls_pred  = np.full(len(model_df), -1,   dtype=int)\n",
    "\n",
    "fold_summaries = []\n",
    "\n",
    "# ---- Build all possible splits first, then cap to MAX_FOLDS_TO_RUN ----\n",
    "all_splits = list(\n",
    "    walk_forward_splits(\n",
    "        model_df, date_col=DATE_COL,\n",
    "        min_train_days=MIN_TRAIN_DAYS,\n",
    "        test_days=TEST_DAYS,\n",
    "        horizon=H,\n",
    "        embargo_days=EMBARGO_DAYS\n",
    "    )\n",
    ")\n",
    "total_possible_folds = len(all_splits)\n",
    "splits = all_splits[:MAX_FOLDS_TO_RUN]\n",
    "\n",
    "print(f\"Total possible folds: {total_possible_folds} | Running: {len(splits)} \"\n",
    "      f\"(MIN_TRAIN_DAYS={MIN_TRAIN_DAYS}, TEST_DAYS={TEST_DAYS}, EMBARGO_DAYS={EMBARGO_DAYS}, H={H})\")\n",
    "\n",
    "# ---- Walk-forward training/eval ----\n",
    "for fold_no, (tr_idx, te_idx) in enumerate(splits, start=1):\n",
    "    X_tr = model_df.iloc[tr_idx][features].astype(\"float32\")\n",
    "    X_te = model_df.iloc[te_idx][features].astype(\"float32\")\n",
    "\n",
    "    # Train‑set medians only (no leakage)\n",
    "    med = X_tr.median(numeric_only=True)\n",
    "    X_tr = X_tr.fillna(med)\n",
    "    X_te = X_te.fillna(med)\n",
    "\n",
    "    y_reg_tr = model_df.iloc[tr_idx][\"target_logret_H\"].astype(\"float32\").to_numpy()\n",
    "    y_reg_te = model_df.iloc[te_idx][\"target_logret_H\"].astype(\"float32\").to_numpy()\n",
    "\n",
    "    y_cls_tr = model_df.iloc[tr_idx][\"target_dir_H\"].astype(\"int8\").to_numpy()\n",
    "    y_cls_te = model_df.iloc[te_idx][\"target_dir_H\"].astype(\"int8\").to_numpy()\n",
    "\n",
    "    # Models\n",
    "    reg = XGBRegressor(**REG_PARAMS)\n",
    "    reg.fit(X_tr, y_reg_tr)\n",
    "\n",
    "    pos = int((y_cls_tr == 1).sum())\n",
    "    neg = int((y_cls_tr == 0).sum())\n",
    "    spw = (neg / max(pos, 1)) if pos > 0 else 1.0\n",
    "\n",
    "    cls = XGBClassifier(**CLS_PARAMS, scale_pos_weight=spw)\n",
    "    cls.fit(X_tr, y_cls_tr)\n",
    "\n",
    "    # Predictions\n",
    "    y_reg_hat = reg.predict(X_te)\n",
    "    y_cls_pro = cls.predict_proba(X_te)[:, 1]\n",
    "    y_cls_hat = (y_cls_pro >= 0.5).astype(int)\n",
    "\n",
    "    # Store OOF\n",
    "    oof_reg_preds[te_idx] = y_reg_hat\n",
    "    oof_cls_proba[te_idx] = y_cls_pro\n",
    "    oof_cls_pred[te_idx]  = y_cls_hat\n",
    "\n",
    "    # Fold metrics (evaluated on RAW targets)\n",
    "    rmse = root_mean_squared_error(y_reg_te, y_reg_hat)  # ✅ no FutureWarning\n",
    "    mae  = mean_absolute_error(y_reg_te, y_reg_hat)\n",
    "    r2   = r2_score(y_reg_te, y_reg_hat)\n",
    "    corr = np.corrcoef(y_reg_te, y_reg_hat)[0, 1] if len(y_reg_te) > 1 else np.nan\n",
    "\n",
    "    acc = accuracy_score(y_cls_te, y_cls_hat)\n",
    "    auc = roc_auc_score(y_cls_te, y_cls_pro)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_cls_te, y_cls_hat, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold_no:02d}  |  [REG] RMSE={rmse:.4f}  MAE={mae:.4f}  R^2={r2:.4f}  Corr={corr:.4f}  \"\n",
    "        f\"||  [CLS] Acc={acc:.4f}  AUC={auc:.4f}  Prec={prec:.4f}  Rec={rec:.4f}  F1={f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    fold_summaries.append(dict(\n",
    "        fold=fold_no, rmse=rmse, mae=mae, r2=r2, corr=corr,\n",
    "        acc=acc, auc=auc, prec=prec, rec=rec, f1=f1\n",
    "    ))\n",
    "\n",
    "print(f\"\\nRan {len(splits)} folds (out of {total_possible_folds} possible).\")\n",
    "\n",
    "# ------- Aggregate OOF diagnostics (full backtest) -------\n",
    "mask_reg = np.isfinite(oof_reg_preds) & model_df[\"target_logret_H\"].notna().values\n",
    "y_true_reg = model_df.loc[mask_reg, \"target_logret_H\"].to_numpy()\n",
    "y_pred_reg = oof_reg_preds[mask_reg]\n",
    "\n",
    "rmse = root_mean_squared_error(y_true_reg, y_pred_reg)  # ✅ new API\n",
    "mae  = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "r2   = r2_score(y_true_reg, y_pred_reg)\n",
    "corr = np.corrcoef(y_true_reg, y_pred_reg)[0, 1] if len(y_true_reg) > 1 else np.nan\n",
    "\n",
    "mask_cls = np.isfinite(oof_cls_proba) & model_df[\"target_dir_H\"].notna().values\n",
    "y_true_cls = model_df.loc[mask_cls, \"target_dir_H\"].to_numpy().astype(int)\n",
    "y_prob_cls = oof_cls_proba[mask_cls]\n",
    "y_pred_cls = (y_prob_cls >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_true_cls, y_pred_cls)\n",
    "auc = roc_auc_score(y_true_cls, y_prob_cls)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true_cls, y_pred_cls, average=\"binary\", zero_division=0)\n",
    "\n",
    "print(\"\\n==== OOF Summary (evaluated on RAW returns) ====\")\n",
    "print(f\"[REG] H={H}d log-return  | RMSE={rmse:.4f}  MAE={mae:.4f}  R^2={r2:.4f}  Corr={corr:.4f}\")\n",
    "print(f\"[CLS] H={H}d direction   | Acc={acc:.4f}  AUC={auc:.4f}  Prec={prec:.4f}  Rec={rec:.4f}  F1={f1:.4f}\")\n",
    "\n",
    "# Optional: confusion matrix for sign\n",
    "cm = pd.crosstab(pd.Series(y_true_cls, name=\"Actual\"),\n",
    "                 pd.Series(y_pred_cls, name=\"Predicted\"))\n",
    "print(\"\\nConfusion Matrix (OOF):\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed6540-6de9-4599-877b-af1c9531b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_latest_per_symbol(latest_df: pd.DataFrame,\n",
    "                              feature_cols: List[str],\n",
    "                              reg_model: XGBRegressor,\n",
    "                              cls_model: XGBClassifier,\n",
    "                              train_medians: pd.Series) -> pd.DataFrame:\n",
    "    last_idx = latest_df.groupby(SYMBOL_COL)[DATE_COL].idxmax()\n",
    "    snap = latest_df.loc[last_idx, [SYMBOL_COL, DATE_COL] + feature_cols].copy()\n",
    "    Xsnap = snap[feature_cols].astype(\"float32\").fillna(train_medians)\n",
    "\n",
    "    reg_hat = reg_model.predict(Xsnap)\n",
    "    up_prob = cls_model.predict_proba(Xsnap)[:, 1]\n",
    "    up_lab  = (up_prob >= 0.5).astype(int)\n",
    "\n",
    "    out = snap[[SYMBOL_COL, DATE_COL]].copy()\n",
    "    out[f\"pred_logret_{H}d\"]   = reg_hat\n",
    "    out[f\"pred_up_{H}d_prob\"]  = up_prob\n",
    "    out[f\"pred_up_{H}d\"]       = up_lab\n",
    "    return out.sort_values(f\"pred_logret_{H}d\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Example usage with the last trained fold’s models:\n",
    "# (If you want a production “latest” model, retrain on *all* data up to the latest date.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0f36a-7556-4786-b40a-914b47977121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52b288-5372-44e3-8eca-dd94d1bb08e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8f59f-18e8-40b4-a03a-5c45f701d0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (svd_forecasting)",
   "language": "python",
   "name": "svd_forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
