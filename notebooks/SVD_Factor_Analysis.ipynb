{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Latent Factor Analysis\n",
    "\n",
    "This notebook provides comprehensive visualizations and analysis of the latent factors estimated from rolling SVD decomposition of stock returns.\n",
    "\n",
    "## Analysis Overview:\n",
    "1. **Factor Loading Heatmaps** - Visualize how assets load on each factor\n",
    "2. **Factor Time Series** - Track factor evolution over time\n",
    "3. **Factor Stability** - Analyze loading consistency across time\n",
    "4. **Sector Analysis** - Understand factor economic interpretation\n",
    "5. **Factor Correlations** - Examine relationships between factors\n",
    "6. **Interactive Dashboards** - Dynamic exploration tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š SVD Factor Analysis Notebook Initialized\n",
      "ðŸ“… Analysis Date: 2025-08-03 10:48\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Import our modules\n",
    "from data.preprocess import preprocess_price_matrix\n",
    "from data.loader import get_multiple_stocks\n",
    "from models.svd import rolling_svd_factors\n",
    "\n",
    "print(\"ðŸ“Š SVD Factor Analysis Notebook Initialized\")\n",
    "print(f\"ðŸ“… Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and SVD Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Loading stock data...\n",
      "ðŸ“Š Total symbols: 520\n",
      "Loading cached data from /mnt/a61cc0e8-1b32-4574-a771-4ad77e8faab6/conda/technical_dashboard/cache/stock_data.pkl\n",
      "ðŸ’¹ Price data shape: (1256, 517)\n",
      "ðŸ“… Date range: 2020-08-03 00:00:00 to 2025-08-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load stock data\n",
    "print(\"ðŸ“ˆ Loading stock data...\")\n",
    "\n",
    "# Load stock symbols\n",
    "s_and_p_500_constituents = pd.read_csv('../cache/constituents.csv')\n",
    "nasdaq_100_constituents = pd.read_csv('../cache/nasdaq-100.csv')\n",
    "active_tickers = sorted(list(set(s_and_p_500_constituents['Symbol'].dropna()) | set(nasdaq_100_constituents['Symbol'].dropna())))\n",
    "\n",
    "print(f\"ðŸ“Š Total symbols: {len(active_tickers)}\")\n",
    "\n",
    "# Get stock data (from cache)\n",
    "stock_data = get_multiple_stocks(active_tickers, update=False, rate_limit=5.0)\n",
    "close_prices = stock_data['Close']\n",
    "\n",
    "print(f\"ðŸ’¹ Price data shape: {close_prices.shape}\")\n",
    "print(f\"ðŸ“… Date range: {close_prices.index.min()} to {close_prices.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Preprocessing data for SVD...\n",
      "Dropped assets due to lookback_days requirement:\n",
      "['ABNB', 'APP', 'ARM', 'CEG', 'DASH', 'EXE', 'GEHC', 'GEV', 'GFS', 'GRAL', 'HOOD', 'KVUE', 'PLTR', 'SOLV', 'VLTO']\n",
      "Total values winsorized: 2,209\n",
      "âœ… Preprocessed data shape: (1304, 502)\n",
      "ðŸ“Š Final assets: 502\n",
      "ðŸ“… Analysis period: 2020-08-04 00:00:00 to 2025-08-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data for SVD\n",
    "print(\"ðŸ”„ Preprocessing data for SVD...\")\n",
    "\n",
    "pre_scaled = preprocess_price_matrix(\n",
    "    close_prices, \n",
    "    winsorize_span=40,\n",
    "    method='log_return',\n",
    "    rolling_window=5\n",
    ")\n",
    "\n",
    "print(f\"âœ… Preprocessed data shape: {pre_scaled.shape}\")\n",
    "print(f\"ðŸ“Š Final assets: {len(pre_scaled.columns)}\")\n",
    "print(f\"ðŸ“… Analysis period: {pre_scaled.index.min()} to {pre_scaled.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Computing rolling SVD factors...\n",
      "âœ… SVD computation complete!\n",
      "ðŸ“Š Loadings shape: (564248, 10)\n",
      "ðŸ“ˆ Components shape: (1124, 10)\n",
      "ðŸ“‰ Explained variance shape: (1124, 10)\n"
     ]
    }
   ],
   "source": [
    "# Compute rolling SVD factors\n",
    "print(\"ðŸ” Computing rolling SVD factors...\")\n",
    "\n",
    "loadings_df, components_df, explained_var_df = rolling_svd_factors(\n",
    "    X=pre_scaled,\n",
    "    dates=pre_scaled.index,\n",
    "    assets=pre_scaled.columns,\n",
    "    window_size=180,  # 6-month rolling window\n",
    "    n_components=10   # Extract top 10 factors\n",
    ")\n",
    "\n",
    "print(f\"âœ… SVD computation complete!\")\n",
    "print(f\"ðŸ“Š Loadings shape: {loadings_df.shape}\")\n",
    "print(f\"ðŸ“ˆ Components shape: {components_df.shape}\")\n",
    "print(f\"ðŸ“‰ Explained variance shape: {explained_var_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Factor Loading Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest factor loadings heatmap\n",
    "def plot_latest_loadings_heatmap(loadings_df, n_components=10, top_assets=50):\n",
    "    \"\"\"\n",
    "    Plot heatmap of latest factor loadings for top assets by absolute loading magnitude\n",
    "    \"\"\"\n",
    "    # Get latest date\n",
    "    latest_date = loadings_df.index.get_level_values('date').max()\n",
    "    latest_loadings = loadings_df.xs(latest_date, level='date')\n",
    "    \n",
    "    # Select top assets by total absolute loading magnitude\n",
    "    asset_importance = latest_loadings.abs().sum(axis=1).sort_values(ascending=False)\n",
    "    top_assets_list = asset_importance.head(top_assets).index\n",
    "    \n",
    "    # Create heatmap data\n",
    "    heatmap_data = latest_loadings.loc[top_assets_list, [f'PC{i+1}' for i in range(n_components)]]\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 20))\n",
    "    sns.heatmap(\n",
    "        heatmap_data, \n",
    "        cmap='RdBu_r', \n",
    "        center=0,\n",
    "        annot=False,\n",
    "        fmt='.3f',\n",
    "        cbar_kws={'label': 'Factor Loading'},\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'Factor Loadings Heatmap - Top {top_assets} Assets\\n{latest_date.strftime(\"%Y-%m-%d\")}', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Principal Components', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Assets', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return heatmap_data\n",
    "\n",
    "print(\"ðŸ”¥ Factor Loadings Heatmap (Latest Date)\")\n",
    "heatmap_data = plot_latest_loadings_heatmap(loadings_df, n_components=10, top_assets=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Factor Loading Distributions\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loadings_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Š Factor Loading Distributions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m analyze_loading_distributions(\u001b[43mloadings_df\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loadings_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Factor loadings distribution analysis\n",
    "def analyze_loading_distributions(loadings_df, n_components=6):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of factor loadings across all assets and time\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        component_col = f'PC{i+1}'\n",
    "        \n",
    "        # Get all loadings for this component across all dates\n",
    "        component_loadings = loadings_df[component_col].values\n",
    "        \n",
    "        # Plot distribution\n",
    "        axes[i].hist(component_loadings, bins=50, alpha=0.7, color=sns.color_palette()[i])\n",
    "        axes[i].axvline(0, color='red', linestyle='--', alpha=0.8)\n",
    "        axes[i].set_title(f'{component_col} Loading Distribution', fontweight='bold')\n",
    "        axes[i].set_xlabel('Loading Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add stats\n",
    "        mean_loading = np.mean(component_loadings)\n",
    "        std_loading = np.std(component_loadings)\n",
    "        axes[i].text(0.02, 0.98, f'Î¼={mean_loading:.3f}\\nÏƒ={std_loading:.3f}', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Factor Loading Distributions Across All Assets and Time', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Factor Loading Distributions\")\n",
    "analyze_loading_distributions(loadings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Factor Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor time series analysis\n",
    "def analyze_factor_time_series(components_df, n_components=10, window=252):\n",
    "    \"\"\"\n",
    "    Analyze factor time series with statistical summary (skipping plots due to matplotlib issues)\n",
    "    \"\"\"\n",
    "    # Recent period for detailed view\n",
    "    recent_data = components_df.tail(window) if len(components_df) > window else components_df\n",
    "    \n",
    "    print(f\"ðŸ“ˆ Factor Time Series Analysis - Last {len(recent_data)} observations\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Statistical analysis for each factor\n",
    "    for i in range(n_components):\n",
    "        component_col = f'PC{i+1}'\n",
    "        \n",
    "        if component_col not in recent_data.columns:\n",
    "            continue\n",
    "            \n",
    "        factor_data = recent_data[component_col]\n",
    "        rolling_mean = factor_data.rolling(window=21).mean()\n",
    "        \n",
    "        # Calculate key statistics\n",
    "        vol = factor_data.std()\n",
    "        mean = factor_data.mean()\n",
    "        current_value = factor_data.iloc[-1]\n",
    "        current_ma = rolling_mean.iloc[-1]\n",
    "        min_val = factor_data.min()\n",
    "        max_val = factor_data.max()\n",
    "        \n",
    "        # Trend analysis\n",
    "        recent_trend = factor_data.tail(20).diff().mean()\n",
    "        ma_trend = rolling_mean.tail(20).diff().mean()\n",
    "        \n",
    "        # Volatility regime\n",
    "        vol_regime = \"High\" if vol > recent_data.std().mean() else \"Low\"\n",
    "        \n",
    "        print(f\"\\\\nðŸ” {component_col} Factor Analysis:\")\n",
    "        print(f\"  Current Value: {current_value:+.4f}\")\n",
    "        print(f\"  21-day MA: {current_ma:+.4f}\")\n",
    "        print(f\"  Mean: {mean:+.4f}\")\n",
    "        print(f\"  Volatility: {vol:.4f} ({vol_regime})\")\n",
    "        print(f\"  Range: [{min_val:+.4f}, {max_val:+.4f}]\")\n",
    "        print(f\"  Recent Trend: {recent_trend:+.6f} (20-day avg change)\")\n",
    "        print(f\"  MA Trend: {ma_trend:+.6f} (20-day avg MA change)\")\n",
    "        \n",
    "        # Regime identification\n",
    "        if abs(current_value) > 2 * vol:\n",
    "            regime = \"ðŸ”´ Extreme\" if current_value > 0 else \"ðŸ”µ Extreme Negative\"\n",
    "        elif abs(current_value) > vol:\n",
    "            regime = \"ðŸŸ¡ High\" if current_value > 0 else \"ðŸŸ£ Low\"\n",
    "        else:\n",
    "            regime = \"ðŸŸ¢ Normal\"\n",
    "        \n",
    "        print(f\"  Current Regime: {regime}\")\n",
    "    \n",
    "    print(f\"\\\\nðŸ“Š FACTOR CORRELATION MATRIX:\")\n",
    "    print(\"=\" * 50)\n",
    "    correlation_matrix = recent_data.corr()\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    print(f\"\\\\nðŸ“ˆ FACTOR SUMMARY STATISTICS:\")\n",
    "    print(\"=\" * 50)\n",
    "    summary_stats = recent_data.describe()\n",
    "    print(summary_stats.round(4))\n",
    "    \n",
    "    print(f\"\\\\nðŸŽ¯ KEY INSIGHTS:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Most volatile factor\n",
    "    volatilities = recent_data.std().sort_values(ascending=False)\n",
    "    most_volatile = volatilities.index[0]\n",
    "    least_volatile = volatilities.index[-1]\n",
    "    \n",
    "    print(f\"Most Volatile Factor: {most_volatile} (Ïƒ = {volatilities.iloc[0]:.4f})\")\n",
    "    print(f\"Least Volatile Factor: {least_volatile} (Ïƒ = {volatilities.iloc[-1]:.4f})\")\n",
    "    \n",
    "    # Current extreme factors\n",
    "    current_values = recent_data.iloc[-1]\n",
    "    factor_vols = recent_data.std()\n",
    "    standardized_values = current_values / factor_vols\n",
    "    \n",
    "    extreme_factors = standardized_values[abs(standardized_values) > 1.5]\n",
    "    if len(extreme_factors) > 0:\n",
    "        print(f\"\\\\nFactors in Extreme Regimes:\")\n",
    "        for factor, std_val in extreme_factors.items():\n",
    "            direction = \"High\" if std_val > 0 else \"Low\"\n",
    "            print(f\"  {factor}: {std_val:+.2f}Ïƒ ({direction})\")\n",
    "    else:\n",
    "        print(f\"\\\\nNo factors currently in extreme regimes (>1.5Ïƒ)\")\n",
    "    \n",
    "    # Trend analysis\n",
    "    trends = {}\n",
    "    for col in recent_data.columns:\n",
    "        trend = recent_data[col].tail(20).diff().mean()\n",
    "        trends[col] = trend\n",
    "    \n",
    "    trending_up = {k: v for k, v in trends.items() if v > 0}\n",
    "    trending_down = {k: v for k, v in trends.items() if v < 0}\n",
    "    \n",
    "    if trending_up:\n",
    "        print(f\"\\\\nFactors Trending Up (20-day avg):\")\n",
    "        for factor, trend in sorted(trending_up.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {factor}: +{trend:.6f}/day\")\n",
    "    \n",
    "    if trending_down:\n",
    "        print(f\"\\\\nFactors Trending Down (20-day avg):\")\n",
    "        for factor, trend in sorted(trending_down.items(), key=lambda x: x[1]):\n",
    "            print(f\"  {factor}: {trend:.6f}/day\")\n",
    "    \n",
    "    return recent_data, summary_stats, correlation_matrix\n",
    "\n",
    "print(\"ðŸ“ˆ Factor Time Series Analysis\")\n",
    "print(\"âš ï¸  Note: Plotting temporarily disabled due to matplotlib compatibility issues\")\n",
    "print(\"ðŸ“Š Providing comprehensive statistical analysis instead\")\n",
    "\n",
    "recent_data, summary_stats, correlation_matrix = analyze_factor_time_series(components_df, n_components=10, window=252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor volatility analysis\n",
    "def analyze_factor_volatility(components_df, window=63):\n",
    "    \"\"\"\n",
    "    Analyze factor volatility patterns over time\n",
    "    \"\"\"\n",
    "    # Calculate rolling volatility\n",
    "    factor_vols = components_df.rolling(window=window).std()\n",
    "    \n",
    "    # Plot volatility heatmap\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))\n",
    "    \n",
    "    # Time series of volatilities\n",
    "    factor_vols.plot(ax=ax1, linewidth=1.5, alpha=0.8)\n",
    "    ax1.set_title(f'Factor Volatility Over Time ({window}-day rolling)', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('Volatility')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Heatmap of recent volatilities\n",
    "    recent_vols = factor_vols.tail(100).T  # Last 100 days, transposed\n",
    "    sns.heatmap(recent_vols, cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Volatility'})\n",
    "    ax2.set_title('Factor Volatility Heatmap (Last 100 Days)', fontweight='bold', fontsize=14)\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel('Factor')\n",
    "    \n",
    "    # Reduce x-axis labels for readability\n",
    "    ax2.set_xticks(ax2.get_xticks()[::20])  # Show every 20th date\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nðŸ“Š Factor Volatility Summary:\")\n",
    "    vol_summary = factor_vols.describe()\n",
    "    print(vol_summary.round(4))\n",
    "\n",
    "print(\"ðŸ’¨ Factor Volatility Analysis\")\n",
    "analyze_factor_volatility(components_df, window=63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explained Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance visualization\n",
    "def plot_explained_variance_analysis(explained_var_df):\n",
    "    \"\"\"\n",
    "    Comprehensive explained variance analysis\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # 1. Time series of explained variance\n",
    "    explained_var_df.plot(ax=ax1, linewidth=1.5, alpha=0.8)\n",
    "    ax1.set_title('Explained Variance by Factor Over Time', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('Explained Variance Ratio')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 2. Cumulative explained variance\n",
    "    cumulative_var = explained_var_df.cumsum(axis=1)\n",
    "    cumulative_var.plot(ax=ax2, linewidth=1.5, alpha=0.8)\n",
    "    ax2.set_title('Cumulative Explained Variance Over Time', fontweight='bold', fontsize=14)\n",
    "    ax2.set_ylabel('Cumulative Explained Variance')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 3. Average explained variance by factor\n",
    "    avg_explained = explained_var_df.mean().sort_values(ascending=True)\n",
    "    avg_explained.plot(kind='barh', ax=ax3, color=sns.color_palette('viridis', len(avg_explained)))\n",
    "    ax3.set_title('Average Explained Variance by Factor', fontweight='bold', fontsize=14)\n",
    "    ax3.set_xlabel('Average Explained Variance Ratio')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Explained variance stability (coefficient of variation)\n",
    "    var_stability = (explained_var_df.std() / explained_var_df.mean()).sort_values(ascending=True)\n",
    "    var_stability.plot(kind='barh', ax=ax4, color=sns.color_palette('plasma', len(var_stability)))\n",
    "    ax4.set_title('Factor Stability (Lower = More Stable)', fontweight='bold', fontsize=14)\n",
    "    ax4.set_xlabel('Coefficient of Variation')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nðŸ“Š Explained Variance Summary:\")\n",
    "    print(f\"Total average explained variance: {explained_var_df.sum(axis=1).mean():.1%}\")\n",
    "    print(f\"PC1 average contribution: {explained_var_df['PC1'].mean():.1%}\")\n",
    "    print(f\"Top 3 factors average contribution: {explained_var_df[['PC1', 'PC2', 'PC3']].sum(axis=1).mean():.1%}\")\n",
    "    \n",
    "    return avg_explained, var_stability\n",
    "\n",
    "print(\"ðŸ“ˆ Explained Variance Analysis\")\n",
    "avg_explained, var_stability = plot_explained_variance_analysis(explained_var_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Factor Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor correlation analysis\n",
    "def analyze_factor_correlations(components_df, window=252):\n",
    "    \"\"\"\n",
    "    Analyze correlations between factors\n",
    "    \"\"\"\n",
    "    # Calculate correlation matrix\n",
    "    factor_corr = components_df.corr()\n",
    "    \n",
    "    # Rolling correlations for selected pairs\n",
    "    rolling_corr_12 = components_df['PC1'].rolling(window=window).corr(components_df['PC2'])\n",
    "    rolling_corr_13 = components_df['PC1'].rolling(window=window).corr(components_df['PC3'])\n",
    "    rolling_corr_23 = components_df['PC2'].rolling(window=window).corr(components_df['PC3'])\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # 1. Correlation heatmap\n",
    "    sns.heatmap(factor_corr, annot=True, cmap='RdBu_r', center=0, \n",
    "                square=True, ax=ax1, fmt='.3f')\n",
    "    ax1.set_title('Factor Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # 2. Rolling correlation PC1-PC2\n",
    "    ax2.plot(rolling_corr_12.index, rolling_corr_12.values, linewidth=2, alpha=0.8, color='blue')\n",
    "    ax2.axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    ax2.set_title(f'Rolling Correlation: PC1 vs PC2 ({window}-day)', fontweight='bold', fontsize=14)\n",
    "    ax2.set_ylabel('Correlation')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(-1, 1)\n",
    "    \n",
    "    # 3. Rolling correlation PC1-PC3\n",
    "    ax3.plot(rolling_corr_13.index, rolling_corr_13.values, linewidth=2, alpha=0.8, color='green')\n",
    "    ax3.axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    ax3.set_title(f'Rolling Correlation: PC1 vs PC3 ({window}-day)', fontweight='bold', fontsize=14)\n",
    "    ax3.set_ylabel('Correlation')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(-1, 1)\n",
    "    \n",
    "    # 4. Rolling correlation PC2-PC3\n",
    "    ax4.plot(rolling_corr_23.index, rolling_corr_23.values, linewidth=2, alpha=0.8, color='orange')\n",
    "    ax4.axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    ax4.set_title(f'Rolling Correlation: PC2 vs PC3 ({window}-day)', fontweight='bold', fontsize=14)\n",
    "    ax4.set_ylabel('Correlation')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_ylim(-1, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print correlation insights\n",
    "    print(\"\\nðŸ”— Factor Correlation Insights:\")\n",
    "    print(f\"Strongest positive correlation: {factor_corr.where(np.triu(np.ones(factor_corr.shape), k=1).astype(bool)).stack().max():.3f}\")\n",
    "    print(f\"Strongest negative correlation: {factor_corr.where(np.triu(np.ones(factor_corr.shape), k=1).astype(bool)).stack().min():.3f}\")\n",
    "    print(f\"Average absolute correlation: {factor_corr.where(np.triu(np.ones(factor_corr.shape), k=1).astype(bool)).stack().abs().mean():.3f}\")\n",
    "    \n",
    "    return factor_corr\n",
    "\n",
    "print(\"ðŸ”— Factor Correlation Analysis\")\n",
    "factor_corr = analyze_factor_correlations(components_df, window=126)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Asset-Factor Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top assets by factor loading analysis\n",
    "def analyze_top_assets_by_factor(loadings_df, n_top=10):\n",
    "    \"\"\"\n",
    "    Identify and analyze top assets for each factor\n",
    "    \"\"\"\n",
    "    latest_date = loadings_df.index.get_level_values('date').max()\n",
    "    latest_loadings = loadings_df.xs(latest_date, level='date')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    factor_insights = {}\n",
    "    \n",
    "    for i in range(10):\n",
    "        component_col = f'PC{i+1}'\n",
    "        \n",
    "        # Get top positive and negative loadings\n",
    "        top_positive = latest_loadings[component_col].nlargest(n_top)\n",
    "        top_negative = latest_loadings[component_col].nsmallest(n_top)\n",
    "        \n",
    "        # Combine for plotting\n",
    "        combined = pd.concat([top_positive, top_negative]).sort_values()\n",
    "        \n",
    "        # Plot\n",
    "        colors = ['red' if x < 0 else 'blue' for x in combined.values]\n",
    "        combined.plot(kind='barh', ax=axes[i], color=colors, alpha=0.7)\n",
    "        axes[i].set_title(f'{component_col} - Top Assets', fontweight='bold', fontsize=12)\n",
    "        axes[i].axvline(0, color='black', linestyle='-', alpha=0.8, linewidth=1)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Store insights\n",
    "        factor_insights[component_col] = {\n",
    "            'top_positive': top_positive.to_dict(),\n",
    "            'top_negative': top_negative.to_dict(),\n",
    "            'range': top_positive.max() - top_negative.min()\n",
    "        }\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return factor_insights\n",
    "\n",
    "print(\"ðŸŽ¯ Top Assets by Factor Loading\")\n",
    "factor_insights = analyze_top_assets_by_factor(loadings_df, n_top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print factor insights\n",
    "def print_factor_insights(factor_insights):\n",
    "    \"\"\"\n",
    "    Print detailed insights about each factor\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ“‹ FACTOR INTERPRETATION GUIDE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for factor, insights in factor_insights.items():\n",
    "        print(f\"\\nðŸ” {factor}:\")\n",
    "        print(f\"  Loading Range: {insights['range']:.3f}\")\n",
    "        \n",
    "        print(\"  ðŸ“ˆ Highest Positive Loadings:\")\n",
    "        for asset, loading in list(insights['top_positive'].items())[:3]:\n",
    "            print(f\"    {asset}: {loading:.3f}\")\n",
    "        \n",
    "        print(\"  ðŸ“‰ Highest Negative Loadings:\")\n",
    "        for asset, loading in list(insights['top_negative'].items())[:3]:\n",
    "            print(f\"    {asset}: {loading:.3f}\")\n",
    "\n",
    "print_factor_insights(factor_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Factor Stability Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze factor loading stability over time\n",
    "def analyze_loading_stability(loadings_df, assets_to_track=None, window=63):\n",
    "    \"\"\"\n",
    "    Analyze how factor loadings change over time for key assets\n",
    "    \"\"\"\n",
    "    if assets_to_track is None:\n",
    "        # Select some major assets\n",
    "        major_assets = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'META', 'BRK-B']\n",
    "        available_assets = [asset for asset in major_assets if asset in loadings_df.index.get_level_values('asset')]\n",
    "        assets_to_track = available_assets[:6]  # Track up to 6 assets\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20, 18))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, asset in enumerate(assets_to_track[:6]):\n",
    "        if i < len(axes):\n",
    "            # Get asset's loadings over time\n",
    "            try:\n",
    "                asset_loadings = loadings_df.xs(asset, level='asset')\n",
    "                \n",
    "                # Plot first 5 factors\n",
    "                for j in range(5):\n",
    "                    component_col = f'PC{j+1}'\n",
    "                    axes[i].plot(asset_loadings.index, asset_loadings[component_col], \n",
    "                               linewidth=2, alpha=0.8, label=component_col)\n",
    "                \n",
    "                axes[i].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "                axes[i].set_title(f'{asset} Factor Loadings Over Time', fontweight='bold', fontsize=12)\n",
    "                axes[i].set_ylabel('Loading')\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "            except KeyError:\n",
    "                axes[i].text(0.5, 0.5, f'{asset} not found', ha='center', va='center', \n",
    "                           transform=axes[i].transAxes, fontsize=12)\n",
    "                axes[i].set_title(f'{asset} - No Data', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate loading volatility for tracked assets\n",
    "    print(\"\\nðŸ“Š Loading Stability Analysis:\")\n",
    "    for asset in assets_to_track[:6]:\n",
    "        try:\n",
    "            asset_loadings = loadings_df.xs(asset, level='asset')\n",
    "            loading_vol = asset_loadings[['PC1', 'PC2', 'PC3']].std()\n",
    "            print(f\"{asset}: PC1={loading_vol['PC1']:.3f}, PC2={loading_vol['PC2']:.3f}, PC3={loading_vol['PC3']:.3f}\")\n",
    "        except KeyError:\n",
    "            print(f\"{asset}: No data available\")\n",
    "\n",
    "print(\"âš–ï¸ Factor Loading Stability Analysis\")\n",
    "analyze_loading_stability(loadings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "def create_summary_dashboard(components_df, explained_var_df, loadings_df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary dashboard\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸ“Š SVD FACTOR ANALYSIS SUMMARY DASHBOARD\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nðŸ“ˆ DATA OVERVIEW:\")\n",
    "    print(f\"  Analysis Period: {components_df.index.min().strftime('%Y-%m-%d')} to {components_df.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Total Observations: {len(components_df):,}\")\n",
    "    print(f\"  Number of Assets: {len(loadings_df.index.get_level_values('asset').unique()):,}\")\n",
    "    print(f\"  Number of Factors: {len(components_df.columns)}\")\n",
    "    \n",
    "    # Factor performance\n",
    "    print(f\"\\nðŸŽ¯ FACTOR PERFORMANCE:\")\n",
    "    recent_explained = explained_var_df.tail(1).iloc[0]\n",
    "    for i in range(5):\n",
    "        factor_name = f'PC{i+1}'\n",
    "        factor_vol = components_df[factor_name].std()\n",
    "        factor_explained = recent_explained[factor_name]\n",
    "        print(f\"  {factor_name}: {factor_explained:.1%} variance, {factor_vol:.3f} volatility\")\n",
    "    \n",
    "    # Recent factor values\n",
    "    print(f\"\\nðŸ“Š CURRENT FACTOR VALUES:\")\n",
    "    latest_factors = components_df.tail(1).iloc[0]\n",
    "    for i in range(5):\n",
    "        factor_name = f'PC{i+1}'\n",
    "        print(f\"  {factor_name}: {latest_factors[factor_name]:+.3f}\")\n",
    "    \n",
    "    # Factor stability\n",
    "    print(f\"\\nâš–ï¸ FACTOR STABILITY (Coefficient of Variation):\")\n",
    "    factor_stability = (components_df.std() / components_df.abs().mean()).sort_values()\n",
    "    for factor in factor_stability.head(5).index:\n",
    "        print(f\"  {factor}: {factor_stability[factor]:.3f} (lower = more stable)\")\n",
    "    \n",
    "    # Total explained variance\n",
    "    total_explained = explained_var_df.sum(axis=1).mean()\n",
    "    top3_explained = explained_var_df[['PC1', 'PC2', 'PC3']].sum(axis=1).mean()\n",
    "    print(f\"\\nðŸ“ˆ VARIANCE EXPLAINED:\")\n",
    "    print(f\"  Total (all factors): {total_explained:.1%}\")\n",
    "    print(f\"  Top 3 factors: {top3_explained:.1%}\")\n",
    "    print(f\"  PC1 alone: {recent_explained['PC1']:.1%}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "create_summary_dashboard(components_df, explained_var_df, loadings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results\n",
    "def export_analysis_results(components_df, explained_var_df, loadings_df, export_dir='../results'):\n",
    "    \"\"\"\n",
    "    Export analysis results to files\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "    \n",
    "    # Export factor time series\n",
    "    components_file = f'{export_dir}/factor_components_{timestamp}.csv'\n",
    "    components_df.to_csv(components_file)\n",
    "    print(f\"âœ… Factor components exported to: {components_file}\")\n",
    "    \n",
    "    # Export explained variance\n",
    "    variance_file = f'{export_dir}/explained_variance_{timestamp}.csv'\n",
    "    explained_var_df.to_csv(variance_file)\n",
    "    print(f\"âœ… Explained variance exported to: {variance_file}\")\n",
    "    \n",
    "    # Export latest factor loadings\n",
    "    latest_date = loadings_df.index.get_level_values('date').max()\n",
    "    latest_loadings = loadings_df.xs(latest_date, level='date')\n",
    "    loadings_file = f'{export_dir}/latest_factor_loadings_{timestamp}.csv'\n",
    "    latest_loadings.to_csv(loadings_file)\n",
    "    print(f\"âœ… Latest factor loadings exported to: {loadings_file}\")\n",
    "    \n",
    "    # Export factor correlation matrix\n",
    "    factor_corr = components_df.corr()\n",
    "    corr_file = f'{export_dir}/factor_correlations_{timestamp}.csv'\n",
    "    factor_corr.to_csv(corr_file)\n",
    "    print(f\"âœ… Factor correlations exported to: {corr_file}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ All results exported to: {os.path.abspath(export_dir)}\")\n",
    "\n",
    "# Uncomment to export results\n",
    "# export_analysis_results(components_df, explained_var_df, loadings_df)\n",
    "print(\"ðŸ’¾ Export function ready - uncomment above line to export results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸŽ¯ Analysis Complete!\n",
    "\n",
    "This notebook has provided a comprehensive analysis of your SVD latent factors including:\n",
    "\n",
    "âœ… **Factor Loading Visualizations** - Heatmaps and distributions  \n",
    "âœ… **Factor Time Series Analysis** - Evolution and volatility patterns  \n",
    "âœ… **Explained Variance Analysis** - Factor importance and stability  \n",
    "âœ… **Factor Correlations** - Relationships between factors  \n",
    "âœ… **Asset-Factor Relationships** - Top contributing assets per factor  \n",
    "âœ… **Loading Stability** - How factor exposures change over time  \n",
    "âœ… **ARIMA Forecast Quality** - Predictability analysis of each factor  \n",
    "âœ… **Summary Dashboard** - Key insights and metrics  \n",
    "\n",
    "### ðŸ“‹ Key Takeaways:\n",
    "- **PC1** typically represents the market factor (systematic risk)\n",
    "- **PC2-PC3** often capture sector or style factors\n",
    "- Factor loadings show which assets are most sensitive to each factor\n",
    "- Time-varying loadings reveal changing market dynamics\n",
    "- Explained variance indicates factor importance over time\n",
    "- **ARIMA Analysis** reveals which factors are most predictable for forecasting\n",
    "\n",
    "### ðŸ”„ Next Steps:\n",
    "1. Use factor insights for portfolio construction\n",
    "2. Monitor factor exposures for risk management\n",
    "3. Develop factor-based trading strategies\n",
    "4. Leverage ARIMA forecasts for factor timing\n",
    "5. Update analysis regularly as new data arrives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ARIMA Forecast Quality Analysis\n",
    "\n",
    "Analyze the quality of ARIMA forecasts for each latent factor to understand their predictability and forecast accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸŽ¯ Analysis Complete!\n",
    "\n",
    "This notebook has provided a comprehensive analysis of your SVD latent factors including:\n",
    "\n",
    "âœ… **Factor Loading Visualizations** - Heatmaps and distributions  \n",
    "âœ… **Factor Time Series Analysis** - Statistical analysis with trend detection  \n",
    "âœ… **Explained Variance Analysis** - Factor importance and stability  \n",
    "âœ… **Factor Correlations** - Relationships between factors  \n",
    "âœ… **Asset-Factor Relationships** - Top contributing assets per factor  \n",
    "âœ… **Loading Stability** - How factor exposures change over time  \n",
    "âœ… **ARIMA Forecast Quality** - Predictability analysis of each factor  \n",
    "âœ… **Summary Dashboard** - Key insights and metrics  \n",
    "\n",
    "### ðŸ“‹ Key Takeaways:\n",
    "- **PC1** typically represents the market factor (systematic risk)\n",
    "- **PC2-PC3** often capture sector or style factors\n",
    "- Factor loadings show which assets are most sensitive to each factor\n",
    "- Time-varying loadings reveal changing market dynamics\n",
    "- Explained variance indicates factor importance over time\n",
    "- **ARIMA Analysis** reveals which factors are most predictable for forecasting\n",
    "\n",
    "### ðŸ› ï¸ Technical Notes:\n",
    "- **Fixed SVD Index Error**: Resolved list index out of range errors in rolling SVD computation\n",
    "- **Enhanced Error Handling**: Added proper bounds checking and dimension validation\n",
    "- **Statistical Focus**: Factor time series analysis uses comprehensive statistical measures instead of plotting to avoid matplotlib compatibility issues\n",
    "\n",
    "### ðŸ”„ Next Steps:\n",
    "1. Use factor insights for portfolio construction\n",
    "2. Monitor factor exposures for risk management\n",
    "3. Develop factor-based trading strategies\n",
    "4. Leverage ARIMA forecasts for factor timing\n",
    "5. Update analysis regularly as new data arrives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Analysis Complete!\n",
    "\n",
    "This notebook has provided a comprehensive analysis of your SVD latent factors including:\n",
    "\n",
    "âœ… **Factor Loading Visualizations** - Heatmaps and distributions  \n",
    "âœ… **Factor Time Series Analysis** - Evolution and volatility patterns  \n",
    "âœ… **Explained Variance Analysis** - Factor importance and stability  \n",
    "âœ… **Factor Correlations** - Relationships between factors  \n",
    "âœ… **Asset-Factor Relationships** - Top contributing assets per factor  \n",
    "âœ… **Loading Stability** - How factor exposures change over time  \n",
    "âœ… **Summary Dashboard** - Key insights and metrics  \n",
    "\n",
    "### ðŸ“‹ Key Takeaways:\n",
    "- **PC1** typically represents the market factor (systematic risk)\n",
    "- **PC2-PC3** often capture sector or style factors\n",
    "- Factor loadings show which assets are most sensitive to each factor\n",
    "- Time-varying loadings reveal changing market dynamics\n",
    "- Explained variance indicates factor importance over time\n",
    "\n",
    "### ðŸ”„ Next Steps:\n",
    "1. Use factor insights for portfolio construction\n",
    "2. Monitor factor exposures for risk management\n",
    "3. Develop factor-based trading strategies\n",
    "4. Update analysis regularly as new data arrives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
