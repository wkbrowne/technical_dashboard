{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cbf3d-31ab-4a79-a9a7-b3f9129b5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src') \n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')  # Load .env file from parent directory\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93ebbe-98e7-4d04-8e09-ad3c4ad11b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data.loader import load_stock_universe, load_etf_universe\n",
    "import pandas as pd\n",
    "# ✅ Verify API key\n",
    "rapidapi_key = os.getenv('RAPIDAPI_KEY')\n",
    "if rapidapi_key:\n",
    "    print(f\"✅ RAPIDAPI_KEY loaded (first 10 chars: {rapidapi_key[:10]}...)\")\n",
    "else:\n",
    "    print(\"⚠️ RAPIDAPI_KEY not found in environment variables\")\n",
    "    print(\"   Make sure your .env contains: RAPIDAPI_KEY=your_key_here\")\n",
    "\n",
    "# ---- Fetch stocks (cached to *_universe.parquet) ----\n",
    "stocks, sectors = load_stock_universe(update=False, include_sectors=True)\n",
    "\n",
    "\n",
    "# ---- Fetch ETFs (cached to *_etf.parquet) ----\n",
    "etfs = load_etf_universe(\n",
    "    etf_symbols=None,             # or pass a list like [\"SPY\",\"QQQ\",...]\n",
    "    etf_csv_path=None,            # or a CSV path with a Symbol column\n",
    "    update=False,                 # True to re-fetch\n",
    "    rate_limit=1.0,\n",
    "    interval=\"1d\"\n",
    ")\n",
    "\n",
    "# ---- Quick summaries ----\n",
    "def _summary(block, label):\n",
    "    if not block:\n",
    "        print(f\"❌ No {label} loaded\"); return\n",
    "    n_syms = len(block.get(\"Close\", pd.DataFrame()).columns) if \"Close\" in block else 0\n",
    "    n_days = len(block.get(\"Close\", pd.DataFrame()).index) if \"Close\" in block else 0\n",
    "    print(f\"✅ {label}: {n_syms} symbols × {n_days} days\")\n",
    "\n",
    "_summary(stocks, \"stocks\")\n",
    "_summary(etfs, \"ETFs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13a130-7589-4790-87ed-b6236cf734b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    k: pd.concat([stocks.get(k, pd.DataFrame()),\n",
    "                  etfs.get(k, pd.DataFrame())], axis=1).sort_index()\n",
    "    for k in (set(stocks) | set(etfs))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f886b-8c41-401b-81b5-e7aad719601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ View what's loaded\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(f\"{k}: shape = {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eaaf39-6b45-42ee-b815-138101150c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nolds\n",
    "import pandas_ta as ta\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ===================== Params =====================\n",
    "HURST_WINDOWS        = (128,)         # rolling window(s) for Hurst on returns\n",
    "HURST_EMA_FOR        = 128            # which H track to smooth (must be in HURST_WINDOWS)\n",
    "HURST_HALFLIFE       = 5              # halflife EMA for H smoothing; set None/0 to disable\n",
    "HURST_EMA_SPAN       = None           # alternative to halflife; keep None if using halflife\n",
    "\n",
    "BB_LENGTH            = 20\n",
    "BB_STD               = 2.0\n",
    "\n",
    "INCLUDE_TA           = False\n",
    "MA_LIST              = [20, 50, 100, 200]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*deprecated.*\")\n",
    "\n",
    "# ===================== Helpers =====================\n",
    "def _rolling_hurst_vec(x: np.ndarray) -> float:\n",
    "    try:\n",
    "        return float(nolds.hurst_rs(np.asarray(x, dtype=float), fit=\"poly\"))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _ensure_bollinger(df: pd.DataFrame, src='adjclose', length=BB_LENGTH, std=BB_STD) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    try:\n",
    "        bb = ta.bbands(out[src], length=length, std=std)\n",
    "    except Exception:\n",
    "        bb = None\n",
    "\n",
    "    if bb is not None and not bb.empty:\n",
    "        lo = bb.filter(like=\"BBL\").iloc[:, 0]\n",
    "        mid = bb.filter(like=\"BBM\").iloc[:, 0]\n",
    "        up = bb.filter(like=\"BBU\").iloc[:, 0]\n",
    "        out[\"bb_lower\"] = lo\n",
    "        out[\"bb_middle\"] = mid\n",
    "        out[\"bb_upper\"] = up\n",
    "    else:\n",
    "        mid = out[src].rolling(length, min_periods=max(2, length//2)).mean()\n",
    "        sd = out[src].rolling(length, min_periods=max(2, length//2)).std(ddof=0)\n",
    "        out[\"bb_middle\"] = mid\n",
    "        out[\"bb_upper\"]  = mid + std * sd\n",
    "        out[\"bb_lower\"]  = mid - std * sd\n",
    "\n",
    "    rng = (out[\"bb_upper\"] - out[\"bb_lower\"]).replace(0, np.nan)\n",
    "    out[\"bb_pct_b\"] = (out[src] - out[\"bb_lower\"]) / rng\n",
    "    out[\"bb_width_pct\"] = (out[\"bb_upper\"] - out[\"bb_lower\"]) / out[\"bb_middle\"].replace(0, np.nan)\n",
    "    return out\n",
    "\n",
    "def compute_hurst_windows(series: pd.Series,\n",
    "                          windows=HURST_WINDOWS,\n",
    "                          ema_smooth_for=HURST_EMA_FOR,\n",
    "                          ema_span=HURST_EMA_SPAN,\n",
    "                          ema_halflife=HURST_HALFLIFE,\n",
    "                          prefix=\"hurst\") -> pd.DataFrame:\n",
    "    out = pd.DataFrame(index=series.index)\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "    for w in windows:\n",
    "        col = f\"{prefix}_{w}\"\n",
    "        out[col] = s.rolling(window=w, min_periods=w).apply(_rolling_hurst_vec, raw=False)\n",
    "\n",
    "    if ema_smooth_for is not None:\n",
    "        base_col = f\"{prefix}_{ema_smooth_for}\"\n",
    "        if base_col in out.columns:\n",
    "            if ema_halflife is not None and ema_halflife > 0:\n",
    "                out[f\"{base_col}_emaHL{ema_halflife}\"] = out[base_col].ewm(\n",
    "                    halflife=ema_halflife, adjust=False, min_periods=1\n",
    "                ).mean()\n",
    "            elif ema_span is not None and ema_span > 1:\n",
    "                out[f\"{base_col}_ema{ema_span}\"] = out[base_col].ewm(\n",
    "                    span=ema_span, adjust=False, min_periods=1\n",
    "                ).mean()\n",
    "    return out\n",
    "\n",
    "# ===================== WORKER =====================\n",
    "def compute_indicators(symbol: str, series: pd.Series):\n",
    "    df = pd.DataFrame(series).copy()\n",
    "    df.columns = ['adjclose']\n",
    "    df['adjclose'] = pd.to_numeric(df['adjclose'], errors='coerce')\n",
    "    df['ret'] = np.log(df['adjclose']).diff()\n",
    "\n",
    "    # Hurst\n",
    "    hurst_block = compute_hurst_windows(\n",
    "        df['ret'],\n",
    "        windows=HURST_WINDOWS,\n",
    "        ema_smooth_for=HURST_EMA_FOR,\n",
    "        ema_span=HURST_EMA_SPAN,\n",
    "        ema_halflife=HURST_HALFLIFE,\n",
    "        prefix=\"hurst_ret\"\n",
    "    )\n",
    "    out = df.join(hurst_block, how='left')\n",
    "\n",
    "    # Bollinger\n",
    "    out = _ensure_bollinger(out, src='adjclose', length=BB_LENGTH, std=BB_STD)\n",
    "\n",
    "    if INCLUDE_TA:\n",
    "        for ma in MA_LIST:\n",
    "            out[f'ma_{ma}'] = ta.sma(out['adjclose'], length=ma)\n",
    "\n",
    "    return symbol, out\n",
    "\n",
    "# ===================== PARALLEL =====================\n",
    "adj_df = data['AdjClose'].copy()\n",
    "\n",
    "results = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
    "    delayed(compute_indicators)(symbol, adj_df[symbol]) for symbol in adj_df.columns\n",
    ")\n",
    "\n",
    "indicators_by_symbol = {sym: df for sym, df in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045af74-50fd-4c60-834d-f82fac40e7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3643d-c40d-48a5-83ee-be1718a75372",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_by_symbol['TSLA']['hurst_ret_128_emaHL5'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dec6da-4b19-45d5-93e3-460d763c71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def _ensure_mas(df: pd.DataFrame, src='adjclose', periods=(10,20,30,50,75,100,150,200)):\n",
    "    for p in periods:\n",
    "        col = f'ma_{p}'\n",
    "        if col not in df.columns:\n",
    "            df[col] = df[src].rolling(p, min_periods=p//2).mean()\n",
    "    return df\n",
    "\n",
    "def add_trend_features(\n",
    "    df: pd.DataFrame,\n",
    "    src_col: str = 'adjclose',\n",
    "    ma_periods=(10, 20, 30, 50, 75, 100, 150, 200),\n",
    "    slope_window: int = 20,\n",
    "    eps: float = 1e-5,\n",
    "    persist_halflife: int = 5,\n",
    "    slope_scale: float = 0.05,   # scale for % slope -> [-1,1] via tanh\n",
    "    weights=(0.5, 0.3, 0.2)      # (sign, slope, alignment) weights\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds continuous trend features (no manual buckets):\n",
    "\n",
    "      - trend_score_sign ∈ [-1,1]: mean sign of MA slopes (deadzone eps)\n",
    "      - trend_score_slope ∈ [-1,1]: avg % slope across MAs (tanh-normalized)\n",
    "      - trend_alignment  ∈ [-1,1]: signed stacking of adjacent MAs\n",
    "      - trend_persist_ema: EMA( trend_score_sign ) with halflife\n",
    "      - trend_score_granular ∈ [-1,1]: weighted blend of (sign, slope, alignment)\n",
    "\n",
    "    Also keeps helper columns:\n",
    "      - sign_ma_{p}\n",
    "      - pct_slope_ma_{p}   (per-day % slope of MA p over slope_window)\n",
    "    \"\"\"\n",
    "    df = _ensure_mas(df, src=src_col, periods=ma_periods)\n",
    "\n",
    "    # ---- 1) Sign of MA slopes over a lookback ----\n",
    "    sign_cols, pct_slope_cols = [], []\n",
    "    for p in ma_periods:\n",
    "        ma = df[f'ma_{p}']\n",
    "        # per-bar slope in price units\n",
    "        raw_slope = (ma - ma.shift(slope_window)) / float(slope_window)\n",
    "        # convert to % slope (relative to prior MA level) to be scale-invariant\n",
    "        pct_slope = raw_slope / ma.shift(slope_window)\n",
    "        df[f'pct_slope_ma_{p}'] = pct_slope.astype('float32')\n",
    "        pct_slope_cols.append(f'pct_slope_ma_{p}')\n",
    "\n",
    "        # soft sign with deadzone\n",
    "        sign = np.where(raw_slope > eps, 1, np.where(raw_slope < -eps, -1, 0)).astype('float32')\n",
    "        df[f'sign_ma_{p}'] = sign\n",
    "        sign_cols.append(f'sign_ma_{p}')\n",
    "\n",
    "    # Mean sign over non-zero votes\n",
    "    sign_mat = df[sign_cols].to_numpy(dtype='float32')\n",
    "    nz = (sign_mat != 0).sum(axis=1)\n",
    "    sums = sign_mat.sum(axis=1)\n",
    "    score_sign = np.divide(\n",
    "        sums,\n",
    "        np.where(nz == 0, np.nan, nz),\n",
    "        out=np.zeros_like(sums, dtype='float32'),\n",
    "        where=nz != 0\n",
    "    )\n",
    "    df['trend_score_sign'] = score_sign.astype('float32')\n",
    "\n",
    "    # ---- 2) Magnitude via % slope, squashed to [-1,1] ----\n",
    "    # average % slope across MAs, then tanh to bound & damp outliers\n",
    "    avg_pct_slope = df[pct_slope_cols].mean(axis=1)\n",
    "    df['trend_score_slope'] = np.tanh(avg_pct_slope / slope_scale).astype('float32')\n",
    "\n",
    "    # ---- 3) Stacking / alignment of adjacent MAs ----\n",
    "    # For uptrend: ma_short > ma_long; for downtrend: reverse.\n",
    "    up_pairs = 0\n",
    "    down_pairs = 0\n",
    "    total_pairs = 0\n",
    "    # compute signed alignment per row: (#up_pairs - #down_pairs) / total_pairs\n",
    "    # adjacent pairs only for stability\n",
    "    for i in range(len(ma_periods) - 1):\n",
    "        a = df[f'ma_{ma_periods[i]}']\n",
    "        b = df[f'ma_{ma_periods[i+1]}']\n",
    "        up = (a > b).astype('int8')\n",
    "        down = (a < b).astype('int8')\n",
    "        if i == 0:\n",
    "            up_pairs = up\n",
    "            down_pairs = down\n",
    "        else:\n",
    "            up_pairs = up_pairs + up\n",
    "            down_pairs = down_pairs + down\n",
    "        total_pairs += 1\n",
    "    # signed alignment in [-1,1]\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        align = (up_pairs - down_pairs) / float(total_pairs if total_pairs else 1)\n",
    "    df['trend_alignment'] = align.astype('float32')\n",
    "\n",
    "    # ---- 4) Persistence: EMA of the sign score ----\n",
    "    if persist_halflife and persist_halflife > 0:\n",
    "        df['trend_persist_ema'] = (\n",
    "            df['trend_score_sign']\n",
    "            .ewm(halflife=persist_halflife, adjust=False, min_periods=1)\n",
    "            .mean()\n",
    "            .astype('float32')\n",
    "        )\n",
    "    else:\n",
    "        df['trend_persist_ema'] = df['trend_score_sign'].astype('float32')\n",
    "\n",
    "    # ---- 5) Weighted blended granular score ----\n",
    "    w_sign, w_slope, w_align = weights\n",
    "    wsum = float(w_sign + w_slope + w_align) or 1.0\n",
    "    df['trend_score_granular'] = (\n",
    "        (w_sign * df['trend_score_sign'] +\n",
    "         w_slope * df['trend_score_slope'] +\n",
    "         w_align * df['trend_alignment']) / wsum\n",
    "    ).astype('float32')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _add_trend_for_symbol(sym, df):\n",
    "    try:\n",
    "        out = add_trend_features(\n",
    "            df.copy(),\n",
    "            src_col='adjclose',\n",
    "            ma_periods=(10, 20, 30, 50, 75, 100, 150, 200),\n",
    "            slope_window=20,\n",
    "            eps=1e-5,\n",
    "            persist_halflife=5,   # ~1 trading week\n",
    "            slope_scale=0.05,     # 5%/day slope ~ tanh(1)\n",
    "            weights=(0.5, 0.3, 0.2)\n",
    "        )\n",
    "        return sym, out\n",
    "    except Exception as e:\n",
    "        print(f\"[trend_features] {sym} failed: {e}\")\n",
    "        return sym, df\n",
    "\n",
    "# Parallel pass\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(_add_trend_for_symbol)(sym, indicators_by_symbol[sym])\n",
    "    for sym in indicators_by_symbol.keys()\n",
    ")\n",
    "\n",
    "indicators_by_symbol = {sym: df for sym, df in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a9ddb-cf0b-44ae-b7ea-a6ed360b3b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d28a8-fc15-41d6-bf47-a3314bc17e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _ensure_mas(df: pd.DataFrame, src='adjclose', periods=(20,50,100,200)) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for p in periods:\n",
    "        col = f'ma_{p}'\n",
    "        if col not in out.columns:\n",
    "            out[col] = out[src].rolling(p, min_periods=max(2, p//2)).mean()\n",
    "    return out\n",
    "\n",
    "def _zscore(s: pd.Series) -> pd.Series:\n",
    "    m = s.mean(skipna=True); sd = s.std(skipna=True)\n",
    "    if sd is None or sd == 0 or np.isnan(sd): \n",
    "        return s - m\n",
    "    return (s - m) / sd\n",
    "\n",
    "# ---------- main feature builder ----------\n",
    "def add_min_pct_dist_ma(\n",
    "    df: pd.DataFrame,\n",
    "    ma_lengths=[20, 50, 100, 200],\n",
    "    normalize=True,                      # per-symbol z-score columns\n",
    "    add_phase_features=True              # pct_dist_ma_20, pct_dist_ma_50, relative_dist_20_50\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds % distance to each MA, min absolute % distance, and (optionally)\n",
    "    20/50-specific phase features and per-column z-scores.\n",
    "    \"\"\"\n",
    "    out = _ensure_mas(df, src='adjclose', periods=tuple(ma_lengths)).copy()\n",
    "\n",
    "    # % distances to each MA\n",
    "    pct_cols = []\n",
    "    for ma in ma_lengths:\n",
    "        col_ma = f'ma_{ma}'\n",
    "        if col_ma not in out.columns:\n",
    "            raise ValueError(f\"Missing column: {col_ma}\")\n",
    "        denom = out[col_ma].replace(0, np.nan)\n",
    "        col_pct = f'pct_dist_ma_{ma}'\n",
    "        out[col_pct] = (out['adjclose'] - out[col_ma]) / denom\n",
    "        pct_cols.append(col_pct)\n",
    "\n",
    "    # min absolute % distance across the set of MAs\n",
    "    out['min_pct_dist_ma'] = out[pct_cols].abs().min(axis=1)\n",
    "\n",
    "    # phase features (short vs mid-term stretch)\n",
    "    if add_phase_features:\n",
    "        if 'pct_dist_ma_20' in out and 'pct_dist_ma_50' in out:\n",
    "            out['relative_dist_20_50'] = out['pct_dist_ma_20'] - out['pct_dist_ma_50']\n",
    "\n",
    "    # optional z-scored versions (per symbol)\n",
    "    if normalize:\n",
    "        for c in pct_cols + ['min_pct_dist_ma'] + (['relative_dist_20_50'] if 'relative_dist_20_50' in out else []):\n",
    "            out[f'{c}_z'] = _zscore(out[c])\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------- parallel application across your dict ----------\n",
    "def _add_min_pct_for_symbol(sym: str, df: pd.DataFrame, ma_lengths=(20,50,100,200),\n",
    "                            normalize=True, add_phase_features=True):\n",
    "    try:\n",
    "        out = add_min_pct_dist_ma(df, ma_lengths=list(ma_lengths),\n",
    "                                  normalize=normalize, add_phase_features=add_phase_features)\n",
    "        return sym, out\n",
    "    except Exception as e:\n",
    "        print(f\"[min_pct_dist_ma] {sym} failed: {e}\")\n",
    "        return sym, df\n",
    "\n",
    "# Run after your indicators_by_symbol has 'adjclose' (and any existing columns you want to keep)\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(_add_min_pct_for_symbol)(\n",
    "        sym, indicators_by_symbol[sym],\n",
    "        ma_lengths=(20,50,100,200),\n",
    "        normalize=True,\n",
    "        add_phase_features=True\n",
    "    )\n",
    "    for sym in indicators_by_symbol.keys()\n",
    ")\n",
    "\n",
    "# Rebuild the dict\n",
    "indicators_by_symbol = {sym: df for sym, df in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c8803-e592-4a12-bc0d-8c942a0d577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RS_LOOKBACK = 60\n",
    "RS_SLOPE_WIN = 20\n",
    "SPY_SYMBOL = \"SPY\"  # change if you use another benchmark\n",
    "\n",
    "# ---- Build sector benchmarks (ETF adjclose per sector) ----\n",
    "def build_sector_benchmarks(indicators_by_symbol, sectors, sector_to_etf):\n",
    "    if 'sectors' not in globals():\n",
    "        raise NameError(\"`sectors` dict is not defined in this scope.\")\n",
    "\n",
    "    # collect ETF adjclose series for mapped sectors\n",
    "    sector_benchmarks = {}\n",
    "    for sec, etf_sym in sector_to_etf.items():\n",
    "        df_etf = indicators_by_symbol.get(etf_sym)\n",
    "        if df_etf is not None and 'adjclose' in df_etf.columns:\n",
    "            sector_benchmarks[sec] = pd.to_numeric(df_etf['adjclose'], errors='coerce')\n",
    "        # else: silently skip; RS worker will handle None\n",
    "\n",
    "    # (optional) sanity: warn for sector names present in `sectors` but missing a benchmark\n",
    "    missing = {sectors[sym] for sym in sectors if isinstance(sectors[sym], str)} - set(sector_benchmarks.keys())\n",
    "    if missing:\n",
    "        print(f\"⚠️ No ETF benchmark for sectors: {sorted(missing)[:8]}{' …' if len(missing)>8 else ''}\")\n",
    "\n",
    "    return sector_benchmarks\n",
    "\n",
    "\n",
    "sector_to_etf = {\n",
    "    \"technology services\": \"XLK\",\n",
    "    \"electronic technology\": \"XLK\",\n",
    "    \"finance\": \"XLF\",\n",
    "    \"retail trade\": \"XLY\",\n",
    "    \"consumer services\": \"XLY\",\n",
    "    \"consumer durables\": \"XLY\",\n",
    "    \"consumer non-durables\": \"XLP\",\n",
    "    \"health technology\": \"XLV\",\n",
    "    \"health services\": \"XLV\",\n",
    "    \"producer manufacturing\": \"XLI\",\n",
    "    \"industrial services\": \"XLI\",\n",
    "    \"distribution services\": \"XLI\",\n",
    "    \"transportation\": \"XLI\",\n",
    "    \"energy minerals\": \"XLE\",\n",
    "    \"non-energy minerals\": \"XLB\",\n",
    "    \"process industries\": \"XLB\",\n",
    "    \"communications\": \"XLC\",\n",
    "    \"commercial services\": \"XLC\",\n",
    "    \"utilities\": \"XLU\",\n",
    "    \"miscellaneous\": \"SPY\",\n",
    "}\n",
    "\n",
    "# Normalize lookup\n",
    "sector_benchmarks = {\n",
    "    sector: indicators_by_symbol[etf]['adjclose']\n",
    "    for sector, etf in sector_to_etf.items()\n",
    "    if etf in indicators_by_symbol\n",
    "}\n",
    "\n",
    "\n",
    "def _rs_worker(sym: str, df: pd.DataFrame, sector_bench: pd.Series, spy_bench: pd.Series):\n",
    "    \"\"\"\n",
    "    Return ONLY the new RS columns for symbol `sym` to reduce IPC payload size.\n",
    "    Adds:\n",
    "      - rel_strength_sector, rel_strength_sector_norm, rel_strength_sector_slope20\n",
    "      - rel_strength_spy,    rel_strength_spy_norm,    rel_strength_spy_slope20\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'adjclose' not in df.columns:\n",
    "            return sym, None\n",
    "\n",
    "        idx   = df.index\n",
    "        price = pd.to_numeric(df['adjclose'], errors='coerce')\n",
    "\n",
    "        out = pd.DataFrame(index=idx)\n",
    "\n",
    "        # ---- helper to compute RS set safely ----\n",
    "        def _compute_rs(block_prefix: str, bench: pd.Series):\n",
    "            if bench is None:\n",
    "                return\n",
    "            bench_s = pd.to_numeric(bench.reindex(idx), errors='coerce').replace(0, np.nan)\n",
    "            if bench_s.notna().sum() < 10 or price.notna().sum() < 10:\n",
    "                return\n",
    "            rs = price / bench_s\n",
    "            roll = rs.rolling(RS_LOOKBACK, min_periods=max(5, RS_LOOKBACK//3)).mean()\n",
    "            rs_norm = (rs / roll) - 1.0\n",
    "            rs_slope = (rs - rs.shift(RS_SLOPE_WIN)) / float(RS_SLOPE_WIN)\n",
    "            out[f'{block_prefix}']            = rs.astype('float32')\n",
    "            out[f'{block_prefix}_norm']       = rs_norm.astype('float32')\n",
    "            out[f'{block_prefix}_slope20']    = rs_slope.astype('float32')\n",
    "\n",
    "        # Sector RS\n",
    "        _compute_rs('rel_strength_sector', sector_bench)\n",
    "\n",
    "        # SPY RS (skip for SPY itself)\n",
    "        if spy_bench is not None and sym != SPY_SYMBOL:\n",
    "            _compute_rs('rel_strength_spy', spy_bench)\n",
    "\n",
    "        return sym, (out if not out.empty else None)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[RS worker] {sym} failed: {e}\")\n",
    "        return sym, None\n",
    "\n",
    "\n",
    "# ---- Build SPY benchmark series once ----\n",
    "spy_series = None\n",
    "if SPY_SYMBOL in indicators_by_symbol and 'adjclose' in indicators_by_symbol[SPY_SYMBOL].columns:\n",
    "    spy_series = pd.to_numeric(indicators_by_symbol[SPY_SYMBOL]['adjclose'], errors='coerce')\n",
    "else:\n",
    "    print(f\"⚠️ {SPY_SYMBOL} not found in indicators_by_symbol or missing 'adjclose' — SPY RS will be skipped.\")\n",
    "\n",
    "# ---- Parallel compute: pass both sector benchmark and SPY series ----\n",
    "results = Parallel(\n",
    "    n_jobs=max(1, os.cpu_count() - 1),\n",
    "    backend=\"loky\",         # use 'threading' if memory is tight\n",
    "    batch_size=8,\n",
    "    verbose=0\n",
    ")(\n",
    "    delayed(_rs_worker)(\n",
    "        sym,\n",
    "        indicators_by_symbol[sym],\n",
    "        sector_benchmarks.get(sectors.get(sym,\"\").lower()) if 'sectors' in globals() else None,\n",
    "        spy_series\n",
    "    )\n",
    "    for sym in indicators_by_symbol.keys()\n",
    ")\n",
    "\n",
    "# ---- Merge results safely (overwrite or create columns) ----\n",
    "for sym, small_df in results:\n",
    "    if small_df is not None and not small_df.empty:\n",
    "        base = indicators_by_symbol[sym]\n",
    "        # align index (in case); then assign columns in-place\n",
    "        small_df = small_df.reindex(base.index)\n",
    "        for col in small_df.columns:\n",
    "            base[col] = small_df[col].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a87eb-c4ec-41d6-b5c2-2791034c2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_by_symbol['TSLA'].columns\n",
    "# sectors['TSLA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43411644-59e1-4f79-801a-7d67c592b629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "ATR_LENGTH = 14\n",
    "\n",
    "def _add_atr_pct_for_symbol(sym: str, df: pd.DataFrame,\n",
    "                            high_s: pd.Series, low_s: pd.Series,\n",
    "                            length: int = ATR_LENGTH):\n",
    "    try:\n",
    "        out = df.copy()\n",
    "        h = pd.to_numeric(high_s, errors='coerce')\n",
    "        l = pd.to_numeric(low_s, errors='coerce')\n",
    "        c = pd.to_numeric(out['adjclose'], errors='coerce')\n",
    "        atr = ta.atr(h, l, c, length=length)\n",
    "        out['atr_percent'] = (atr / c) * 100\n",
    "        return sym, out\n",
    "    except Exception as e:\n",
    "        print(f\"[ATR%] {sym} failed: {e}\")\n",
    "        return sym, df\n",
    "\n",
    "# Wide OHLC inputs\n",
    "high_df = data['High']\n",
    "low_df  = data['Low']\n",
    "\n",
    "# Parallel: same rebuild pattern you already use\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(_add_atr_pct_for_symbol)(\n",
    "        sym,\n",
    "        indicators_by_symbol[sym],\n",
    "        high_df[sym],\n",
    "        low_df[sym],\n",
    "        ATR_LENGTH\n",
    "    )\n",
    "    for sym in indicators_by_symbol.keys()\n",
    "    if sym in high_df.columns and sym in low_df.columns\n",
    ")\n",
    "\n",
    "indicators_by_symbol = {sym: df for sym, df in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa29e34-04c4-4671-91a3-b09e0d7d2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_by_symbol['TSLA'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624fab0-fa55-43ed-9179-e0eb306d16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "VOL_WIN_SHORT = 20\n",
    "VOL_WIN_LONG  = 50\n",
    "OBV_USE_ADJCLOSE = True\n",
    "\n",
    "def _basic_volume_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "\n",
    "    has_vol = 'volume' in df.columns\n",
    "    if has_vol:\n",
    "        vol = pd.to_numeric(df['volume'], errors='coerce')\n",
    "\n",
    "        # rolling means\n",
    "        out['vol_ma_20'] = vol.rolling(VOL_WIN_SHORT, min_periods=5).mean()\n",
    "        out['vol_ma_50'] = vol.rolling(VOL_WIN_LONG,  min_periods=10).mean()\n",
    "\n",
    "        # z-scores\n",
    "        mu20 = vol.rolling(20, min_periods=5).mean()\n",
    "        sd20 = vol.rolling(20, min_periods=5).std(ddof=0)\n",
    "        out['vol_z_20'] = (vol - mu20) / sd20.replace(0, np.nan)\n",
    "\n",
    "        mu60 = vol.rolling(60, min_periods=15).mean()\n",
    "        sd60 = vol.rolling(60, min_periods=15).std(ddof=0)\n",
    "        out['vol_z_60'] = (vol - mu60) / sd60.replace(0, np.nan)\n",
    "\n",
    "        # relative volume\n",
    "        out['rvol_20'] = vol / out['vol_ma_20'].replace(0, np.nan)\n",
    "        out['rvol_50'] = vol / out['vol_ma_50'].replace(0, np.nan)\n",
    "\n",
    "    # dollar volume & OBV need price; they’ll be skipped if price missing\n",
    "    px_col = 'adjclose' if (OBV_USE_ADJCLOSE and 'adjclose' in df.columns) else ('close' if 'close' in df.columns else None)\n",
    "    if px_col is not None and has_vol:\n",
    "        px = pd.to_numeric(df[px_col], errors='coerce')\n",
    "        vol = pd.to_numeric(df['volume'], errors='coerce')\n",
    "        dvol = (px * vol)\n",
    "        dvol_ma20 = dvol.rolling(20, min_periods=5).mean()\n",
    "        out['dollar_vol_ma_20'] = dvol_ma20\n",
    "        out['rdollar_vol_20']   = dvol / dvol_ma20.replace(0, np.nan)\n",
    "\n",
    "        obv = (np.sign(px.diff()).fillna(0.0) * vol).fillna(0.0).cumsum()\n",
    "        out['obv'] = obv\n",
    "        obv_mu = obv.rolling(60, min_periods=20).mean()\n",
    "        obv_sd = obv.rolling(60, min_periods=20).std(ddof=0)\n",
    "        out['obv_z_60'] = (obv - obv_mu) / obv_sd.replace(0, np.nan)\n",
    "\n",
    "    # cast light\n",
    "    for c in out.columns:\n",
    "        out[c] = pd.to_numeric(out[c], errors='coerce').astype('float32')\n",
    "    return out\n",
    "\n",
    "def _vol_worker(sym: str):\n",
    "    df = indicators_by_symbol[sym]\n",
    "    small = _basic_volume_features(df)\n",
    "    # Always return the frame, even if all NaN columns (not empty)\n",
    "    return sym, small\n",
    "\n",
    "# Ensure OHLCV columns are present in indicators_by_symbol\n",
    "# Columns we want to pull in from `data`\n",
    "cols_to_add = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "for sym, df in indicators_by_symbol.items():\n",
    "    for col in cols_to_add:\n",
    "        if col in data and sym in data[col]:\n",
    "            # Get the Series for this column/symbol\n",
    "            series = data[col][sym]\n",
    "\n",
    "            # Align to main DataFrame index\n",
    "            series = series.reindex(df.index)\n",
    "\n",
    "            # Lowercase column name\n",
    "            col_lower = col.lower()\n",
    "\n",
    "            # Add or overwrite\n",
    "            if col_lower not in df.columns:\n",
    "                df[col_lower] = series\n",
    "            else:\n",
    "                df[col_lower].update(series)\n",
    "\n",
    "    indicators_by_symbol[sym] = df\n",
    "# indicators_by_symbol['TSLA']#['Open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d7af2-4a51-49fb-833a-6e867af2d62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# run (threads) and write back explicitly\n",
    "results = Parallel(\n",
    "    n_jobs=min(8, os.cpu_count() or 4),\n",
    "    backend=\"threading\",\n",
    "    prefer=\"threads\",\n",
    "    verbose=0\n",
    ")(\n",
    "    delayed(_vol_worker)(sym) for sym in indicators_by_symbol.keys()\n",
    ")\n",
    "\n",
    "added_report = []\n",
    "for sym, small_df in results:\n",
    "    base = indicators_by_symbol[sym]\n",
    "    # ensure index alignment\n",
    "    small_df = small_df.reindex(base.index)\n",
    "\n",
    "    # add/overwrite columns\n",
    "    before = set(base.columns)\n",
    "    for col in small_df.columns:\n",
    "        base[col] = small_df[col]\n",
    "    indicators_by_symbol[sym] = base  # explicit write-back\n",
    "\n",
    "    added = sorted(set(base.columns) - before)\n",
    "    # keep a tiny sample report for sanity\n",
    "    if added:\n",
    "        added_report.append((sym, added[:4] + (['...'] if len(added) > 4 else [])))\n",
    "\n",
    "# Optional: quick confirmation print for a few symbols\n",
    "for sym, cols in added_report[:5]:\n",
    "    print(f\"[volume] {sym}: added {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3f03e-419d-41e9-a91b-bb5d62593555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RANGE_LOOKBACKS = (5, 10, 20)\n",
    "\n",
    "def _range_features_one(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute range-based features for a single symbol DataFrame.\n",
    "    Requires columns: 'open','high','low','close' (lowercase).\n",
    "    Uses 'atr_percent' and 'rvol_20' if present for extras.\n",
    "    \"\"\"\n",
    "    need = {'open','high','low','close'}\n",
    "    if not need.issubset(df.columns):\n",
    "        return pd.DataFrame(index=df.index)  # nothing to do\n",
    "\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "\n",
    "    o = pd.to_numeric(df['open'], errors='coerce')\n",
    "    h = pd.to_numeric(df['high'], errors='coerce')\n",
    "    l = pd.to_numeric(df['low'],  errors='coerce')\n",
    "    c = pd.to_numeric(df['close'],errors='coerce')\n",
    "\n",
    "    # Basic ranges\n",
    "    hl = h - l\n",
    "    out['hl_range'] = hl\n",
    "    out['hl_range_pct_close'] = hl / c.replace(0, np.nan)\n",
    "\n",
    "    # True range (Wilder)\n",
    "    prev_c = c.shift(1)\n",
    "    tr = pd.concat([h - l, (h - prev_c).abs(), (l - prev_c).abs()], axis=1).max(axis=1)\n",
    "    out['true_range'] = tr\n",
    "    out['tr_pct_close'] = tr / c.replace(0, np.nan)\n",
    "\n",
    "    # Rolling n-day constructs\n",
    "    for n in RANGE_LOOKBACKS:\n",
    "        n_high = h.rolling(n, min_periods=max(3, n//3)).max()\n",
    "        n_low  = l.rolling(n, min_periods=max(3, n//3)).min()\n",
    "        n_rng  = n_high - n_low\n",
    "        n_rng_ma = hl.rolling(n, min_periods=max(3, n//3)).mean()\n",
    "        n_rng_mu = hl.rolling(n, min_periods=max(3, n//3)).mean()\n",
    "        n_rng_sd = hl.rolling(n, min_periods=max(3, n//3)).std(ddof=0)\n",
    "\n",
    "        out[f'{n}d_high'] = n_high\n",
    "        out[f'{n}d_low']  = n_low\n",
    "        out[f'{n}d_range'] = n_rng\n",
    "        out[f'{n}d_range_pct_close'] = n_rng / c.replace(0, np.nan)\n",
    "\n",
    "        denom = (n_high - n_low).replace(0, np.nan)\n",
    "        out[f'pos_in_{n}d_range'] = (c - n_low) / denom\n",
    "\n",
    "        out[f'breakout_up_{n}d'] = (c - n_high) / n_high.replace(0, np.nan)\n",
    "        out[f'breakout_dn_{n}d'] = (c - n_low)  / n_low.replace(0, np.nan)\n",
    "\n",
    "        out[f'range_expansion_{n}d'] = hl / n_rng_ma.replace(0, np.nan)\n",
    "        out[f'range_z_{n}d'] = (hl - n_rng_mu) / n_rng_sd.replace(0, np.nan)\n",
    "\n",
    "    # Gaps\n",
    "    out['gap_pct'] = (o - prev_c) / prev_c.replace(0, np.nan)\n",
    "\n",
    "    if 'atr_percent' in df.columns:\n",
    "        atrp = pd.to_numeric(df['atr_percent'], errors='coerce')  # already percent\n",
    "        out['gap_atr_ratio'] = out['gap_pct'] / (atrp.replace(0, np.nan) / 100.0)\n",
    "\n",
    "    # Interaction with RVOL if available\n",
    "    if 'rvol_20' in df.columns:\n",
    "        rvol20 = pd.to_numeric(df['rvol_20'], errors='coerce')\n",
    "        out['range_x_rvol20'] = out['hl_range_pct_close'] * rvol20\n",
    "\n",
    "    # Light dtypes\n",
    "    for ccol in out.columns:\n",
    "        out[ccol] = pd.to_numeric(out[ccol], errors='coerce').astype('float32')\n",
    "\n",
    "    return out\n",
    "\n",
    "def _range_worker(sym: str):\n",
    "    base = indicators_by_symbol[sym]\n",
    "    small = _range_features_one(base)\n",
    "    return sym, small\n",
    "\n",
    "# --- run (threads) and join into indicators_by_symbol ---\n",
    "results = Parallel(\n",
    "    n_jobs=min(8, os.cpu_count() or 4),\n",
    "    backend=\"threading\",\n",
    "    prefer=\"threads\",\n",
    "    verbose=0\n",
    ")(\n",
    "    delayed(_range_worker)(sym) for sym in indicators_by_symbol.keys()\n",
    ")\n",
    "\n",
    "for sym, small_df in results:\n",
    "    if small_df is not None and not small_df.empty:\n",
    "        base = indicators_by_symbol[sym]\n",
    "        small_df = small_df.reindex(base.index)\n",
    "        for col in small_df.columns:\n",
    "            base[col] = small_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750a861-81db-409f-909d-3e64bd4fe1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def _forward_returns(df: pd.DataFrame, max_days: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute forward returns from 1 to max_days trading days out.\n",
    "    Returns a DataFrame with columns: ret_1d ... ret_{max_days}d\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    if \"adjclose\" not in df.columns:\n",
    "        return out\n",
    "\n",
    "    px = pd.to_numeric(df[\"adjclose\"], errors=\"coerce\")\n",
    "    for n in range(1, max_days + 1):\n",
    "        out[f\"ret_{n}d\"] = px.shift(-n) / px - 1\n",
    "\n",
    "    # lighter dtype\n",
    "    for c in out.columns:\n",
    "        out[c] = out[c].astype(\"float32\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _ret_worker(sym: str, max_days: int = 20):\n",
    "    df = indicators_by_symbol[sym]\n",
    "    small = _forward_returns(df, max_days=max_days)\n",
    "    return sym, small\n",
    "\n",
    "\n",
    "# --- run in parallel and join ---\n",
    "results = Parallel(\n",
    "    n_jobs=min(8, os.cpu_count() or 4),\n",
    "    backend=\"threading\",\n",
    "    prefer=\"threads\",\n",
    "    verbose=0\n",
    ")(\n",
    "    delayed(_ret_worker)(sym, max_days=20) for sym in indicators_by_symbol.keys()\n",
    ")\n",
    "\n",
    "for sym, small_df in results:\n",
    "    if small_df is not None and not small_df.empty:\n",
    "        base = indicators_by_symbol[sym]\n",
    "        small_df = small_df.reindex(base.index)\n",
    "        for col in small_df.columns:\n",
    "            base[col] = small_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fbc16-9085-438b-b2fe-f5c18eebd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def _volvol_features_one(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute rolling volume volatility features on a single symbol frame.\n",
    "    Requires a 'volume' column (lowercase).\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    if 'volume' not in df.columns:\n",
    "        return out\n",
    "\n",
    "    v = pd.to_numeric(df['volume'], errors='coerce')\n",
    "\n",
    "    # Rolling std of volume (short & long)\n",
    "    out['vol_rolling_20d'] = v.rolling(20, min_periods=10).std(ddof=0)\n",
    "    out['vol_rolling_60d'] = v.rolling(60, min_periods=20).std(ddof=0)\n",
    "\n",
    "    # Vol of vol: std of the 20d rolling std over another 20d window\n",
    "    out['vol_of_vol_20d'] = out['vol_rolling_20d'].rolling(20, min_periods=10).std(ddof=0)\n",
    "\n",
    "    # Light dtypes\n",
    "    for c in out.columns:\n",
    "        out[c] = pd.to_numeric(out[c], errors='coerce').astype('float32')\n",
    "\n",
    "    return out\n",
    "\n",
    "def _volvol_worker(sym: str):\n",
    "    base = indicators_by_symbol[sym]\n",
    "    small = _volvol_features_one(base)\n",
    "    return sym, small\n",
    "\n",
    "# Use threads to avoid pickling large frames; adjust n_jobs as you like\n",
    "results = Parallel(\n",
    "    n_jobs=min(8, os.cpu_count() or 4),\n",
    "    backend=\"threading\",\n",
    "    prefer=\"threads\",\n",
    "    verbose=0\n",
    ")(\n",
    "    delayed(_volvol_worker)(sym) for sym in indicators_by_symbol.keys()\n",
    ")\n",
    "\n",
    "# Join back into the master dict\n",
    "for sym, small_df in results:\n",
    "    if small_df is not None and not small_df.empty:\n",
    "        base = indicators_by_symbol[sym]\n",
    "        small_df = small_df.reindex(base.index)\n",
    "        for col in small_df.columns:\n",
    "            base[col] = small_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cac47a-3eb2-43cc-8e30-66802b0eb917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71d838b-44e3-4717-afc5-fafb2b2fc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- 1) Provide or load your S&P 500 list ----\n",
    "# Replace this with your maintained list (or load from a CSV)\n",
    "sp500_tickers = [\"MMM\", \"AOS\", \"ABT\", \"ABBV\", \"ACN\", \"ADBE\", \"AMD\", \"AES\", \"AFL\", \"A\", \n",
    "                 \"APD\", \"ABNB\", \"AKAM\", \"ALB\", \"ARE\", \"ALGN\", \"ALLE\", \"LNT\", \"ALL\", \"GOOGL\", \n",
    "                 \"GOOG\", \"MO\", \"AMZN\", \"AMCR\", \"AEE\", \"AEP\", \"AXP\", \"AIG\", \"AMT\", \"AWK\", \n",
    "                 \"AMP\", \"AME\", \"AMGN\", \"APH\", \"ADI\", \"AON\", \"APA\", \"APO\", \"AAPL\", \"AMAT\", \n",
    "                 \"APTV\", \"ACGL\", \"ADM\", \"ANET\", \"AJG\", \"AIZ\", \"T\", \"ATO\", \"ADSK\", \"ADP\", \n",
    "                 \"AZO\", \"AVB\", \"AVY\", \"AXON\", \"BKR\", \"BALL\", \"BAC\", \"BAX\", \"BDX\", \"BRK-B\", \n",
    "                 \"BBY\", \"TECH\", \"BIIB\", \"BLK\", \"BX\", \"XYZ\", \"BK\", \"BA\", \"BKNG\", \"BSX\", \"BMY\", \n",
    "                 \"AVGO\", \"BR\", \"BRO\", \"BF-B\", \"BLDR\", \"BG\", \"BXP\", \"CHRW\", \"CDNS\", \"CZR\",\n",
    "                 \"CPT\", \"CPB\", \"COF\", \"CAH\", \"KMX\", \"CCL\", \"CARR\", \"CAT\", \"CBOE\", \"CBRE\", \n",
    "                 \"CDW\", \"COR\", \"CNC\", \"CNP\", \"CF\", \"CRL\", \"SCHW\", \"CHTR\", \"CVX\", \"CMG\", \"CB\", \n",
    "                 \"CHD\", \"CI\", \"CINF\", \"CTAS\", \"CSCO\", \"C\", \"CFG\", \"CLX\", \"CME\", \"CMS\", \"KO\", \n",
    "                 \"CTSH\", \"COIN\", \"CL\", \"CMCSA\", \"CAG\", \"COP\", \"ED\", \"STZ\", \"CEG\", \"COO\", \"CPRT\", \n",
    "                 \"GLW\", \"CPAY\", \"CTVA\", \"CSGP\", \"COST\", \"CTRA\", \"CRWD\", \"CCI\", \"CSX\", \"CMI\", \n",
    "                 \"CVS\", \"DHR\", \"DRI\", \"DDOG\", \"DVA\", \"DAY\", \"DECK\", \"DE\", \"DELL\", \"DAL\", \"DVN\",\n",
    "                 \"DXCM\", \"FANG\", \"DLR\", \"DG\", \"DLTR\", \"D\", \"DPZ\", \"DASH\", \"DOV\", \"DOW\", \"DHI\", \n",
    "                 \"DTE\", \"DUK\", \"DD\", \"EMN\", \"ETN\", \"EBAY\", \"ECL\", \"EIX\", \"EW\", \"EA\", \"ELV\", \"EMR\", \n",
    "                 \"ENPH\", \"ETR\", \"EOG\", \"EPAM\", \"EQT\", \"EFX\", \"EQIX\", \"EQR\", \"ERIE\", \"ESS\", \"EL\",\n",
    "                 \"EG\", \"EVRG\", \"ES\", \"EXC\", \"EXE\", \"EXPE\", \"EXPD\", \"EXR\", \"XOM\", \"FFIV\", \"FDS\", \n",
    "                 \"FICO\", \"FAST\", \"FRT\", \"FDX\", \"FIS\", \"FITB\", \"FSLR\", \"FE\", \"FI\", \"F\", \"FTNT\", \n",
    "                 \"FTV\", \"FOXA\", \"FOX\", \"BEN\", \"FCX\", \"GRMN\", \"IT\", \"GE\", \"GEHC\", \"GEV\", \"GEN\", \n",
    "                 \"GNRC\", \"GD\", \"GIS\", \"GM\", \"GPC\", \"GILD\", \"GPN\", \"GL\", \"GDDY\", \"GS\", \"HAL\",\n",
    "                 \"HIG\", \"HAS\", \"HCA\", \"DOC\", \"HSIC\", \"HSY\", \"HPE\", \"HLT\", \"HOLX\", \"HD\", \"HON\",\n",
    "                 \"HRL\", \"HST\", \"HWM\", \"HPQ\", \"HUBB\", \"HUM\", \"HBAN\", \"HII\", \"IBM\", \"IEX\", \"IDXX\", \n",
    "                 \"ITW\", \"INCY\", \"IR\", \"PODD\", \"INTC\", \"ICE\", \"IFF\", \"IP\", \"IPG\", \"INTU\", \"ISRG\",\n",
    "                 \"IVZ\", \"INVH\", \"IQV\", \"IRM\", \"JBHT\", \"JBL\", \"JKHY\", \"J\", \"JNJ\", \"JCI\", \"JPM\", \n",
    "                 \"K\", \"KVUE\", \"KDP\", \"KEY\", \"KEYS\", \"KMB\", \"KIM\", \"KMI\", \"KKR\", \"KLAC\", \"KHC\", \n",
    "                 \"KR\", \"LHX\", \"LH\", \"LRCX\", \"LW\", \"LVS\", \"LDOS\", \"LEN\", \"LII\", \"LLY\", \"LIN\", \"LYV\", \n",
    "                 \"LKQ\", \"LMT\", \"L\", \"LOW\", \"LULU\", \"LYB\", \"MTB\", \"MPC\", \"MKTX\", \"MAR\", \"MMC\", \"MLM\", \n",
    "                 \"MAS\", \"MA\", \"MTCH\", \"MKC\", \"MCD\", \"MCK\", \"MDT\", \"MRK\", \"META\", \"MET\", \"MTD\", \"MGM\", \n",
    "                 \"MCHP\", \"MU\", \"MSFT\", \"MAA\", \"MRNA\", \"MHK\", \"MOH\", \"TAP\", \"MDLZ\", \"MPWR\", \"MNST\", \n",
    "                 \"MCO\", \"MS\", \"MOS\", \"MSI\", \"MSCI\", \"NDAQ\", \"NTAP\", \"NFLX\", \"NEM\", \"NWSA\", \"NWS\", \n",
    "                 \"NEE\", \"NKE\", \"NI\", \"NDSN\", \"NSC\", \"NTRS\", \"NOC\", \"NCLH\", \"NRG\", \"NUE\", \"NVDA\", \n",
    "                 \"NVR\", \"NXPI\", \"ORLY\", \"OXY\", \"ODFL\", \"OMC\", \"ON\", \"OKE\", \"ORCL\", \"OTIS\", \"PCAR\",\n",
    "                 \"PKG\", \"PLTR\", \"PANW\", \"PH\", \"PAYX\", \"PAYC\", \"PYPL\", \"PNR\", \"PEP\", \"PFE\", \"PCG\",\n",
    "                 \"PM\", \"PSX\", \"PNW\", \"PNC\", \"POOL\", \"PPG\", \"PPL\", \"PFG\", \"PG\", \"PGR\", \"PLD\", \"PRU\", \n",
    "                 \"PEG\", \"PTC\", \"PSA\", \"PHM\", \"PWR\", \"QCOM\", \"DGX\", \"RL\", \"RJF\", \"RTX\", \"O\", \"REG\",\n",
    "                 \"REGN\", \"RF\", \"RSG\", \"RMD\", \"RVTY\", \"ROK\", \"ROL\", \"ROP\", \"ROST\", \"RCL\", \"SPGI\", \n",
    "                 \"CRM\", \"SBAC\", \"SLB\", \"STX\", \"SRE\", \"NOW\", \"SHW\", \"SPG\", \"SWKS\", \"SJM\", \"SW\", \"SNA\",\n",
    "                 \"SOLV\", \"SO\", \"LUV\", \"SWK\", \"SBUX\", \"STT\", \"STLD\", \"STE\", \"SYK\", \"SMCI\", \"SYF\", \"SNPS\", \n",
    "                 \"SYY\", \"TMUS\", \"TROW\", \"TTWO\", \"TPR\", \"TRGP\", \"TGT\", \"TEL\", \"TDY\", \"TER\", \"TSLA\", \n",
    "                 \"TXN\", \"TPL\", \"TXT\", \"TMO\", \"TJX\", \"TKO\", \"TTD\", \"TSCO\", \"TT\", \"TDG\", \"TRV\", \"TRMB\", \n",
    "                 \"TFC\", \"TYL\", \"TSN\", \"USB\", \"UBER\", \"UDR\", \"ULTA\", \"UNP\", \"UAL\", \"UPS\", \"URI\", \"UNH\", \n",
    "                 \"UHS\", \"VLO\", \"VTR\", \"VLTO\", \"VRSN\", \"VRSK\", \"VZ\", \"VRTX\", \"VTRS\", \"VICI\", \"V\", \"VST\", \n",
    "                 \"VMC\", \"WRB\", \"GWW\", \"WAB\", \"WBA\", \"WMT\", \"DIS\", \"WBD\", \"WM\", \"WAT\", \"WEC\", \"WFC\", \"WELL\", \n",
    "                 \"WST\", \"WDC\", \"WY\", \"WSM\", \"WMB\", \"WTW\", \"WDAY\", \"WYNN\", \"XEL\", \"XYL\", \"YUM\", \"ZBRA\", \"ZBH\", \"ZTS\"]\n",
    "sp500_tickers = [t for t in sp500_tickers if t in indicators_by_symbol]\n",
    "\n",
    "# --- 2) Build per-symbol \"above MAxx\" indicators ---\n",
    "above_ma20 = {}\n",
    "above_ma50 = {}\n",
    "above_ma200 = {}\n",
    "\n",
    "for sym in sp500_tickers:\n",
    "    df = indicators_by_symbol[sym]\n",
    "\n",
    "    # Choose a price column\n",
    "    price_col = 'close' if 'close' in df.columns else ('adjclose' if 'adjclose' in df.columns else None)\n",
    "    if price_col is None:\n",
    "        continue\n",
    "\n",
    "    px = pd.to_numeric(df[price_col], errors='coerce')\n",
    "\n",
    "    # MA20\n",
    "    if 'ma_20' in df.columns:\n",
    "        ma20 = pd.to_numeric(df['ma_20'], errors='coerce')\n",
    "    else:\n",
    "        ma20 = px.rolling(20, min_periods=10).mean()\n",
    "    above_ma20[sym] = (px > ma20).astype('float32')\n",
    "\n",
    "    # MA50\n",
    "    if 'ma_50' in df.columns:\n",
    "        ma50 = pd.to_numeric(df['ma_50'], errors='coerce')\n",
    "    else:\n",
    "        ma50 = px.rolling(50, min_periods=25).mean()\n",
    "    above_ma50[sym] = (px > ma50).astype('float32')\n",
    "\n",
    "    # MA200\n",
    "    if 'ma_200' in df.columns:\n",
    "        ma200 = pd.to_numeric(df['ma_200'], errors='coerce')\n",
    "    else:\n",
    "        ma200 = px.rolling(200, min_periods=100).mean()\n",
    "    above_ma200[sym] = (px > ma200).astype('float32')\n",
    "\n",
    "# --- 3) Bail gracefully if no data ---\n",
    "if not above_ma50:\n",
    "    raise ValueError(\"No S&P 500 members with usable price/MA data found in indicators_by_symbol.\")\n",
    "\n",
    "# --- 4) Combine to compute % above per date ---\n",
    "panel_20 = pd.DataFrame(above_ma20)\n",
    "panel_50 = pd.DataFrame(above_ma50)\n",
    "panel_200 = pd.DataFrame(above_ma200)\n",
    "\n",
    "pct_above_20 = panel_20.mean(axis=1, skipna=True) * 100.0\n",
    "pct_above_50 = panel_50.mean(axis=1, skipna=True) * 100.0\n",
    "pct_above_200 = panel_200.mean(axis=1, skipna=True) * 100.0\n",
    "\n",
    "pct_above_20.name = \"pct_sp500_above_ma20\"\n",
    "pct_above_50.name = \"pct_sp500_above_ma50\"\n",
    "pct_above_200.name = \"pct_sp500_above_ma200\"\n",
    "\n",
    "# --- 5) Attach breadth series to every symbol’s DataFrame ---\n",
    "for sym, df in indicators_by_symbol.items():\n",
    "    df['pct_sp500_above_ma20'] = pd.to_numeric(pct_above_20.reindex(df.index), errors='coerce').astype('float32')\n",
    "    df['pct_sp500_above_ma50'] = pd.to_numeric(pct_above_50.reindex(df.index), errors='coerce').astype('float32')\n",
    "    df['pct_sp500_above_ma200'] = pd.to_numeric(pct_above_200.reindex(df.index), errors='coerce').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f7e43-24be-482c-a22a-7b2ec80d8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_universe = [t for t in sp500_tickers if t in indicators_by_symbol]\n",
    "\n",
    "# Keep only tickers we actually have data for\n",
    "\n",
    "# --- 1) Build a daily change sign panel for the S&P 500 ---\n",
    "chg_sign = {}\n",
    "for sym in sp500_universe:\n",
    "    df = indicators_by_symbol[sym]\n",
    "    # choose a price column\n",
    "    price_col = 'close' if 'close' in df.columns else ('adjclose' if 'adjclose' in df.columns else None)\n",
    "    if price_col is None:\n",
    "        continue\n",
    "    px = pd.to_numeric(df[price_col], errors='coerce')\n",
    "    chg = px.diff()\n",
    "    # sign: +1 advance, -1 decline, 0 unchanged/NaN\n",
    "    chg_sign[sym] = np.sign(chg).replace({np.nan: 0}).astype('int8')\n",
    "\n",
    "if not chg_sign:\n",
    "    raise ValueError(\"Could not compute change signs for any S&P 500 members.\")\n",
    "\n",
    "panel = pd.DataFrame(chg_sign)  # index: dates, columns: symbols\n",
    "\n",
    "# --- 2) Daily breadth stats ---\n",
    "adv = (panel > 0).sum(axis=1).astype('float32')\n",
    "dec = (panel < 0).sum(axis=1).astype('float32')\n",
    "tot = (panel != 0).sum(axis=1).astype('float32')  # excludes unchanged\n",
    "\n",
    "ad_net  = (adv - dec).astype('float32')\n",
    "ad_line = ad_net.fillna(0).cumsum().astype('float32')\n",
    "pct_adv = (adv / tot.replace(0, np.nan) * 100.0).astype('float32')\n",
    "\n",
    "adv.name = \"sp500_adv\"\n",
    "dec.name = \"sp500_dec\"\n",
    "ad_net.name = \"sp500_ad_net\"\n",
    "ad_line.name = \"sp500_ad_line\"\n",
    "pct_adv.name = \"sp500_pct_advancing\"\n",
    "\n",
    "# --- 3) Attach to EVERY symbol’s DataFrame ---\n",
    "for sym, df in indicators_by_symbol.items():\n",
    "    idx = df.index\n",
    "    df['sp500_adv']           = adv.reindex(idx).astype('float32')\n",
    "    df['sp500_dec']           = dec.reindex(idx).astype('float32')\n",
    "    df['sp500_ad_net']        = ad_net.reindex(idx).astype('float32')\n",
    "    df['sp500_ad_line']       = ad_line.reindex(idx).astype('float32')\n",
    "    df['sp500_pct_advancing'] = pct_adv.reindex(idx).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b409d9-4619-45aa-b8f3-844c0f2a4ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in indicators_by_symbol['TSLA'].columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04ecce-6909-4b2e-ad73-50d79adaeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# First series: hurst (left y-axis)\n",
    "color1 = 'tab:blue'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Hurst (emaHL5)', color=color1)\n",
    "df['hurst_ret_128_emaHL5'].plot(ax=ax1, color=color1, label='Hurst (emaHL5)')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "# Second series: trend_score (right y-axis)\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'tab:orange'\n",
    "ax2.set_ylabel('Trend Score', color=color2)\n",
    "df['trend_persist_ema'].plot(ax=ax2, color=color2, label='Trend Score')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# Optional: align legends\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "plt.title('Hurst vs Trend Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320236d-09fc-461c-b812-ac15de3ba7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('ATR%', color='tab:green')\n",
    "df['atr_percent'].plot(ax=ax1, color='tab:green', label='ATR%')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:green')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Trend Score', color='tab:orange')\n",
    "df['trend_persist_ema'].plot(ax=ax2, color='tab:orange', label='Trend Score')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "plt.title('ATR% vs Trend Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c19cf81-7b31-45d4-8afb-314404e69968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_atr_trend_quadrants(df, symbol):\n",
    "    # Pick the two features\n",
    "    x = df['trend_score']\n",
    "    y = df['atr_percent']\n",
    "\n",
    "    # Midlines (could use median, mean, or 0 for trend)\n",
    "    trend_mid = 0\n",
    "    atr_mid = y.median()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.scatter(x, y, alpha=0.5, edgecolor='k', s=40)\n",
    "\n",
    "    # Draw quadrant lines\n",
    "    ax.axvline(trend_mid, color='black', linestyle='--', alpha=0.7)\n",
    "    ax.axhline(atr_mid, color='black', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Annotate quadrants\n",
    "    ax.text(0.75, 0.95, 'High Trend\\nHigh ATR%', transform=ax.transAxes,\n",
    "            ha='center', va='top', fontsize=12, color='darkgreen')\n",
    "    ax.text(0.25, 0.95, 'Low Trend\\nHigh ATR%', transform=ax.transAxes,\n",
    "            ha='center', va='top', fontsize=12, color='red')\n",
    "    ax.text(0.75, 0.05, 'High Trend\\nLow ATR%', transform=ax.transAxes,\n",
    "            ha='center', va='bottom', fontsize=12, color='green')\n",
    "    ax.text(0.25, 0.05, 'Low Trend\\nLow ATR%', transform=ax.transAxes,\n",
    "            ha='center', va='bottom', fontsize=12, color='gray')\n",
    "\n",
    "    ax.set_xlabel('Trend Score')\n",
    "    ax.set_ylabel('ATR %')\n",
    "    ax.set_title(f'ATR% vs Trend Score Quadrants – {symbol}')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_atr_trend_quadrants(indicators_by_symbol['AAPL'], 'AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a23202-741e-4f06-8b80-153a391a8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_heatmap(df, x_col, y_col, price_col='adjclose', horizon=5, bins=6,\n",
    "                         use_quantile_bins=True, cmap='RdYlGn', title=None,\n",
    "                         mark_current=True):\n",
    "    \"\"\"\n",
    "    Heatmap of mean forward return by (x,y) feature bins, with support counts.\n",
    "    If mark_current=True, overlay a larger, semi-transparent marker at the most recent (x,y) point.\n",
    "    \"\"\"\n",
    "    d = df[[x_col, y_col, price_col]].dropna().copy()\n",
    "    d['fwd_ret'] = d[price_col].shift(-horizon) / d[price_col] - 1.0\n",
    "    d = d.dropna(subset=['fwd_ret'])\n",
    "\n",
    "    # Bin features\n",
    "    if use_quantile_bins:\n",
    "        x_bins = pd.qcut(d[x_col], q=bins, duplicates='drop')\n",
    "        y_bins = pd.qcut(d[y_col], q=bins, duplicates='drop')\n",
    "    else:\n",
    "        x_bins = pd.cut(d[x_col], bins=bins)\n",
    "        y_bins = pd.cut(d[y_col], bins=bins)\n",
    "\n",
    "    # Aggregations\n",
    "    pivot_mean = pd.pivot_table(d, index=y_bins, columns=x_bins, values='fwd_ret', aggfunc='mean')\n",
    "    pivot_cnt  = pd.pivot_table(d, index=y_bins, columns=x_bins, values='fwd_ret', aggfunc='count')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    im = ax.imshow(pivot_mean.values, cmap=cmap, origin='lower',\n",
    "                   vmin=np.nanpercentile(pivot_mean.values, 5),\n",
    "                   vmax=np.nanpercentile(pivot_mean.values, 95))\n",
    "\n",
    "    # Axis ticks/labels\n",
    "    ax.set_xticks(range(pivot_mean.shape[1]))\n",
    "    ax.set_yticks(range(pivot_mean.shape[0]))\n",
    "    ax.set_xticklabels([str(c) for c in pivot_mean.columns], rotation=45, ha='right')\n",
    "    ax.set_yticklabels([str(i) for i in pivot_mean.index])\n",
    "\n",
    "    # Annotate with counts (support)\n",
    "    for i in range(pivot_mean.shape[0]):\n",
    "        for j in range(pivot_mean.shape[1]):\n",
    "            val = pivot_mean.values[i, j]\n",
    "            cnt = pivot_cnt.values[i, j]\n",
    "            if np.isfinite(val) and cnt > 0:\n",
    "                ax.text(j, i, f\"{val*100:.1f}%\\n(n={int(cnt)})\",\n",
    "                        ha='center', va='center', fontsize=8, color='black')\n",
    "\n",
    "    # ---- Mark the most recent (x,y) point ----\n",
    "    if mark_current:\n",
    "        latest_idx = df[[x_col, y_col]].dropna().index.max()\n",
    "        if pd.notna(latest_idx):\n",
    "            cur_x = df.loc[latest_idx, x_col]\n",
    "            cur_y = df.loc[latest_idx, y_col]\n",
    "\n",
    "            # Map to bin indices\n",
    "            x_cats = x_bins.cat.categories\n",
    "            y_cats = y_bins.cat.categories\n",
    "            xi = x_cats.get_indexer([cur_x])[0]\n",
    "            yi = y_cats.get_indexer([cur_y])[0]\n",
    "\n",
    "            if xi < 0:\n",
    "                xi = np.clip(np.searchsorted(x_cats.left, cur_x) - 1, 0, len(x_cats)-1)\n",
    "            if yi < 0:\n",
    "                yi = np.clip(np.searchsorted(y_cats.left, cur_y) - 1, 0, len(y_cats)-1)\n",
    "\n",
    "            # Bigger, transparent marker\n",
    "            ax.scatter(xi, yi, s=300, color='cyan', alpha=0.5,\n",
    "                       edgecolor='black', linewidths=1.2, zorder=5)\n",
    "            ax.annotate(str(latest_idx.date()), (xi, yi), xytext=(10, 10),\n",
    "                        textcoords='offset points', fontsize=9, color='black',\n",
    "                        bbox=dict(boxstyle='round,pad=0.2', fc='white', alpha=0.6))\n",
    "\n",
    "    ax.set_xlabel(x_col)\n",
    "    ax.set_ylabel(y_col)\n",
    "    ax.set_title(title or f\"{y_col} vs {x_col} — mean {horizon}D forward return\")\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Mean forward return')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: trend vs hurst map for one symbol\n",
    "df = indicators_by_symbol['TSLA']  # or any symbol\n",
    "plot_feature_heatmap(df, x_col='rel_strength_sector', y_col='pct_dist_ma_200_z',\n",
    "                     price_col='adjclose', horizon=5, bins=6, use_quantile_bins=True,\n",
    "                     title='AAPL: 10D mean return by Sector Strenght × Hurst (smoothed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5af75-5322-4f43-83f5-01a9aea78779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., did we hit +2% within horizon?\n",
    "d['hit_up_2pct'] = (d[price_col].shift(-horizon).rolling(horizon).max() / d[price_col] - 1 >= 0.02).astype(int)\n",
    "pivot_mean = pd.pivot_table(d, index=y_bins, columns=x_bins, values='hit_up_2pct', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a5af4-0588-441c-b8fd-118ce5e776e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df[\"ret_10d\"] = df[\"adjclose\"].shift(-10) / df[\"adjclose\"] - 1\n",
    "target_col = \"ret_10d\"  # replace with your actual 10-day return column name\n",
    "\n",
    "# Compute Pearson correlations\n",
    "corr_with_target = df.corr(method=\"pearson\")[target_col].drop(target_col)\n",
    "\n",
    "# Sort descending by absolute correlation strength\n",
    "top2 = corr_with_target.abs().sort_values(ascending=False).head(20)\n",
    "\n",
    "print(\"Top 5 features correlated with 10-day return:\")\n",
    "print(top2)\n",
    "\n",
    "# Optional: actual signed correlations\n",
    "print(\"\\nSigned correlations:\")\n",
    "print(corr_with_target[top2.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67143a-e60a-43da-af8f-82e1531d7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5b3a2-d2f5-4e9c-9e4c-905896b89110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features parquet file\n",
    "import pandas as pd\n",
    "\n",
    "features_df = pd.read_parquet('../artifacts/features_long.parquet')\n",
    "print(f\"✅ Loaded features: {features_df.shape}\")\n",
    "print(f\"Columns: {list(features_df.columns)}\")\n",
    "print(f\"Date range: {features_df['date'].min()} to {features_df['date'].max()}\")\n",
    "print(f\"Unique symbols: {features_df['symbol'].nunique()}\")\n",
    "print(f\"Sample data:\")\n",
    "print(features_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e145fd-dacb-4c47-8060-adb6df6ba0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (svd_forecasting)",
   "language": "python",
   "name": "svd_forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
