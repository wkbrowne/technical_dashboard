{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57173339-f2f1-42a7-9619-b1f5b13b8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from src.io.saving import load_long_parquet\n",
    "from src.features.target_generation import generate_triple_barrier_targets, get_target_summary\n",
    "\n",
    "# Set logging level (optional for debugging)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1a2a5-363d-4e4e-b30c-30cc3be7190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = load_long_parquet(Path(\"artifacts/features_long.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe740e5-06b5-4166-b88f-76a3f3ae0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_rows = df_long[df_long['close'] > df_long['high']+1e-6]\n",
    "bad_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d07787-5bc7-4d42-802f-b292788952ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df_long.reset_index()\n",
    "\n",
    "# Confirm required columns\n",
    "df_long['symbol'] = df_long['symbol'].astype(str)\n",
    "df_long['date'] = pd.to_datetime(df_long['date'], errors='coerce')\n",
    "df_long['atr'] = df_long['atr14']  # Triple barrier logic expects this column\n",
    "\n",
    "required_cols = ['symbol', 'date', 'close', 'high', 'low', 'atr']\n",
    "missing = [col for col in required_cols if col not in df_long.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb3dde-77d8-4d7a-9c52-834d4ac462d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118965aa-9560-4b14-a5bf-bc1c875caebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "\n",
    "def _process_symbol_group(df: pd.DataFrame, config: Dict, symbols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Process a group of symbols by filtering and running triple barrier target generation.\"\"\"\n",
    "    chunk_df = df[df['symbol'].isin(symbols)]  # No need to copy if generate_triple_barrier_targets is read-only\n",
    "    return generate_triple_barrier_targets(chunk_df, config)\n",
    "\n",
    "\n",
    "def generate_targets_parallel(\n",
    "    df: pd.DataFrame,\n",
    "    config: Dict,\n",
    "    n_jobs: int = -1,\n",
    "    chunk_size: int = 25\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Chunk long-format data by symbol and run target generation in parallel.\n",
    "\n",
    "    Args:\n",
    "        df: Long-format DataFrame with columns ['symbol', 'date', 'close', 'high', 'low', 'atr']\n",
    "        config: Triple barrier config dictionary\n",
    "        n_jobs: Number of parallel workers (-1 = all cores)\n",
    "        chunk_size: Number of symbols per parallel chunk\n",
    "\n",
    "    Returns:\n",
    "        Combined DataFrame with triple barrier targets for all symbols\n",
    "    \"\"\"\n",
    "    # Deduplicate and sort symbols to reduce memory fragmentation\n",
    "    unique_symbols = pd.Series(df['symbol'].dropna().unique())\n",
    "    symbol_chunks = [unique_symbols[i:i + chunk_size].tolist() for i in range(0, len(unique_symbols), chunk_size)]\n",
    "\n",
    "    # Avoid re-pickling the entire DataFrame repeatedly by moving it outside parallel calls if possible\n",
    "    df_shared = df.copy(deep=False)  # Avoid unnecessary deep copy\n",
    "    # display(df_shared)\n",
    "    results = Parallel(n_jobs=n_jobs, backend='loky', verbose=1, prefer=\"processes\")(\n",
    "        delayed(_process_symbol_group)(df_shared[df_shared[\"symbol\"].isin(chunk)], config, chunk) for chunk in symbol_chunks\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Use iterator for efficiency if many chunks\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "config = {\n",
    "    'up_mult': 3.0,\n",
    "    'dn_mult': 2.5,\n",
    "    'max_horizon': 20,\n",
    "    'start_every': 3,\n",
    "}\n",
    "\n",
    "targets = generate_targets_parallel(df_long, config, n_jobs=-1, chunk_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e432082-d6c1-43c6-b1f7-9df3dc4c6b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466db6a-7ff0-408b-a84e-fd6fdb29741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper datetime types\n",
    "df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "targets['t0'] = pd.to_datetime(targets['t0'])\n",
    "drop_cols = ['open', 'high', 'low', 'close']  # Add any other overlapping OHLC names if needed\n",
    "targets_clean = targets.drop(columns=[col for col in drop_cols if col in targets.columns])\n",
    "\n",
    "# Merge on symbol + date = t0\n",
    "df_merged = df_long.merge(\n",
    "    targets_clean,\n",
    "    how='inner',\n",
    "    left_on=['symbol', 'date'],\n",
    "    right_on=['symbol', 't0'],\n",
    "    suffixes=('', '_target')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1fe2c-d195-4ed1-ad6e-76f4091d3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === USER CONFIGURABLE ===\n",
    "TICKER   = \"AAPL\"                      # Example ticker\n",
    "START_DT = pd.Timestamp(\"2020-09-03\")  # Entry point to visualize\n",
    "# Column names\n",
    "DATE_COL = 'date'\n",
    "PRICE_COL = 'close'\n",
    "ATR_COL = 'atr14'\n",
    "HIGH_COL = 'high'\n",
    "LOW_COL = 'low'\n",
    "OPEN_COL = 'open'\n",
    "SYMBOL_COL = 'symbol'\n",
    "# --- Get the relevant trajectory row\n",
    "row = df_merged[(df_merged.symbol == TICKER) & (df_merged[\"t0\"] == START_DT)].squeeze()\n",
    "if row.empty:\n",
    "    raise ValueError(\"No trajectory found for given ticker and date\")\n",
    "\n",
    "# --- Price data for the ticker\n",
    "px = df_merged[df_merged.symbol == TICKER].set_index(DATE_COL).sort_index()\n",
    "\n",
    "# --- Extract key trajectory values\n",
    "# t0__up4.0_dn2.0_h21_s5\n",
    "# t_hit__up4.0_dn2.0_h21_s5\n",
    "# hit__up4.0_dn2.0_h21_s5\n",
    "# entry_px__up4.0_dn2.0_h21_s5\n",
    "# top__up4.0_dn2.0_h21_s5\n",
    "# bot__up4.0_dn2.0_h21_s5\n",
    "# h_used__up4.0_dn2.0_h21_s5\n",
    "# price_hit__up4.0_dn2.0_h21_s5\n",
    "# ret_from_entry__up4.0_dn2.0_h21_s5\n",
    "# n_overlapping_trajs__up4.0_dn2.0_h21_s5\n",
    "\n",
    "entry_dt = pd.to_datetime(row['t0'])\n",
    "hit_dt   = pd.to_datetime(row['t_hit'])\n",
    "horizon  = row['h_used']\n",
    "entry_px = row['entry_px']\n",
    "top_px   = row['top']\n",
    "bot_px   = row['bot']\n",
    "hit      = row['hit']\n",
    "\n",
    "# === Get full price trajectory from t0 to t0 + h_used\n",
    "trajectory_mask = (\n",
    "    (df_long['symbol'] == TICKER) &\n",
    "    (df_long['date'] >= entry_dt) &\n",
    "    (df_long['date'] <= entry_dt + pd.Timedelta(days=int(horizon)))\n",
    ")\n",
    "\n",
    "px_window = df_long[trajectory_mask].set_index('date').sort_index().copy()\n",
    "\n",
    "# Pad context around the trajectory\n",
    "pre_window = df_long[\n",
    "    (df_long['symbol'] == TICKER) &\n",
    "    (df_long['date'] < entry_dt)\n",
    "].sort_values('date').tail(10)\n",
    "\n",
    "post_window = df_long[\n",
    "    (df_long['symbol'] == TICKER) &\n",
    "    (df_long['date'] > entry_dt + pd.Timedelta(days=int(horizon)))\n",
    "].sort_values('date').head(10)\n",
    "\n",
    "# Combine the full padded window\n",
    "px_window = pd.concat([\n",
    "    pre_window.set_index('date'),\n",
    "    px_window,\n",
    "    post_window.set_index('date')\n",
    "]).sort_index()\n",
    "\n",
    "print(\"Column mappings:\")\n",
    "print(\"OPEN_COL:\", OPEN_COL)\n",
    "print(\"HIGH_COL:\", HIGH_COL)\n",
    "print(\"LOW_COL:\", LOW_COL)\n",
    "print(\"PRICE_COL:\", PRICE_COL)\n",
    "\n",
    "print(\"\\npx_window sample:\")\n",
    "print(px_window[[OPEN_COL, HIGH_COL, LOW_COL, PRICE_COL]].head(10))\n",
    "\n",
    "# Required OHLC format\n",
    "ohlc = px_window[[OPEN_COL, HIGH_COL, LOW_COL, PRICE_COL]].copy()\n",
    "ohlc.columns = ['Open', 'High', 'Low', 'Close']\n",
    "\n",
    "# --- Count overlapping trajectories\n",
    "active_mask = (targets.symbol == TICKER) & \\\n",
    "              (targets['t0'] <= entry_dt) & \\\n",
    "              ((targets['t_hit'] >= entry_dt) | (targets['hit'].isna()))\n",
    "overlap_count = active_mask.sum()\n",
    "\n",
    "# --- Custom Lines and Markers\n",
    "add_lines = [\n",
    "    mpf.make_addplot([top_px] * len(ohlc), color='green', linestyle='dotted', width=0.8),\n",
    "    mpf.make_addplot([bot_px] * len(ohlc), color='red', linestyle='dotted', width=0.8)\n",
    "]\n",
    "\n",
    "# --- Custom marker for entry and hit\n",
    "entry_idx = ohlc.index.get_loc(entry_dt) if entry_dt in ohlc.index else None\n",
    "hit_idx   = ohlc.index.get_loc(hit_dt) if hit_dt in ohlc.index else None\n",
    "\n",
    "if entry_idx is not None:\n",
    "    add_lines.append(\n",
    "        mpf.make_addplot(\n",
    "            [np.nan if i != entry_idx else entry_px for i in range(len(ohlc))],\n",
    "            type='scatter', markersize=70, marker='o', color='black'\n",
    "        )\n",
    "    )\n",
    "\n",
    "if hit_idx is not None:\n",
    "    add_lines.append(\n",
    "        mpf.make_addplot(\n",
    "            [np.nan if i != hit_idx else row.price_hit for i in range(len(ohlc))],\n",
    "            type='scatter', markersize=70, marker='x', color='purple'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- Plot the candlestick chart\n",
    "mpf.plot(\n",
    "    ohlc,\n",
    "    type='candle',\n",
    "    style='yahoo',\n",
    "    addplot=add_lines,\n",
    "    title=f\"{TICKER} â€” Triple Barrier Trajectory\",\n",
    "    ylabel='Price',\n",
    "    datetime_format='%Y-%m-%d',\n",
    "    xrotation=15,\n",
    "    tight_layout=True,\n",
    "    volume=False,\n",
    "    alines=dict(\n",
    "        alines=[[(entry_dt, ohlc['Low'].min()), (entry_dt, ohlc['High'].max())]],\n",
    "        colors=['gray'], linestyle=':', linewidths=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Metadata display below the chart\n",
    "print(f\"\\nMetadata for {TICKER} on {entry_dt.date()}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Entry Date        : {entry_dt.date()}\")\n",
    "print(f\"Hit Date          : {hit_dt.date()}\")\n",
    "print(f\"Hit Type          : {hit} ({'Top' if hit==1 else 'Bottom' if hit==-1 else 'Horizon'})\")\n",
    "print(f\"Entry Price       : {entry_px:.2f}\")\n",
    "print(f\"Top Barrier       : {top_px:.2f}\")\n",
    "print(f\"Bottom Barrier    : {bot_px:.2f}\")\n",
    "print(f\"Overlapping Count : {overlap_count}\")\n",
    "print(f\"Horizon Used      : {horizon} bars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015a59f-3ccb-4221-8d5c-5b6c67db5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.symbol == TICKER]['t0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53e54f-8051-41e2-bd17-9c23e4f01e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ec2fe-2990-4884-86b0-daf3f956f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    # Moving averages and slopes\n",
    "    \"ma_10\", \"ma_20\", \"ma_50\", \"ma_100\", \"ma_200\",\n",
    "    \"pct_slope_ma_10\", \"pct_slope_ma_20\", \"pct_slope_ma_50\", \"pct_slope_ma_100\", \"pct_slope_ma_200\",\n",
    "    \"pct_dist_ma_20\", \"pct_dist_ma_50\", \"pct_dist_ma_100\", \"pct_dist_ma_200\",\n",
    "    \"pct_dist_ma_20_z\", \"pct_dist_ma_50_z\", \"pct_dist_ma_100_z\", \"pct_dist_ma_200_z\",\n",
    "    \"min_pct_dist_ma\",\n",
    "    \"relative_dist_20_50\", \"relative_dist_20_50_z\",\n",
    "    \n",
    "    # Trend indicators\n",
    "    \"trend_score_granular\", \"trend_score_slope\", \"trend_alignment\", \"trend_persist_ema\",\n",
    "    \"quiet_trend\", \"ret\",\n",
    "    \n",
    "    # Volatility indicators\n",
    "    \"rv_10\", \"rv_20\", \"rv_60\", \"rv_100\",\n",
    "    \"rv_ratio_10_60\", \"rv_ratio_20_100\",\n",
    "    \"rv_z_60\", \"rv60_slope_norm\", \"rv100_slope_norm\",\n",
    "    \"vol_of_vol_20d\", \"volshock_z\", \"volshock_dir\",\n",
    "    \"atr14\", \"atr_percent\", \"gap_atr_ratio\",\n",
    "    \n",
    "    # Range position and breakout\n",
    "    \"pos_in_5d_range\", \"pos_in_10d_range\", \"pos_in_20d_range\",\n",
    "    \"breakout_up_5d\", \"breakout_up_10d\", \"breakout_up_20d\",\n",
    "    \"breakout_dn_5d\", \"breakout_dn_10d\", \"breakout_dn_20d\",\n",
    "    \"range_z_5d\", \"range_z_10d\", \"range_z_20d\",\n",
    "    \n",
    "    # Volume and relative volume\n",
    "    \"vol_z_20\", \"vol_z_60\", \"rvol_20\", \"rvol_50\",\n",
    "    \"dollar_vol_ma_20\", \"rdollar_vol_20\",\n",
    "    \n",
    "    # OBV\n",
    "    \"obv\", \"obv_z_60\",\n",
    "    \n",
    "    # Hurst exponent\n",
    "    \"hurst_ret_64\", \"hurst_ret_64_emaHL5\", \"hurst_ret_128\",\n",
    "    \n",
    "    # Alpha vs benchmark\n",
    "    \"alpha_resid_spy\", \"alpha_mom_spy_20_ema10\", \"alpha_mom_spy_60_ema10\",\n",
    "    \"alpha_resid_sector\", \"alpha_mom_sector_20_ema10\", \"alpha_mom_sector_60_ema10\",\n",
    "    \"alpha_mom_combo_20_ema10\", \"alpha_mom_combo_60_ema10\",\n",
    "    \n",
    "    # Relative strength\n",
    "    \"rel_strength_spy\", \"rel_strength_spy_norm\", \"rel_strength_spy_slope20\",\n",
    "    \"rel_strength_sector\", \"rel_strength_sector_norm\", \"rel_strength_sector_slope20\",\n",
    "    \n",
    "    # Cross-sectional momentum\n",
    "    \"xsec_mom_5d_z\", \"xsec_mom_20d_z\", \"xsec_mom_60d_z\",\n",
    "    \"xsec_mom_5d_sect_neutral_z\", \"xsec_mom_20d_sect_neutral_z\", \"xsec_mom_60d_sect_neutral_z\",\n",
    "    \n",
    "    # Breadth\n",
    "    \"pct_universe_above_ma20\", \"pct_universe_above_ma50\", \"pct_universe_above_ma200\",\n",
    "    \"ad_line_universe\"\n",
    "]\n",
    "\n",
    "total_df = df_merged[['date'] +selected_features+['hit','weight']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cae6de-b59b-4794-8f9e-3234aee98e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# --- Sort by date and define train/test split\n",
    "total_df = total_df.sort_values(\"date\")\n",
    "split_date = total_df[\"date\"].quantile(0.8)  # 80% train, 20% test\n",
    "\n",
    "# --- Create train/test sets\n",
    "train_df = total_df[total_df[\"date\"] <= split_date].copy()\n",
    "test_df  = total_df[total_df[\"date\"] > split_date].copy()\n",
    "\n",
    "# --- Define selected features (must exclude 'hit', 'target', 'date', 'symbol')\n",
    "\n",
    "\n",
    "X_train = train_df[selected_features]\n",
    "X_test  = test_df[selected_features]\n",
    "\n",
    "# --- Encode class labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['hit'])  # [-1, 0, 1] â†’ [0, 1, 2]\n",
    "y_test  = label_encoder.transform(test_df['hit'])\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# --- Compute balanced sample weights\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# --- Train XGBoost model\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    num_class=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    # ðŸŒ± Regularization\n",
    "    max_depth=2,                 # Shallower trees\n",
    "    min_child_weight=5,          # Require more samples per leaf\n",
    "    gamma=2.0,                   # Higher threshold for node splits\n",
    "    subsample=0.7,               # Lower row sampling\n",
    "    colsample_bytree=0.7,        # Lower feature sampling\n",
    "    reg_alpha=2.0,               # Stronger L1 regularization\n",
    "    reg_lambda=3.0,              # Stronger L2 regularization\n",
    "\n",
    "    # ðŸ“‰ Learning rate and tree count\n",
    "    learning_rate=0.03,          # Lower LR for stability\n",
    "    n_estimators=700,            # More trees to compensate\n",
    ")\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# --- Predict\n",
    "y_pred_proba_test = model.predict_proba(X_test)\n",
    "y_pred_proba_train = model.predict_proba(X_train)\n",
    "\n",
    "# --- Evaluate Test AUC\n",
    "macro_auc_test = roc_auc_score(y_test, y_pred_proba_test, multi_class='ovr', average='macro')\n",
    "print(f\"\\nðŸ§ª Test AUC (macro-averaged OVR): {macro_auc_test:.4f}\")\n",
    "\n",
    "# --- Per-class AUC (Test)\n",
    "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
    "print(\"\\nPer-Class AUC (Test):\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    auc = roc_auc_score(y_test_bin[:, i], y_pred_proba_test[:, i])\n",
    "    print(f\"  Class {label}: AUC = {auc:.4f}\")\n",
    "\n",
    "# --- Evaluate Train AUC\n",
    "macro_auc_train = roc_auc_score(y_train, y_pred_proba_train, multi_class='ovr', average='macro')\n",
    "print(f\"\\nðŸ§  Train AUC (macro-averaged OVR): {macro_auc_train:.4f}\")\n",
    "\n",
    "# --- Per-class AUC (Train)\n",
    "y_train_bin = label_binarize(y_train, classes=[0, 1, 2])\n",
    "print(\"\\nPer-Class AUC (Train):\")\n",
    "for i, label in enumerate(class_labels):\n",
    "    auc = roc_auc_score(y_train_bin[:, i], y_pred_proba_train[:, i])\n",
    "    print(f\"  Class {label}: AUC = {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bbd13-6b67-41fd-b804-d46388e1e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b0425-3462-489c-8d64-db6e2c79a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d7fc9-d55d-4481-8be2-aba41fd902bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f1066-9541-46fa-8469-4d025bfd46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import json\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# --- Encode class labels\n",
    "label_encoder = LabelEncoder()\n",
    "total_df['target_encoded'] = label_encoder.fit_transform(total_df['hit'])  # [-1, 0, 1] â†’ [0, 1, 2]\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# --- Prepare fold splits based on time\n",
    "total_df = total_df.sort_values(\"date\").reset_index(drop=True)\n",
    "n_folds = 10\n",
    "fold_size = len(total_df) // n_folds\n",
    "\n",
    "fold_indices = []\n",
    "for i in range(n_folds):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < n_folds - 1 else len(total_df)\n",
    "    fold_indices.append((start, end))\n",
    "\n",
    "# --- Hyperopt Objective\n",
    "def objective(space):\n",
    "    fold_aucs = []\n",
    "\n",
    "    for i in range(n_folds - 1):  # leave last fold for testing, train on earlier ones\n",
    "        train_end = fold_indices[i][1]\n",
    "        val_start, val_end = fold_indices[i + 1]\n",
    "\n",
    "        train_df = total_df.iloc[:train_end]\n",
    "        val_df = total_df.iloc[val_start:val_end]\n",
    "\n",
    "        X_train = train_df[selected_features].values\n",
    "        y_train = train_df['target_encoded'].values\n",
    "        X_val   = val_df[selected_features].values\n",
    "        y_val   = val_df['target_encoded'].values\n",
    "\n",
    "        # Step 1: Extract raw weights\n",
    "        raw_weights = train_df[\"weight\"].values\n",
    "        y_train = train_df[\"target_encoded\"].values\n",
    "        \n",
    "        # Step 2: Compute class balancing weights\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=np.unique(y_train),\n",
    "            y=y_train\n",
    "        )\n",
    "        class_weight_map = {cls: w for cls, w in zip(np.unique(y_train), class_weights)}\n",
    "        \n",
    "        # Step 3: Combine raw weight Ã— class weight\n",
    "        combined_weights = np.array([\n",
    "            raw_weights[i] * class_weight_map[y_train[i]]\n",
    "            for i in range(len(y_train))\n",
    "        ])\n",
    "        \n",
    "        # Step 4: Clip extreme weights to avoid dominance\n",
    "        # You can tune these thresholds â€” e.g., 1stâ€“99th percentile\n",
    "        lower_clip = np.percentile(combined_weights, 1)\n",
    "        upper_clip = np.percentile(combined_weights, 99)\n",
    "        clipped_weights = np.clip(combined_weights, lower_clip, upper_clip)\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train, weight=clipped_weights)\n",
    "        dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"num_class\": 3,\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"eta\": space[\"eta\"],\n",
    "            \"max_depth\": int(space[\"max_depth\"]),\n",
    "            \"subsample\": space[\"subsample\"],\n",
    "            \"colsample_bytree\": space[\"colsample_bytree\"],\n",
    "            \"lambda\": space[\"reg_lambda\"],\n",
    "            \"alpha\": space[\"reg_alpha\"],\n",
    "        }\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dval, \"eval\")],\n",
    "            early_stopping_rounds=30,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        y_val_pred = model.predict(dval)\n",
    "        y_val_bin = label_binarize(y_val, classes=[0, 1, 2])\n",
    "        auc = roc_auc_score(y_val_bin, y_val_pred, multi_class='ovr', average='macro')\n",
    "        fold_aucs.append(auc)\n",
    "\n",
    "    return {\"loss\": -np.mean(fold_aucs), \"status\": STATUS_OK}\n",
    "\n",
    "# --- Search space\n",
    "space = {\n",
    "    \"eta\": hp.loguniform(\"eta\", np.log(0.005), np.log(0.3)),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 3, 10, 1),\n",
    "    \"min_child_weight\": hp.loguniform(\"min_child_weight\", np.log(1), np.log(20)),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "    \"gamma\": hp.uniform(\"gamma\", 0, 5),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(1e-3), np.log(10.0)),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(1e-3), np.log(10.0)),\n",
    "    \"booster\": hp.choice(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "}\n",
    "# --- Run optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=500, trials=trials)\n",
    "best[\"max_depth\"] = int(best[\"max_depth\"])\n",
    "\n",
    "# --- Save results\n",
    "with open(\"xgb_hyperopt_best_params.json\", \"w\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Best hyperparameters saved:\")\n",
    "print(json.dumps(best, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3593a3-8418-45d7-8349-c84fb9905adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (svd_forecasting)",
   "language": "python",
   "name": "svd_forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
